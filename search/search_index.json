{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lighter","text":""},{"location":"#_1","title":"Lighter","text":"<pre><code>pip install lighter\n</code></pre> <ul> <li> <p> Configuration-based</p> <p>Define, reproduce, and share experiments through config files.</p> </li> <li> <p> Task-agnostic</p> <p>Classification, segmentation, or self-supervised learning? Lighter can handle it.</p> </li> <li> <p> Minimal</p> <p>Lighter handles the boilerplate, letting you run experiments with little to no code.</p> </li> <li> <p> Customizable</p> <p>Add custom code seamlessly, whether it's models, datasets, or any other component.</p> </li> </ul>"},{"location":"#lighter-vs-pytorch-lightning","title":"Lighter vs. PyTorch Lightning","text":"<p>See how training a model on CIFAR-10 differs between Lighter and PyTorch Lightning.</p> LighterPyTorch Lightning Terminal<pre><code>lighter fit config.yaml\n</code></pre> config.yaml<pre><code>trainer:\n    _target_: pytorch_lightning.Trainer\n    max_epochs: 2\n\nsystem:\n    _target_: lighter.System\n\n    model:\n        _target_: torchvision.models.resnet18\n        num_classes: 10\n\n    criterion:\n        _target_: torch.nn.CrossEntropyLoss\n\n    optimizer:\n        _target_: torch.optim.Adam\n        params: \"$@system#model.parameters()\"\n        lr: 0.001\n\n    dataloaders:\n        train:\n            _target_: torch.utils.data.DataLoader\n            batch_size: 32\n            shuffle: True\n            dataset:\n                _target_: torchvision.datasets.CIFAR10\n                download: True\n                root: .datasets\n                train: True\n                transform:\n                    _target_: torchvision.transforms.Compose\n                    transforms:\n                        - _target_: torchvision.transforms.ToTensor\n                        - _target_: torchvision.transforms.Normalize\n                          mean: [0.5, 0.5, 0.5]\n                          std: [0.5, 0.5, 0.5]\n</code></pre> Terminal<pre><code>python cifar10.py\n</code></pre> cifar10.py<pre><code>from pytorch_lightning import Trainer, LightningModule\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor, Normalize, Compose\n\n\nclass Model(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = resnet18(num_classes=10)\n        self.criterion = CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        return loss\n\n    def configure_optimizers(self):\n        return Adam(self.model.parameters(), lr=0.001)\n\n\ntransform = Compose([\n    ToTensor(),\n    Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ntrain_dataset = CIFAR10(\n    root=\".datasets\",\n    train=True,\n    download=True,\n    transform=transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nmodel = Model()\ntrainer = Trainer(max_epochs=2)\ntrainer.fit(model, train_loader)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Tutorials</p> <p>Run your first experiments with step-by-step tutorials  Start Learning</p> </li> <li> <p> How-To Guides</p> <p>Learn about Lighter's advanced features with practical guides  Learn More</p> </li> <li> <p> Design</p> <p>Understand Lighter's design principles and architecture  Read More</p> </li> <li> <p> Reference</p> <p>Explore Lighter's classes, functions, and interfaces  View API</p> </li> </ul>"},{"location":"#cite","title":"Cite","text":"<pre><code>@software{lighter,\n    author       = {Ibrahim Hadzic and\n                    Suraj Pai and\n                    Keno Bressem and\n                    Hugo Aerts},\n    title        = {Lighter},\n    publisher    = {Zenodo},\n    doi          = {10.5281/zenodo.8007711},\n    url          = {https://doi.org/10.5281/zenodo.8007711}\n}\n</code></pre>"},{"location":"design/overview/","title":"Overview","text":""},{"location":"design/overview/#challenges-of-deep-learning-experimentation","title":"Challenges of Deep Learning Experimentation","text":"<p>Deep learning has revolutionized many fields, but the process of conducting deep learning research is still complex and time-consuming. Practitioners often face challenges such as:</p> <ul> <li> <p> Boilerplate Code</p> <p>Writing repetitive code for training loops, data loading, metric calculation, and experiment setup for each new project.</p> </li> <li> <p> Experiment Management</p> <p>Managing a range of hyperparameters, datasets, and architectures across different experiments.</p> </li> <li> <p> Reproducibility</p> <p>Ensuring that experiments are reproducible and that results can be reliably replicated.</p> </li> <li> <p> Collaboration</p> <p>Sharing and collaborating on experiments with other researchers or team members.</p> </li> </ul>"},{"location":"design/overview/#lighter-configuration-driven-deep-learning","title":"Lighter: Configuration-Driven Deep Learning","text":"<p>Lighter is a framework designed to address these challenges and streamline deep learning experimentation. It uses YAML configuration files for experiment definition and management.</p>"},{"location":"design/overview/#core-philosophy","title":"Core Philosophy","text":"<p>Lighter's core philosophy is to separate configuration from code and provide a high-level, declarative way to define deep learning experiments. This is achieved through the following key design principles:</p> <ol> <li> <p>Declarative Experiment Definition with YAML: Lighter embraces a declarative approach, where experiments are defined entirely through YAML configuration files. This contrasts with an imperative approach, where the experiment logic is written directly in code. The YAML configuration acts as a blueprint, specifying what components to use and how they are connected, without dictating the implementation details of the training loop or data processing. This separation of concerns is fundamental to Lighter's design.</p> </li> <li> <p>Component-Based Architecture: Lighter structures deep learning experiments into distinct, well-defined components:</p> <ul> <li><code>model</code>: The neural network architecture.</li> <li><code>optimizer</code>: The optimization algorithm (e.g., Adam, SGD).</li> <li><code>scheduler</code>: Learning rate scheduler (optional).</li> <li><code>criterion</code>: The loss function.</li> <li><code>metrics</code>: Evaluation metrics (e.g., accuracy, F1-score).</li> <li><code>dataloaders</code>: Data loading and preprocessing pipelines.</li> <li><code>inferer</code>: (Optional) Handles inference logic, like sliding window inference.</li> <li><code>adapters</code>: Customize data handling, argument passing, and transformations.</li> </ul> <p>Each component is defined in in the <code>system</code> section of the YAML configuration. This <code>system</code> then interacts with PyTorch Lightning's <code>Trainer</code> to orchestrate the training process.</p> </li> <li> <p>Encapsulation and Orchestration with <code>System</code>: The <code>lighter.System</code> acts as a central orchestrator, encapsulating all the experiment's components. It inherits from PyTorch Lightning's <code>LightningModule</code>, providing a familiar and well-defined interface for training, validation, testing, and prediction.  Crucially, the <code>System</code> class is responsible for:</p> <ul> <li>Instantiating the components defined in the YAML configuration.</li> <li>Connecting these components and managing the flow of data between them.  This includes passing the model's parameters to the optimizer, feeding data through the model, calculating the loss, and updating metrics.  The <code>Adapters</code> play a vital role in this data flow management.</li> <li>Providing hooks for custom logic (e.g., <code>on_train_start</code>).</li> <li>Defining the <code>forward</code> method, which is used for inference, and delegating to a custom <code>inferer</code> if one is specified.</li> <li>Implementing the <code>_step</code> method, common to all stages (<code>training_step</code>, <code>validation_step</code>, <code>test_step</code>, <code>predict_step</code>), which handles batch preparation, forward pass, loss calculation, and metric computation.</li> </ul> <p>The <code>System</code> class does not implement the training loop itself. Instead, it defines how the components interact within each step of the loop.</p> </li> <li> <p>Leveraging PyTorch Lightning's <code>Trainer</code>: Lighter builds upon PyTorch Lightning's <code>Trainer</code>, inheriting its powerful features for distributed training, mixed precision, checkpointing, and, most importantly, the training loop logic. The <code>trainer</code> section in the YAML configuration allows users to directly configure the <code>Trainer</code> object, providing access to all of PyTorch Lightning's capabilities.  The <code>Trainer</code> executes the training loop, calling the <code>System</code>'s methods (e.g., <code>training_step</code>, <code>validation_step</code>) at each iteration. This separation of concerns keeps Lighter's core logic focused on experiment configuration and component management.</p> </li> <li> <p>Project-Specific Custom Modules: Lighter is designed to be highly extensible. Users can define custom modules (models, datasets, metrics, callbacks, etc.) within their own project directory and seamlessly integrate them into Lighter experiments. The <code>project</code> key in the YAML configuration specifies the path to the project's root directory. Lighter then dynamically imports these custom modules, making them available for use in the configuration file via the <code>_target_</code> key. This eliminates the need to modify Lighter's core code to add custom functionality, promoting flexibility and code organization.</p> </li> <li> <p>Adapters: The Key to Task-Agnosticism: Lighter's <code>Adapters</code> are a crucial design element that enables Lighter to be task-agnostic. Adapters provide a way to customize the flow of data between different components without modifying the components themselves. They act as intermediaries, handling variations in data formats, argument orders, and pre/post-processing requirements. Lighter provides several types of adapters:</p> <ul> <li> <p><code>BatchAdapter</code>:  Handles variations in batch structure.  Different datasets or tasks might return batches in different formats (e.g., <code>(input, target)</code>, <code>input</code>, <code>{\"image\": input, \"label\": target}</code>). The <code>BatchAdapter</code> allows users to specify how to extract the <code>input</code>, <code>target</code>, and <code>identifier</code> (e.g., filename) from the batch, regardless of its original structure. This uses accessor functions, integer indices, or string keys, as appropriate.</p> </li> <li> <p><code>CriterionAdapter</code> and <code>MetricsAdapter</code>:  Handle variations in the argument signatures of loss functions and metrics. Some loss functions might expect arguments in the order <code>(pred, target)</code>, while others might expect <code>(target, pred)</code> or even include the <code>input</code>. These adapters allow users to specify the correct argument mapping (positional or keyword) and apply transformations (e.g., <code>torch.sigmoid</code>) before the data is passed to the criterion or metric. This avoids the need to write wrapper functions for each loss function or metric.</p> </li> <li> <p><code>LoggingAdapter</code>:  Handles transformations applied to data before it is logged. This is useful for tasks like converting predictions to class labels (using <code>torch.argmax</code>) or converting tensors to NumPy arrays for visualization.</p> </li> </ul> <p>The adapter system allows Lighter to handle a wide range of tasks (classification, segmentation, self-supervised learning, etc.) without requiring changes to the core framework.  It provides a powerful and flexible mechanism for customizing data flow, making Lighter highly adaptable to different research problems.</p> </li> </ol>"},{"location":"design/overview/#recap","title":"Recap","text":"<ul> <li>Declarative Configuration: Defining experiments through YAML files.</li> <li>Modularity:  Breaking down experiments into well-defined components.</li> <li>Encapsulation and Orchestration: Using the <code>System</code> class to manage component interactions and data flow.</li> <li>Extensibility:  Allowing users to easily incorporate custom modules.</li> <li>Flexibility:  Adapting to diverse tasks and data formats through the adapter system.</li> <li>Simplicity: Leveraging PyTorch Lightning for robust training features, allowing the user to run their experiment with a single command.</li> </ul>"},{"location":"how-to/adapters/","title":"Adapters","text":"<p>Adapters are a powerful Lighter feature that allow data flow customization between different steps. This enables Lighter to handle any task without modifying code.</p> <p>Examples of when you might need adapters include:</p> <p>Handling Different Batch Structures</p> <p>Often, a batch is simply a tuple of input and target tensors. Other times, a batch may be just the input tensor (e.g. prediction dataset or self-supervised learning). With adapters, you can handle any batch format without changing the core code.</p> <p>Adapting Argument Order</p> <p>One loss function may require <code>(pred, target)</code> arguments, while another may require <code>(target, pred)</code>. The third may require <code>(input,  pred, target)</code>. You can specify what should be passed using adapters.</p> <p>Transforming Data</p> <p>You're dealing with grayscale images, but the logger expects RGB images for visualization. Adapters allow you to transform data before such operations.</p>"},{"location":"how-to/adapters/#types-of-adapters-in-lighter","title":"Types of Adapters in Lighter","text":"<p>Lighter provides four adapter types, each designed to address specific customization needs:</p>"},{"location":"how-to/adapters/#batchadapter","title":"BatchAdapter","text":"<p>Lighter expects the following data structure:</p> Item Description Input The input data (e.g., images, text, audio) that is fed into the model. Target (optional) The target data (e.g., labels, segmentation masks) used to compute the loss. Identifier (optional) An identifier for the data (e.g., image filenames, patient IDs). <p>Different datasets and tasks may use different batch structures. To address that, we provide <code>lighter.adapters.BatchAdapter</code> to customize how data is extracted from the batch. You can specify accessors to extract the <code>input</code>, <code>target</code>, and <code>identifier</code> tensors from the batch, regardless of the original batch structure.</p> <p>Note</p> <p>By default, Lighter assumes that the batch is an <code>(input, target)</code> tuple, without identifier, defined by: <pre><code>BatchAdapter(input_accessor=0, target_accessor=1, identifier_accessor=None)\n</code></pre></p> <p>BatchAdapter accessors can be:</p> Accessor Type Description Example Integer Index For list or tuple batches, use an integer index to access elements by position. <code>input_accessor=0, target_accessor=1</code> String Key For dictionary batches, use a string key (e.g., <code>\"image\"</code>) to access elements by key. <code>input_accessor=\"image\", target_accessor=\"label\", identifier_accessor=\"name\"</code> Callable Function (Advanced) For more complex batch structures or transformations, provide a callable function that takes the batch as input and returns the desired <code>input</code>, <code>target</code>, or <code>identifier</code>. <code>input_accessor=0, target_accessor=lambda batch: one_hot_encode(batch[1])</code> <p>Below, you can see an example of how to configure the <code>BatchAdapter</code> in your config file:</p> <pre><code>system:\n# ...\n    adapters:\n        train:\n            batch:\n                _target_: lighter.adapters.BatchAdapter\n                input_accessor: \"image\"\n                target_accessor: \"label\"\n                identifier_accessor: \"id\"\n</code></pre> <p>For more information, see the BatchAdapter documentation.</p>"},{"location":"how-to/adapters/#criterionadapter","title":"CriterionAdapter","text":"<p>CriterionAdapter argument mappers specify how the <code>pred</code>, <code>target</code>, and <code>input</code> tensors are passed as arguments to your criterion function.  You can configure argument mappers and transforms for each of <code>pred</code>, <code>target</code>, and <code>input</code>.</p> <p>CriterionAdapter argument mappers can be:</p> Mapper Type Description Example Integer Index For criterion functions expecting positional arguments, use an integer index to specify the argument position (starting from 0). <code>pred_argument=1, target_argument=0</code> String Key For criterion functions expecting keyword arguments, use a string key (e.g., <code>\"prediction\"</code>) to specify the argument name. <code>pred_argument=\"prediction\", target_argument=\"ground_truth\"</code> <code>None</code> If the criterion function doesn't require a specific tensor (<code>pred</code>, <code>target</code>, or <code>input</code>), set the corresponding argument mapper to <code>None</code>. <code>input_argument=None</code> <p>Transforms allow you to apply functions to <code>pred</code>, <code>target</code>, and <code>input</code> tensors before they are passed to the criterion.</p> Transform Type Description Example Callable Function Provide a callable function (or a list of callable functions) that takes the tensor as input and returns the transformed tensor. Transforms are defined using the MONAI Bundle syntax. <code>pred_transforms: [_target_: torch.sigmoid]</code> <p>Below, you can see an example of how to configure the <code>CriterionAdapter</code> in your config file:</p> config.yaml<pre><code>system:\n    adapters:\n        train:\n            criterion:\n                _target_: lighter.adapters.CriterionAdapter\n                # Map 'pred' to the 2nd positional argument (index 1)\n                pred_argument: 1\n                # Map 'target' to the 1st positional argument (index 0)\n                target_argument: 0\n                # Apply sigmoid activation to predictions                 \n                pred_transforms:\n                    - _target_: torch.sigmoid\n</code></pre> <p>For more information, see the CriterionAdapter documentation.</p>"},{"location":"how-to/adapters/#metricsadapter","title":"MetricsAdapter","text":"<p>The configuration of <code>MetricsAdapter</code> is identical to <code>CriterionAdapter</code>. You use argument mappers and transforms in the same way.</p> <p>Below, you can see an example of how to configure the <code>MetricsAdapter</code> in your config file:</p> config.yaml<pre><code>system:\n    adapters:\n        val:\n            metrics:\n                _target_: lighter.adapters.MetricsAdapter\n                # Map 'pred' to keyword argument \"prediction\"\n                pred_argument: \"prediction\"\n                # Map 'target' to keyword argument \"ground_truth\"\n                target_argument: \"ground_truth\"\n                # Convert pred to class labels\n                pred_transforms:\n                    - _target_: torch.argmax              \n                      dim: 1\n</code></pre> <p>For more information, see the MetricsAdapter documentation.</p>"},{"location":"how-to/adapters/#loggingadapter","title":"LoggingAdapter","text":"<p>LoggingAdapter configuration focuses on applying transforms to <code>pred</code>, <code>target</code>, and <code>input</code> tensors before they are logged.</p> <p>Transforms for LoggingAdapter are configured using <code>pred_transforms</code>, <code>target_transforms</code>, and <code>input_transforms</code>.</p> Transform Type Description Example Callable Function Provide a callable function (or a list of callable functions) that takes the tensor as input and returns the transformed tensor. Transforms are defined using the MONAI Bundle syntax. <code>input_transforms: [_target_: monai.transforms.ToNumpy]</code> <p>Below, you can see an example of how to configure the <code>LoggingAdapter</code> in your config file:</p> config.yaml<pre><code>system:\n    adapters:\n    train:\n        logging:\n            _target_: lighter.adapters.LoggingAdapter # Use the built-in LoggingAdapter\n            # Turn grayscale into rgb by repeating the channels\n            input_transforms: \"$lambda x: x.repeat(1, 3, 1, 1)\"\n            pred_transforms:\n                - _target_: torch.argmax\n                  dim: 1\n</code></pre> <p>For more information, see the LoggingAdapter documentation.</p>"},{"location":"how-to/adapters/#recap-and-next-steps","title":"Recap and Next Steps","text":"<p>Adapters are a cornerstone of Lighter's flexibility and extensibility. By using adapters effectively, you can:</p> <ul> <li>Seamlessly integrate Lighter with diverse data formats and batch structures.</li> <li>Customize argument passing to loss functions and metrics.</li> <li>Apply pre-processing and post-processing transforms at various stages of your experiment.</li> <li>Tailor the data that is logged for monitoring and analysis.</li> <li>Create highly specialized and reusable customization components.</li> </ul> <p>With adapters, you adapt Lighter to your specific research needs and build complex, yet well-organized and maintainable deep learning experiment configurations.</p> <p>Next, explore the Writers to learn how to save model predictions and outputs to files. For a further read on Lighter's design principles and why we designed adapters, check out the Design section.</p>"},{"location":"how-to/configure/","title":"Configure","text":""},{"location":"how-to/configure/#config-structure","title":"Config Structure","text":""},{"location":"how-to/configure/#mandatory-sections","title":"Mandatory Sections","text":"<p>Lighter uses YAML configs to define experiments. A typical config is organized into two mandatory sections:</p> <ul> <li><code>trainer</code>: Configures <code>Trainer</code> (PyTorch Lightning).</li> <li><code>system</code>: Defines <code>System</code> (Lighter) that encapsulates components such as model, criterion, optimizer, or dataloaders.</li> </ul> <p>Here's a minimal example illustrating the basic structure:</p> config.yaml<pre><code>trainer:\n    _target_: pytorch_lightning.Trainer\n    max_epochs: 10\n\nsystem:\n    _target_: lighter.System\n\n    model:\n        _target_: torch.nn.Linear\n        in_features: 100\n        out_features: 10\n\n    criterion:\n        _target_: torch.nn.CrossEntropyLoss\n\n    optimizer:\n        _target_: torch.optim.Adam\n        params: \"$@system#model.parameters()\"\n        lr: 0.001\n\n    dataloaders:\n        train:\n            _target_: torch.utils.data.DataLoader\n            batch_size: 32\n            shuffle: True\n            dataset:\n                _target_: torch.utils.data.TensorDataset\n                tensors:\n                    - _target_: torch.randn\n                      size: [1000, 100]\n                    - _target_: torch.randint\n                      low: 0\n                      high: 10\n                      size: [1000]\n</code></pre> <p>In this example, we define a simple linear model, a cross-entropy loss, and an Adam optimizer. The <code>dataloaders</code> section sets up a basic training dataloader using random tensors.</p>"},{"location":"how-to/configure/#optional-sections","title":"Optional Sections","text":"<p>In addition to mandatory <code>trainer</code> and <code>system</code> sections, you can include the following optional sections: </p> <ul> <li><code>_requires_</code>: Evaluated before the rest of the config. Useful for importing modules used by Python expressions in the config as explained in Evaluating Python Expressions.</li> <li><code>project</code>: Path to your project directory. Used to import custom modules. For more details, see Custom Project Modules.</li> <li><code>vars</code>: Store variables for use in other parts of the config. Useful to avoid repetition and easily update values. See Referencing Other Components.</li> <li><code>args</code>: Arguments to pass to the the stage of the experiment being run.</li> </ul>"},{"location":"how-to/configure/#defining-args","title":"Defining <code>args</code>","text":"<p>Lighter operates in stages: <code>fit</code>, <code>validate</code>, <code>test</code>, and <code>predict</code>. We will cover these in the Run guide in detail.</p> <p>To pass arguments to a stage, use the <code>args</code> section in in your config. For example, to set the <code>ckpt_path</code> argument of the <code>fit</code> stage/method in your config:</p> <pre><code>args:\n    fit:\n        ckpt_path: \"path/to/checkpoint.ckpt\"\n\n# ... Rest of the config\n</code></pre> <p>or pass/override it from the command line:</p> <pre><code>lighter fit experiment.yaml --args#fit#ckpt_path=\"path/to/checkpoint.ckpt\"\n</code></pre> <p>The equivalent of this in Python would be:</p> <pre><code>Trainer.fit(model, ckpt_path=\"path/to/checkpoint.ckpt\")\n</code></pre> <p>where <code>model</code> is an instance of <code>System</code> defined in the <code>experiment.yaml</code>.</p>"},{"location":"how-to/configure/#config-syntax","title":"Config Syntax","text":"<p>Lighter relies on MONAI Bundle configuration system to define and instantiate its components in a clear, modular fashion. This system allows you to separate your code from configuration details by specifying classes, functions, and their initialization parameters in YAML.</p>"},{"location":"how-to/configure/#instantiating-a-class","title":"Instantiating a Class","text":"<p>To create an instance of a class, use the <code>_target_</code> key with the fully qualified name of the class, and list its constructor arguments as key-value pairs. This approach is used for all configurable Lighter components (models, datasets, transforms, optimizers, etc.).</p> <p>Example:</p> <pre><code>model:\n    _target_: torchvision.models.resnet18\n    pretrained: False\n    num_classes: 10\n</code></pre> <p>Here, <code>_target_: torchvision.models.resnet18</code> directs Lighter to instantiate the <code>resnet18</code> model from the <code>torchvision.models</code> module using <code>pretrained</code> and <code>num_classes</code> as constructor arguments. This is equivalent to <code>torchvision.models.resnet18(pretrained=False, num_classes=10)</code> in Python.</p>"},{"location":"how-to/configure/#referencing-other-components","title":"Referencing Other Components","text":"<p>Referencing can be achieved either using <code>%</code> or <code>@</code>.</p> <ul> <li> <p><code>%</code> textually replaces the reference with the YAML value that it points to.</p> </li> <li> <p><code>@</code> replaces the reference with the Python evaluated value that it points to.</p> </li> </ul> <p>To understand the difference, consider the following example:</p> <pre><code>system:\n# ...\n    metrics:\n        train:\n            - _target_: torchmetrics.classification.AUROC\n              task: binary\n        # Or use relative referencing \"%#train\" for the same effect \n        val: \"%system#metrics#train\" # (1)!\n</code></pre> <ol> <li>Reference to the same definition as <code>train</code>, not the same instance.</li> </ol> <p>In this example, <code>val: \"%system#metrics#train\"</code> creates a new instance of <code>torchmetrics.classification.AUROC</code> metric with the same definition as the referenced <code>train</code> metric. This is because <code>%</code> is a textual reference, and the reference is replaced with the YAML value it points to. If we used <code>@</code> instead of <code>%</code>, both <code>train</code> and <code>val</code> would point to the same instance of <code>AUROC</code>, which is not the desired behavior.</p> <p>On the other hand, when defining a scheduler, we want to reference the instantiated optimizer. In this case, we use <code>@</code>: <pre><code>system:\n# ...\n    scheduler:\n        _target_: torch.optim.lr_scheduler.StepLR\n        optimizer: \"@system#optimizer\" # (1)!\n        step_size: 1\n</code></pre></p> <ol> <li>Reference to the instantiated optimizer.</li> </ol>"},{"location":"how-to/configure/#evaluating-python-expressions","title":"Evaluating Python Expressions","text":"<p>Sometimes, you may need to evaluate Python expressions in your config. To indicate that, use <code>$</code> before the expression. For example, we can dinamically define the <code>min_lr</code> of a <code>scheduler</code> to a fraction of <code>optimizer#lr</code>:</p> <pre><code>system:\n# ...\n    scheduler:\n        _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n        optimizer: \"@system#optimizer\"\n        end_lr: \"$@system#optimizer#lr * 0.1\" # (1)!\n</code></pre> <ol> <li><code>$</code> denotes that the expression should be run as Python code.</li> </ol> <p>Note: you will regularly use the combination of evaluation and referencing to pass <code>model.parameters()</code> to your optimizer:</p> <pre><code>optimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\" # (1)!\n    lr: 0.001\n</code></pre> <ol> <li>It first fetches the evaluated <code>\"system#model\"</code>, and then runs <code>.parameters()</code> on it, as indicated by the <code>\"$\"</code> prefix.</li> </ol> <p>Difference between <code>#</code> and <code>.</code></p> <p>Use <code>#</code> to reference elements of the config, and <code>.</code> to access attributes/methods of Python objects. For example, <code>\"$@system#model.parameters()\"</code> fetches the model instance from <code>@system#model</code> and runs <code>.parameters()</code> on it as indicated by <code>$</code>.</p>"},{"location":"how-to/configure/#overriding-config-from-cli","title":"Overriding Config from CLI","text":"<p>Any parameter in the config can be overridden from the command line. Consider the following config:</p> config.yaml<pre><code>trainer:\n    max_epochs: 10\n    callbacks:\n        - _target_: pytorch_lightning.callbacks.EarlyStopping\n          monitor: val_acc\n        - _target_: pytorch_lightning.callbacks.ModelCheckpoint\n          monitor: val_acc\n# ...\n</code></pre> <p>To change <code>max_epochs</code> in <code>trainer</code> from <code>10</code> to <code>20</code>:</p> <pre><code>lighter fit config.yaml --trainer#max_epochs=20\n</code></pre> <p>To override an element of a list, simply specify its index:</p> <pre><code>lighter fit config.yaml --trainer#callbacks#1#monitor=\"val_loss\"\n</code></pre>"},{"location":"how-to/configure/#merging-configs","title":"Merging Configs","text":"<p>You can merge multiple configs by combining them with <code>,</code>:</p> <pre><code>lighter fit config1.yaml,config2.yaml\n</code></pre> <p>This will merge the two configs, with the second config overriding any conflicting parameters from the first one.</p>"},{"location":"how-to/configure/#recap-and-next-steps","title":"Recap and Next Steps","text":"<p>This section covered the fundamental aspects of configuring experiments with Lighter using YAML files.  Key takeaways include:</p> <ul> <li>Structure: Lighter configs are structured into mandatory <code>trainer</code> and <code>system</code> sections, with optional sections like <code>_requires_</code>, <code>vars</code>, <code>args</code>, and <code>project</code>.</li> <li>Stages: Lighter operates in stages (e.g., <code>fit</code>, <code>validate</code>, <code>test</code>), each configurable via the <code>args</code> section.</li> <li>Config Syntax: Lighter leverages the MONAI Bundle configuration system, using <code>_target_</code> to instantiate classes and key-value pairs for arguments.</li> <li>Referencing: Components can be referenced using <code>%</code> (textual replacement) or <code>@</code> (evaluated Python value).  Understanding the difference is crucial for correct instantiation and interaction of components.</li> <li>Python Expressions: Python expressions can be evaluated within the config using <code>$</code>. This is frequently used in conjunction with referencing (e.g., <code>\"$@system#model.parameters()\"</code>).</li> <li>CLI Overrides: Any config parameter can be overridden from the command line, providing flexibility for experimentation.</li> <li>Config Merging: Multiple configs can be merged using commas, allowing for modularity and reuse.</li> </ul> <p>By mastering these configuration basics, you can effectively define and manage your Lighter experiments.</p> <p>Next, we will look at the different Lighter stages and how to Run them. </p>"},{"location":"how-to/freezers/","title":"Freezers","text":"<p>Lighter's <code>Freezer</code> callback specifies which model layers/parameters to freeze/unfreeze during training, and duration. For example, to freeze the encoder layers of a pre-trained model while training the classifier head or decoder</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.Freezer # Use the Freezer callback\n      name_starts_with: [\"model.encoder\"] # Freeze layers starting with \"model.encoder\"\n      until_epoch: 10                   # Unfreeze after epoch 10\n</code></pre> <ul> <li><code>_target_: lighter.callbacks.Freezer</code>: <code>Freezer</code> callback.</li> <li><code>name_starts_with</code>: Freeze layers starting with prefixes (ResNet-18 conv layers).</li> <li><code>until_epoch: 10</code>: Unfreeze layers after epoch 10.</li> </ul> <p>Freezing Strategies:</p> <p><code>Freezer</code> callback offers flexible layer freezing strategies:</p> <ol> <li> <p>Freeze by Name Prefix (<code>name_starts_with</code>):</p> <ul> <li>Freeze parameters with names starting with prefix/prefixes in <code>name_starts_with</code> arg.</li> <li>Useful for freezing modules or layer groups.</li> <li> <p>Example:</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.Freezer\n      name_starts_with: [\"model.encoder\", \"model.embedding\"] # Freeze encoder/embedding layers\n      until_epoch: 5\n</code></pre> <p>Config freezes parameters starting with <code>\"model.encoder\"</code> or <code>\"model.embedding\"</code> until epoch 5.</p> </li> </ul> </li> <li> <p>Freeze by Exact Name (<code>names</code>):</p> <ul> <li>Freeze specific parameters by name using <code>names</code> arg.</li> <li>For fine-grained control over individual layers/parameters.</li> <li> <p>Example:</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.Freezer\n      names: [\"model.classifier.weight\", \"model.classifier.bias\"] # Freeze classifier layer weights/bias\n      until_step: 1000\n</code></pre> <p>Config freezes parameters named <code>\"model.classifier.weight\"</code> and <code>\"model.classifier.bias\"</code> until step 1000.</p> </li> </ul> </li> <li> <p>Exclude Layers from Freezing (<code>except_names</code>, <code>except_name_starts_with</code>):</p> <ul> <li>Exclude layers from freezing (even if matched by <code>name_starts_with</code> or <code>names</code>) using <code>except_names</code>/<code>except_name_starts_with</code>.</li> <li>Selectively unfreeze parts of otherwise frozen module.</li> <li> <p>Example:</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.Freezer\n      name_starts_with: [\"model.encoder\"]           # Freeze all encoder layers\n      except_name_starts_with: [\"model.encoder.layer5\"] # Except \"model.encoder.layer5\" layers\n      until_epoch: 7\n</code></pre> <p>Config freezes <code>\"model.encoder\"</code> layers except <code>\"model.encoder.layer5\"</code>, keeping layer5 trainable.</p> </li> </ul> </li> <li> <p>Unfreezing after Condition (<code>until_step</code>, <code>until_epoch</code>):</p> <ul> <li><code>until_step</code>: Unfreeze layers after training step.</li> <li><code>until_epoch</code>: Unfreeze layers after epoch.</li> <li>Use either/both <code>until_step</code>/<code>until_epoch</code>. Unfreezes when either condition met.</li> <li>Omit <code>until_step</code>/<code>until_epoch</code> to freeze layers for entire training (or manual unfreezing).</li> <li> <p>Example:</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.Freezer\n      name_starts_with: [\"model.backbone\"]\n      until_epoch: 5    # Unfreeze after epoch 5\n      until_step: 5000  # OR after step 5000\n</code></pre> <p>Config unfreezes <code>\"model.backbone\"</code> layers after epoch 5 OR step 5000 (whichever first).</p> </li> </ul> </li> </ol> <p>Combining Freezing Strategies:</p> <p>Combine <code>Freezer</code> callbacks in <code>config.yaml</code> for complex freezing schedules. E.g., initial backbone freeze, gradual part unfreezing.</p> <p>Example: Gradual Layer Unfreezing</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.Freezer # Freezer callback for initial freezing\n      name_starts_with: [\"model.backbone\"] # Freeze backbone initially\n      until_epoch: 5                     # Unfreeze backbone after epoch 5\n    - _target_: lighter.callbacks.Freezer # 2nd Freezer callback for gradual unfreezing\n      name_starts_with: [\"model.encoder.layer1\", \"model.encoder.layer2\"] # Gradually unfreeze layer1/layer2\n      start_epoch: 5                     # Start unfreezing from epoch 5\n      unfreeze_every_n_epochs: 5        # Unfreeze every 5 epochs\n</code></pre> <p>Example: 2 <code>Freezer</code> callbacks for gradual unfreezing - initial backbone freeze, gradual encoder layer unfreezing.</p> <p>Inspecting Frozen Layers:</p> <p><code>Freezer</code> callback logs freezing info during training. Check logs (TensorBoard/console) to verify.</p> <p><code>Freezer</code> Callback Use Cases:</p> <ul> <li>Transfer Learning: Freeze pre-trained model's early layers, train head.</li> <li>Fine-tuning: Gradually unfreeze pre-trained layers.</li> <li>Training Stability: Initial layer freezing.</li> <li>Regularization: Layer freezing for regularization.</li> <li>Efficient Training: Reduce training time/memory.</li> </ul>"},{"location":"how-to/freezers/#recap-and-next-steps","title":"Recap and Next Steps","text":"<p>Lighter <code>Freezer</code> callback: flexible, fine-grained model layer training control via <code>config.yaml</code>. Optimize training, performance, pre-trained knowledge.</p> <p>Next: Inferers.</p>"},{"location":"how-to/inferers/","title":"Inferers","text":"<p>In deep learning, inference is the process of using a trained model to make predictions on new, unseen data. Within the Lighter framework, inferers are specialized components that dictate how this inference process is executed during the validation, testing, and prediction stages of your machine learning experiments.</p> <p>Inferers are responsible for handling a variety of tasks, including:</p> <ul> <li>Sliding Window Inference:  Essential for processing large images or volumes that exceed available memory, this technique involves breaking down the input into smaller, overlapping windows, performing inference on each window, and then stitching the results back together.</li> <li>Model Ensembling: This involves combining the predictions from multiple models to produce a more accurate and stable final prediction.</li> <li>Custom Output Processing:  Inferers can also apply custom post-processing logic to the model's raw outputs. This might include thresholding, applying a softmax function for probabilities, or any other transformation specific to your task.</li> </ul> <p>Lighter seamlessly integrates with MONAI's powerful inferer implementations, providing a strong foundation for your inference workflows. You can readily utilize and customize these pre-built inferers, or you can implement your own custom inferers to address specific inference requirements.</p> <p>This guide will walk you through the process of implementing and utilizing custom inferers within the Lighter framework, giving you fine-grained control over the inference process in your deep learning experiments.</p>"},{"location":"how-to/inferers/#using-monai-inferers-in-lighter","title":"Using MONAI Inferers in Lighter","text":"<p>Lighter provides out-of-the-box integration with MONAI's inferers, offering a wide array of pre-built implementations that you can easily incorporate into your projects.</p>"},{"location":"how-to/inferers/#configuration","title":"Configuration","text":"<p>To use a MONAI inferer, you simply need to configure it within the <code>system.inferer</code> section of your <code>config.yaml</code> file. Here's an example of how to configure the <code>SlidingWindowInferer</code>:</p> config.yaml<pre><code>system:\n  inferer:\n    _target_: monai.inferers.SlidingWindowInferer  # Specify the inferer class\n    roi_size:                         # Region of interest size for each window\n    sw_batch_size: 4                               # Batch size for processing windows\n    overlap: 0.5                                   # Overlap ratio between windows\n</code></pre> <ul> <li><code>_target_</code>: This key specifies the fully qualified class name of the inferer you want to use. In this case, it's <code>monai.inferers.SlidingWindowInferer</code>.</li> <li>Inferer-Specific Arguments: The remaining keys (<code>roi_size</code>, <code>sw_batch_size</code>, <code>overlap</code>) are arguments specific to the <code>SlidingWindowInferer</code>. Consult the MONAI documentation for detailed information about the available inferers and their respective arguments.</li> </ul>"},{"location":"how-to/inferers/#commonly-used-monai-inferers","title":"Commonly Used MONAI Inferers","text":"<p>Here are some of the commonly used MONAI inferers:</p> <ul> <li><code>monai.inferers.SlidingWindowInferer</code>:  Ideal for handling large images or volumes that don't fit into memory.</li> <li><code>monai.inferers.SimpleInferer</code>: A basic inferer that directly passes the entire input to the model. Suitable when your input data can fit in memory.</li> <li><code>monai.inferers.EnsembleInferer</code>:  Facilitates combining predictions from multiple models.</li> <li><code>monai.inferers.patch_inferer</code>:  Designed for patch-based inference strategies.</li> </ul>"},{"location":"how-to/inferers/#example-using-slidingwindowinferer-for-validation","title":"Example: Using <code>SlidingWindowInferer</code> for Validation","text":"config.yaml<pre><code>system:\n  model: #... your model definition...\n\n  inferer:\n    _target_: monai.inferers.SlidingWindowInferer\n    roi_size:\n    sw_batch_size: 8\n    overlap: 0.25\n\n  def validation_step(self, batch, batch_idx):\n    output = super().validation_step(batch, batch_idx)\n    pred = output[Data.PRED]  # Access predictions (processed by the inferer)\n    #... rest of your validation logic...\n</code></pre> <p>In this example, the <code>SlidingWindowInferer</code> is configured to process inputs during the validation stage. Lighter automatically incorporates this inferer into the <code>forward</code> pass of your <code>System</code> (defined in <code>system.py</code>). When <code>self.forward(input)</code> is called within <code>validation_step</code>, Lighter checks if an inferer is configured and if the current mode is 'val', 'test', or 'predict'. If so, it utilizes the inferer to process the input and obtain predictions.</p>"},{"location":"how-to/inferers/#implementing-a-custom-inferer","title":"Implementing a Custom Inferer","text":"<p>While MONAI offers a comprehensive collection of inferers, you may encounter situations where you need to implement custom inference logic. This could be due to:</p> <ul> <li>Advanced Ensembling Strategies: Implementing ensembling techniques beyond simple averaging.</li> <li>Highly Specialized Output Processing:  Tailoring output processing to your unique research problem.</li> </ul> <p>To implement a custom inferer in Lighter, you'll create a Python class that adheres to a specific structure.</p>"},{"location":"how-to/inferers/#custom-inferer-class-structure","title":"Custom Inferer Class Structure","text":"my_project/inferers/my_custom_inferer.py<pre><code>from typing import Any\n\nimport torch\nfrom torch.nn import Module\n\nclass MyCustomInferer:\n    def __init__(self, arg1, arg2, **kwargs):\n        \"\"\"\n        Initialize your custom inferer.\n\n        Args:\n            arg1: Custom argument 1.\n            arg2: Custom argument 2.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.arg1 = arg1\n        self.arg2 = arg2\n        #... initialize any internal components...\n\n    def __call__(self, inputs: torch.Tensor, network: Module, *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n        \"\"\"\n        Perform inference using your custom logic.\n\n        Args:\n            inputs: Input tensor(s) to the model.\n            network: The deep learning model (torch.nn.Module).\n            *args: Additional positional arguments (if needed).\n            **kwargs: Additional keyword arguments (if needed).\n\n        Returns:\n            torch.Tensor: The processed prediction tensor(s).\n        \"\"\"\n        # Implement your custom inference logic here\n        # This could include:\n        #   - Test-time augmentation\n        #   - Model ensembling\n        #   - Sliding window or patch-based inference\n        #   - Any other custom processing\n\n        # Example: Simple forward pass with optional post-processing\n        outputs = network(inputs, *args, **kwargs)\n        processed_outputs = self.post_process(outputs)\n        return processed_outputs\n\n    def post_process(self, outputs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Optional post-processing of model outputs.\n\n        Args:\n            outputs (torch.Tensor): Raw model output tensor(s).\n\n        Returns:\n            torch.Tensor: Processed output tensor(s).\n        \"\"\"\n        # Implement post-processing logic if needed (e.g., thresholding, softmax)\n        return outputs\n</code></pre>"},{"location":"how-to/inferers/#key-components","title":"Key Components","text":"<ol> <li> <p><code>__init__</code>:</p> <ul> <li>This is the constructor of your inferer class.</li> <li>It takes any custom arguments that you can define in your <code>config.yaml</code>.</li> <li>Use this method to initialize any internal components or parameters your inferer needs.</li> </ul> </li> <li> <p><code>__call__</code>:</p> <ul> <li>This method makes your class callable like a function, enabling it to be used directly for inference.</li> <li>Arguments:<ul> <li><code>inputs (torch.Tensor)</code>: The input tensor(s) to your model.</li> <li><code>network (torch.nn.Module)</code>: Your deep learning model (equivalent to <code>self.model</code> in your <code>System</code>).</li> <li><code>*args</code>, <code>**kwargs</code>:  These allow you to pass additional arguments if required, although they are not typically used in inferers.</li> </ul> </li> <li>Logic:<ul> <li>This is where you implement your core inference logic.</li> <li>A common pattern is to perform a forward pass through your <code>network</code> using  <code>outputs = network(inputs)</code>.</li> <li>You can integrate various inference techniques here, such as TTA, ensembling, or sliding window inference.</li> <li>You can also call a <code>post_process</code> method to further refine the model's raw outputs.</li> </ul> </li> <li>Return Value:<ul> <li>This method must return the processed prediction tensor(s) as a <code>torch.Tensor</code>. This output will be used as the <code>pred</code> value in your validation, testing, or prediction steps.</li> </ul> </li> </ul> </li> <li> <p><code>post_process</code> (Optional):</p> <ul> <li>This is an optional method for applying post-processing operations to the model's raw outputs.</li> <li>You can use it for tasks like thresholding, applying a softmax function, or any other custom processing relevant to your problem.</li> <li>If no post-processing is required, you can simply return the <code>outputs</code> tensor directly.</li> </ul> </li> </ol>"},{"location":"how-to/inferers/#integrating-a-custom-inferer","title":"Integrating a Custom Inferer","text":"<ol> <li> <p>Save: Save your custom inferer class (e.g., <code>MyCustomInferer</code>) in a Python file within your project (e.g., <code>my_project/inferers/my_custom_inferer.py</code>).</p> </li> <li> <p>Configure:  In your <code>config.yaml</code>, specify the inferer within the <code>system.inferer</code> section, providing the path to your class and any necessary arguments for its <code>__init__</code> method:</p> config.yaml<pre><code>system:\n  inferer:\n    _target_: my_project.inferers.my_custom_inferer.MyCustomInferer\n    arg1: value1\n    arg2: value2\n</code></pre> <ul> <li><code>_target_</code>: Points to your custom inferer class.</li> <li><code>arg1</code> and <code>arg2</code>:  Arguments passed to your inferer's <code>__init__</code> method.</li> </ul> </li> </ol> <p>With this configuration, Lighter will create an instance of your custom inferer and use it during the appropriate stages of your experiment.</p>"},{"location":"how-to/metrics/","title":"Metrics","text":"<p>Metrics are key for evaluating deep learning models. Lighter integrates <code>torchmetrics</code> for defining metrics in PyTorch. While <code>torchmetrics</code> offers many built-in metrics, custom metrics are often needed for specific research problems.</p> <p>This guide walks through creating and using custom metrics in Lighter, enabling deeper insights into model behavior and performance evaluation tailored to your needs.</p>"},{"location":"how-to/metrics/#torchmetrics-basics-for-custom-metrics","title":"<code>torchmetrics</code> Basics for Custom Metrics","text":"<p>Lighter uses <code>torchmetrics</code> as the metric system foundation. To create custom metrics, understand these <code>torchmetrics</code> concepts:</p> <ul> <li>Metric Class: Base class for custom metrics, inheriting from <code>torchmetrics.Metric</code>. Provides structure and methods.</li> <li><code>update()</code> Method: Called for each data batch during train/val/test. Accumulates statistics based on predictions and targets.</li> <li><code>compute()</code> Method: Called at epoch end (or val/test end). Calculates final metric value from accumulated stats.</li> <li><code>add_state()</code>: Method to store accumulated stats in custom metric class. Registers state variables managed by <code>torchmetrics</code> (distributed computation, state resetting).</li> </ul>"},{"location":"how-to/metrics/#creating-a-custom-metric-step-by-step","title":"Creating a Custom Metric: Step-by-Step","text":"<p>Let's walk through the steps of creating a custom metric in Lighter using <code>torchmetrics</code>. We'll create a simple example custom metric called <code>MyCustomMetric</code> for binary classification, which calculates a variation of accuracy.</p> <ol> <li> <p>Subclass <code>torchmetrics.Metric</code>:     First, create a new Python file (e.g., <code>my_project/metrics/my_custom_metric.py</code>) within your project to define your custom metric class. Start by importing <code>torchmetrics.Metric</code> and subclassing it.</p> my_project/metrics/my_custom_metric.py<pre><code>from torchmetrics import Metric\nimport torch\n\nclass MyCustomMetric(Metric):\n    def __init__(self):\n        super().__init__()\n        # ... (state initialization will be added in the next step) ...\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        # ... (update logic will be added in the next step) ...\n        pass\n\n    def compute(self):\n        # ... (compute logic will be added in the next step) ...\n        pass\n</code></pre> </li> <li> <p>Initialize Metric State with <code>add_state()</code>:     In the <code>__init__</code> method, use <code>self.add_state()</code> to initialize state variables for accumulated statistics. For <code>MyCustomMetric</code>, track correct and total predictions:</p> my_project/metrics/my_custom_metric.py<pre><code>from torchmetrics import Metric\nimport torch\n\nclass MyCustomMetric(Metric):\n    def __init__(self):\n        super().__init__()\n    self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\") # Tracks correct predictions\n    self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")   # Tracks total predictions\n</code></pre> <ul> <li>Registers \"correct\" state variable:<ul> <li>Initializes to a PyTorch tensor of 0.</li> <li><code>dist_reduce_fx=\"sum\"</code>: Reduces state across distributed processes by summing.</li> </ul> </li> <li>Registers \"total\" state variable:<ul> <li>Initializes to a PyTorch tensor of 0.</li> <li><code>dist_reduce_fx=\"sum\"</code>: Reduces state across distributed processes similarly.</li> </ul> </li> </ul> </li> <li> <p>Implement <code>update()</code> Method:     The <code>update()</code> method processes each batch of predictions and targets. For <code>MyCustomMetric</code>, implement the following:</p> <ol> <li>Convert probability predictions to binary (0/1).</li> <li>Count correct predictions and update state variables.</li> </ol> my_project/metrics/my_custom_metric.py<pre><code>from torchmetrics import Metric\nimport torch\n\nclass MyCustomMetric(Metric):\n    def __init__(self):\n        super().__init__()\n    self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n    self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\ndef update(self, preds: torch.Tensor, target: torch.Tensor):\n    # 1. Convert probabilities to binary predictions\n    # Convert probabilities to binary (0/1).\n    #   - `binary_preds = (preds &gt;= 0.5).int()`: Converts probabilities to binary predictions (0 or 1). # commented out to avoid repetition\n\n    # 2. Count correct predictions and update state variables.\n    # Count correct predictions and update state variables.\n    #   - `self.correct += torch.sum(binary_preds == target)`: Increments `correct` state with batch's correct predictions. # commented out to avoid repetition\n    #   - `self.total += target.numel()`: Increments `total` state with batch size. # commented out to avoid repetition\n    self.correct += torch.sum(binary_preds == target)\n    self.total += target.numel()\n</code></pre> <ul> <li> <ol> <li>Convert probability predictions to binary (0/1).</li> </ol> </li> <li> <ol> <li>Count correct predictions and update state variables.</li> </ol> </li> </ul> </li> <li> <p>Implement <code>compute()</code> Method:     The <code>compute()</code> method calculates the final metric value at the epoch end. For <code>MyCustomMetric</code>, calculate custom accuracy:</p> <p>```python title=\"my_project/metrics/my_custom_metric.py\" from torchmetrics import Metric import torch</p> <p>class MyCustomMetric(Metric):     def init(self):         super().init()     self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")     self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")</p> <p>def update(self, preds: torch.Tensor, target: torch.Tensor):     binary_preds = (preds &gt;= 0.5).int()     self.correct += torch.sum(binary_preds == target)     self.total += target.numel()</p> <p>def compute(self):     # Returns custom accuracy: correct predictions / total predictions     return self.correct.float() / self.total ```</p> <ul> <li>Returns custom accuracy: correct predictions / total predictions</li> </ul> </li> <li> <p>Integrate with Lighter Configuration:     Reference your custom metric in <code>config.yaml</code> to use it during train/val/test.</p> <p>Example <code>config.yaml</code>:</p> config.yaml<pre><code>project: my_project/ # Project root directory\n\nsystem:\n  metrics:\n    train:\n      - _target_: torchmetrics.Accuracy\n      - _target_: my_project.metrics.MyCustomMetric # Use custom metric\n\n    val:\n      - _target_: torchmetrics.Accuracy\n      - _target_: my_project.metrics.MyCustomMetric\n</code></pre> <p>This config uses both built-in <code>Accuracy</code> and <code>MyCustomMetric</code> during train/val stages.</p> </li> </ol>"},{"location":"how-to/metrics/#complete-custom-metric-example","title":"Complete Custom Metric Example","text":"<p>Here's the complete code for our example custom metric, <code>MyCustomMetric</code> (in <code>my_project/metrics/my_custom_metric.py</code>):</p> my_project/metrics/my_custom_metric.py<pre><code>from torchmetrics import Metric\nimport torch\n\nclass MyCustomMetric(Metric):\n    def __init__(self):\n        super().__init__()\n        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        binary_preds = (preds &gt;= 0.5).int()\n        self.correct += torch.sum(binary_preds == target)\n        self.total += target.numel()\n\n    def compute(self):\n        return self.correct.float() / self.total\n</code></pre> <p>And here's how you would use it in your <code>config.yaml</code>:</p> config.yaml<pre><code>project: my_project/\n\nsystem:\n  metrics:\n    train:\n      - _target_: torchmetrics.Accuracy\n      - _target_: my_project.metrics.MyCustomMetric\n\n    val:\n      - _target_: torchmetrics.Accuracy\n      - _target_: my_project.metrics.MyCustomMetric\n</code></pre>"},{"location":"how-to/metrics/#recap-and-next-steps","title":"Recap and Next Steps","text":"<p>Creating custom metrics in Lighter using <code>torchmetrics</code> involves these key steps:</p> <ol> <li>Subclass <code>torchmetrics.Metric</code>.</li> <li>Initialize State with <code>add_state()</code> in <code>__init__</code>.</li> <li>Implement <code>update()</code> to process batches and update state.</li> <li>Implement <code>compute()</code> to calculate final metric value.</li> <li>Integrate in <code>config.yaml</code> using <code>_target_</code>.</li> </ol> <p>This enables extending Lighter with custom metrics for tailored model evaluation.</p>"},{"location":"how-to/project_module/","title":"Project Module","text":"<p>Lighter's extensibility allows seamless integration of your custom modules (e.g., models, datasets, callbacks, metrics). This guide shows how to use them to tailor Lighter to your needs.</p> <p>Custom modules are Python modules you create within your project, unlike Lighter or other libraries' modules. Benefits include:</p> <ul> <li>Encapsulation: Keep project-specific code (models, datasets) organized and isolated.</li> <li>Flexibility: Extend Lighter's functionality to fit your use-case.</li> <li>Prototyping: Quickly try new ideas by integrating custom modules.</li> </ul>"},{"location":"how-to/project_module/#project-structure","title":"Project Structure","text":"<p>Your project folder can be named and located however and wherever you want. You only need to ensure that any folder that is a Python module contains <code>__init__.py</code>. In the example below, we see that the project root <code>my_project</code> contains <code>__init__.py</code> file, just like the <code>models</code> and <code>datasets</code> subdirectories. On the other hand, the <code>experiments</code> directory does not contain any Python modules, so it does not need an <code>__init__.py</code> file.</p> <pre><code>my_project/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 my_model.py\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 my_dataset.py\n\u2514\u2500\u2500 experiments/\n    \u251c\u2500\u2500 finetune_full.yaml\n    \u2514\u2500\u2500 finetune_decoder.yaml\n</code></pre>"},{"location":"how-to/project_module/#defining-project-modules","title":"Defining Project Modules","text":"<p>Within the module directories (e.g., <code>models/</code>, <code>datasets/</code>), you define your custom Python modules as regular Python files (<code>.py</code>). Let's define a custom model <code>MyModel</code> in <code>my_model.py</code></p> my_project/models/my_model.py<pre><code>import torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        # Define a linear layer\n        self.linear = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        # Forward pass through the linear layer\n        return self.linear(x)\n</code></pre> <p>and a custom dataset <code>MyDataset</code> in <code>my_dataset.py</code>:</p> my_project/datasets/my_dataset.py<pre><code>from torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n        # Load data from data_path (implementation not shown)\n        self.samples = [...] # List of data samples (replace [...] with actual data loading)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        # Preprocess sample (implementation not shown)\n        # ... \n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n</code></pre>"},{"location":"how-to/project_module/#importing-project-modules-in-the-config","title":"Importing Project Modules in the Config","text":"<p>To use your project's modules in Lighter, you need to define it in your config. Lighter's dynamic module loading mechanism, powered by the <code>import_module_from_path</code> function, will then import that folder as a module named <code>project</code>.</p>"},{"location":"how-to/project_module/#specifying-project-path","title":"Specifying <code>project</code> Path","text":"<p>Specify your project's root directory in <code>config.yaml</code> using the <code>project</code> key. This tells Lighter where to find your custom modules.</p> <p>Example:</p> config.yaml<pre><code>project: my_project/ # Project root path\n</code></pre>"},{"location":"how-to/project_module/#referencing-project-modules","title":"Referencing Project Modules","text":"<p>Reference your projects modules just like you reference any other module. For example, look at <code>system#model</code> and <code>system#dataloaders#train#dataset</code>:</p> <p>Example:</p> config.yaml<pre><code>project: my_project/\n\nsystem:\n  model:\n    _target_: project.models.MyModel\n    input_size: 784\n    num_classes: 10\n\n  dataloaders:\n    train:\n      _target_: torch.utils.data.DataLoader\n      dataset:\n        _target_: project.datasets.MyDataset\n        data_path: \"data/train.csv\"\n        # ... dataset arguments ...\n      batch_size: 32\n      shuffle: True\n</code></pre>"},{"location":"how-to/project_module/#running-lighter-with-custom-modules","title":"Running Lighter with Custom Modules","text":"<p>To run your Lighter experiments that use custom modules, you simply execute the <code>lighter fit</code> command (or other Lighter CLI commands) with your <code>config.yaml</code> file, just as you would with built-in modules.</p> <p>Example: Running Training with Custom Modules</p> Terminal<pre><code>lighter fit config.yaml\n</code></pre> <p>As long as your <code>config.yaml</code> file correctly specifies the <code>project</code> path and the <code>_target_</code> paths to your custom modules, Lighter will dynamically load and use them during the experiment execution.</p>"},{"location":"how-to/project_module/#recap-and-next-steps","title":"Recap and Next Steps","text":"<ol> <li>Organize project with clear directory structure (e.g., subdirectories for modules).</li> <li>Define custom modules (models, datasets) as Python files in project directories.</li> <li>Specify <code>project</code> path in <code>config.yaml</code>.</li> <li>Reference modules in <code>config.yaml</code> using <code>_target_</code> with project-relative paths.</li> <li>Run Lighter as usual.</li> </ol> <p>These steps enable seamless integration of custom code, leveraging Lighter's flexibility for customized deep learning systems.</p> <p>Next, explore the Adapters for more ways to customize Lighter.</p>"},{"location":"how-to/run/","title":"Run","text":""},{"location":"how-to/run/#stages","title":"Stages","text":"<p>Lighter uses PyTorch Lightning's Trainer to manage deep learning experiments. The available stages are:</p> <ul> <li><code>fit</code>: Train your model on training data.</li> <li><code>validate</code>: Evaluate model performance on validation data</li> <li><code>test</code>: Evaluate final model performance on test data</li> <li><code>predict</code>: Generate predictions on new data</li> </ul> <p>For documentation on each, please refer to PyTorch Lightning: <code>fit</code>, <code>validate</code>, <code>test</code>, <code>predict</code>.</p>"},{"location":"how-to/run/#running-a-stage","title":"Running a Stage","text":"<p>The basic command to run a stage of an experiment is:</p> <pre><code>lighter &lt;stage&gt; config.yaml\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;stage&gt;</code> is one of the stages mentioned above (e.g., <code>fit</code>, <code>validate</code>, <code>test</code>, <code>predict</code>).</li> <li><code>config.yaml</code> is the configuration file that defines your experiment. You can also define multiple config files separated by commas, which will be merged (e.g., <code>config1.yaml,config2.yaml</code>).</li> </ul> <p>For example, to train your model, you would use:</p> <pre><code>lighter fit config.yaml\n</code></pre>"},{"location":"how-to/run/#passing-arguments-to-a-stage","title":"Passing arguments to a stage","text":"<p>To pass arguments to a stage, use the <code>args</code> section in in your config. For example, to set the <code>ckpt_path</code> argument of the <code>fit</code> stage/method in your config:</p> <pre><code>args:\n    fit:\n        ckpt_path: \"path/to/checkpoint.ckpt\"\n</code></pre> <p>or pass/override it from the command line:</p> <pre><code>lighter fit experiment.yaml --args#fit#ckpt_path=\"path/to/checkpoint.ckpt\"\n</code></pre>"},{"location":"how-to/run/#recap-and-next-steps","title":"Recap and Next Steps","text":"<p>You now know how to run different stages of your experiment using Lighter. Next, explore the Project Module to learn how to organize your project and reference custom modules in your configuration.</p>"},{"location":"how-to/writers/","title":"Writers","text":"<p>Lighter writers are callbacks for saving model predictions and outputs to files during validation, testing, and prediction. They offer a standardized, extensible way to persist experiment results for analysis and visualization.</p> <p>Lighter offers two main writer types:</p> <ol> <li><code>FileWriter</code>: Saves tensors (predictions, images) to files (NIfTI, NRRD, PNG, MP4, NumPy).</li> <li><code>TableWriter</code>: Saves tabular data (metrics, aggregated predictions) to CSV files.</li> </ol> <p>This guide explains how to use and extend <code>FileWriter</code> and <code>TableWriter</code> in Lighter to effectively manage experiment outputs.</p>"},{"location":"how-to/writers/#using-filewriter","title":"Using <code>FileWriter</code>","text":"<p><code>FileWriter</code> callback saves tensors to files, supports various formats, and is customizable.</p> <p>Configuration:</p> <p>Configure <code>FileWriter</code> in <code>config.yaml</code> within <code>trainer.callbacks</code> section:</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.FileWriter # Use the FileWriter callback\n      path: \"outputs/predictions\"          # Directory to save output files\n      writer: \"itk_nifti\"                  # Writer function to use (ITK-NIfTI format)\n</code></pre> <ul> <li><code>_target_: lighter.callbacks.FileWriter</code>: Specifies that you want to use the <code>FileWriter</code> callback.</li> <li><code>path: \"outputs/predictions\"</code>: Defines the directory where the output files will be saved. Lighter will create this directory if it doesn't exist.</li> <li><code>writer: \"itk_nifti\"</code>: Specifies the writer function to be used for saving tensors. In this example, we use <code>\"itk_nifti\"</code>, which saves tensors in the ITK-NIfTI format (commonly used for medical images).</li> </ul> <p>Built-in Writer Functions:</p> <p><code>FileWriter</code> has built-in writer functions for different formats:</p> <ul> <li><code>\"tensor\"</code>: Raw NumPy <code>.npy</code> files (general tensor saving).</li> <li><code>\"image\"</code>: Images (PNG for 2D, MP4 for 3D animation).</li> <li><code>\"video\"</code>: Videos (MP4 for 4D tensor time-series).</li> <li><code>\"itk_nrrd\"</code>: NRRD files (ITK library, medical imaging).</li> <li><code>\"itk_seg_nrrd\"</code>: NRRD segmentation mask files (ITK).</li> <li><code>\"itk_nifti\"</code>: NIfTI files (ITK library, medical imaging).</li> </ul> <p>Usage:</p> <p>Once configured, <code>FileWriter</code> is used by Lighter in validation, test, and predict stages (if enabled).</p> <p>In these stages, per batch, <code>FileWriter</code>:</p> <ol> <li>Receives <code>pred</code> tensor from <code>predict_step</code>, <code>validation_step</code>, or <code>test_step</code>.</li> <li>Applies <code>LoggingAdapter</code> transforms (if configured).</li> <li>Uses writer function (e.g., <code>\"itk_nifti\"</code>) to save <code>pred</code> tensor to file in <code>path</code> dir.</li> <li>Names file using batch <code>identifier</code> (if available) or generates unique name.</li> </ol> <p>Example: Saving Segmentation Predictions in NIfTI Format</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.FileWriter\n      path: \"outputs/segmentations\"\n      writer: \"itk_nifti\" # Save as NIfTI files\n\nsystem:\n  # ... (other system configurations) ...\n  dataloaders:\n    val:\n      dataset:\n        _target_: monai.datasets.DecathlonDataset\n        task: \"Task09_Spleen\"\n        root: \"data/\"\n        section: \"validation\"\n        transform: # ... (data transforms) ...\n      batch_size: 1\n</code></pre> <p>Example config: <code>FileWriter</code> saves segmentation predictions during validation stage as NIfTI files in <code>outputs/segmentations</code> dir. Filenames use validation dataset identifiers (e.g., patient IDs).</p>"},{"location":"how-to/writers/#extending-filewriter-with-custom-writers","title":"Extending <code>FileWriter</code> with Custom Writers","text":"<p>Extend <code>FileWriter</code> by creating custom writer functions or classes for specific needs.</p> <p>1. Create Custom Writer Function:</p> <p>Define a custom writer function with two arguments:</p> <ul> <li><code>path</code>: Full file path for saving tensor (filename &amp; extension).</li> <li><code>tensor</code>: PyTorch tensor to save.</li> </ul> <p>Example: Custom Writer Function for Text Files</p> my_project/writers/my_custom_writer.py<pre><code>import torch\nimport numpy as np\n\ndef write_tensor_as_text(path: str, tensor: torch.Tensor):\n    \"\"\"Saves tensor to text file.\"\"\"\n    tensor_numpy = tensor.cpu().numpy() # Convert to NumPy array\n    np.savetxt(path, tensor_numpy)      # Save as text file\n</code></pre> <p>2. Register Custom Writer Function:</p> <p>Register custom writer function in <code>config.yaml</code> to use with <code>FileWriter</code>:</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.FileWriter\n      path: \"outputs/text_tensors\"\n      writer: my_project.writers.my_custom_writer.write_tensor_as_text # Path to custom writer\n</code></pre> <ul> <li><code>writer</code>: Path to custom writer function. Replace <code>\"my_project.writers.my_custom_writer\"</code> with your module path.</li> </ul> <p>3. Create Custom Writer Class (Advanced):</p> <p>For complex logic or stateful writers, create a custom class inheriting from <code>lighter.callbacks.writer.BaseWriter</code>.</p> <p>Example: Custom Writer Class for Tensors with Metadata</p> my_project/writers/my_custom_writer_class.py<pre><code>from lighter.callbacks.writer import BaseWriter\nimport torch\nimport json\nimport os\n\nclass MyCustomClassWriter(BaseWriter):\n    @property\n    def writers(self):\n        return {\"tensor_with_metadata\": self.write_tensor_with_metadata} # Register writer function\n\n    def write(self, tensor: torch.Tensor, identifier: str):\n        \"\"\"Main write method called by FileWriter.\"\"\"\n        path = os.path.join(self.path, f\"{identifier}.json\") # Define output path\n        self.write_tensor_with_metadata(path, tensor, identifier=identifier) # Call writer function\n\n    def write_tensor_with_metadata(self, path: str, tensor: torch.Tensor, identifier: str):\n        \"\"\"Saves tensor to JSON file with metadata.\"\"\"\n        metadata = {\n            \"identifier\": identifier,\n            \"shape\": list(tensor.shape),\n            \"dtype\": str(tensor.dtype),\n            \"timestamp\": datetime.datetime.now().isoformat()\n        }\n        data = {\n            \"metadata\": metadata,\n            \"data\": tensor.cpu().numpy().tolist() # Convert tensor data to list\n        }\n        with open(path, 'w') as f:\n            json.dump(data, f, indent=4) # Save data+metadata to JSON\n</code></pre> <p><code>MyCustomClassWriter</code> class example:</p> <ul> <li>Inherits from <code>BaseWriter</code>.</li> <li><code>write_tensor_with_metadata</code>: Saves tensors to JSON with metadata (shape, dtype, timestamp).</li> <li>Registers writer function in <code>writers</code> property with key <code>\"tensor_with_metadata\"</code>.</li> <li>Overrides <code>write</code> method for filename generation and calling custom writer function.</li> </ul> <p>4. Use Custom Writer Class in <code>config.yaml</code>:</p> <p>Use custom writer class by specifying class module path and registered writer key in <code>config.yaml</code>:</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.FileWriter\n      path: \"outputs/metadata_tensors\"\n      writer: my_project.writers.my_custom_writer_class.MyCustomClassWriter.tensor_with_metadata # Custom class writer\n</code></pre> <ul> <li><code>writer</code>: Path to custom writer class and registered writer key (<code>\"tensor_with_metadata\"</code>).</li> </ul>"},{"location":"how-to/writers/#using-tablewriter","title":"Using <code>TableWriter</code>","text":"<p><code>TableWriter</code> callback saves tabular data to CSV files, useful for logging metrics or aggregated predictions.</p> <p>Configuration:</p> <p>Configure <code>TableWriter</code> in <code>config.yaml</code> within <code>trainer.callbacks</code> section:</p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.TableWriter # Use the TableWriter callback\n      path: \"outputs/metrics.csv\"          # Path to save CSV file\n</code></pre> <ul> <li><code>_target_: lighter.callbacks.TableWriter</code>: Use <code>TableWriter</code> callback.</li> <li><code>path: \"outputs/metrics.csv\"</code>: CSV file path for saving tabular data.</li> </ul> <p>Usage:</p> <p>To use <code>TableWriter</code>, return a dictionary from <code>validation_step</code>, <code>test_step</code>, or <code>predict_step</code>. <code>TableWriter</code> saves key-value pairs from dict as CSV rows.</p> <p>Example: Logging Metrics to CSV using <code>TableWriter</code></p> config.yaml<pre><code>trainer:\n  callbacks:\n    - _target_: lighter.callbacks.TableWriter\n      path: \"outputs/metrics.csv\" # Save metrics to CSV\n\nsystem:\n  metrics:\n    val:\n      - _target_: torchmetrics.Accuracy\n      - _target_: torchmetrics.DiceCoefficient\n\n  def validation_step(self, batch, batch_idx):\n    output = super().validation_step(batch, batch_idx) # Call base validation step\n    metrics = output[Data.METRICS] # Get computed metrics\n    self.log_dict(metrics)        # Log metrics for display\n    return metrics                # Return metrics dictionary for TableWriter\n</code></pre> <p>Example: <code>TableWriter</code> saves data to <code>outputs/metrics.csv</code>. In <code>validation_step</code>:</p> <ul> <li>Call base class <code>validation_step</code> to compute metrics.</li> <li>Extract metrics from <code>output</code> dict.</li> <li>Return <code>metrics</code> dict from <code>validation_step</code>.</li> </ul> <p><code>TableWriter</code> captures dict from <code>validation_step</code>/<code>test_step</code>/<code>predict_step</code>, saves as CSV rows. Dict keys become CSV column headers.</p>"},{"location":"how-to/writers/#extending-tablewriter-advanced","title":"Extending <code>TableWriter</code> (Advanced)","text":"<p>Like <code>FileWriter</code>, extend <code>TableWriter</code> with custom writer classes for specialized table writing. See <code>lighter/callbacks/writer/table.py</code> for details.</p>"},{"location":"how-to/writers/#recap-and-next-steps","title":"Recap and Next Steps","text":"<p>Lighter writers are key for saving/managing experiment outputs. Use <code>FileWriter</code> and <code>TableWriter</code>, extend with custom writers as needed to:</p> <ul> <li>Save model predictions in various formats for visualization/analysis.</li> <li>Log metrics/tabular data to CSV for experiment tracking/reporting.</li> <li>Create custom output saving logic.</li> </ul> <p>Writers provide a complete solution for running DL experiments and capturing/utilizing valuable outputs.</p> <p>Next, explore Freezers for freezing model layers.</p>"},{"location":"reference/","title":"lighter","text":"<p>Lighter is a framework for streamlining deep learning experiments with configuration files.</p> <ul> <li>adapters</li> <li>engine</li> <li>system</li> <li>callbacks</li> <li>utils</li> </ul>"},{"location":"reference/#lighter.Config","title":"<code>Config</code>","text":"<p>Handles loading, overriding, validating, and normalizing configurations.</p> Source code in <code>src/lighter/engine/config.py</code> <pre><code>class Config:\n    \"\"\"\n    Handles loading, overriding, validating, and normalizing configurations.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: str | dict[str, Any] | None,\n        validate: bool,\n        **config_overrides: Any,\n    ):\n        \"\"\"\n        Initialize the Config object.\n\n        Args:\n            config: Path to a YAML configuration file or a dictionary containing the configuration.\n            validate: Whether to validate the configuration.\n            config_overrides: Keyword arguments to override values in the configuration file\n        \"\"\"\n        if not isinstance(config, (dict, str, type(None))):\n            raise ValueError(\"Invalid type for 'config'. Must be a dictionary or (comma-separated) path(s) to YAML file(s).\")\n\n        self._config_parser = ConfigParser(globals=False)\n        self._config_parser.read_config(config)\n        self._config_parser.parse()\n\n        # TODO: verify that switching from .update(config_overrides) to .set(value, name) is\n        # a valid approach. The latter allows creation of currently non-existent keys.\n        for name, value in config_overrides.items():\n            self._config_parser.set(value, name)\n\n        # Validate the configuration\n        if validate:\n            validator = cerberus.Validator(SCHEMA)\n            valid = validator.validate(self.get())\n            if not valid:\n                errors = format_validation_errors(validator.errors)\n                raise ConfigurationException(errors)\n\n    def get(self, key: str | None = None, default: Any = None) -&gt; Any:\n        \"\"\"Get raw content for the given key. If key is None, get the entire config.\"\"\"\n        return self._config_parser.config if key is None else self._config_parser.config.get(key, default)\n\n    def get_parsed_content(self, key: str | None = None, default: Any = None) -&gt; Any:\n        \"\"\"\n        Get the parsed content for the given key. If key is None, get the entire parsed config.\n        \"\"\"\n        return self._config_parser.get_parsed_content(key, default=default)\n</code></pre>"},{"location":"reference/#lighter.Config.__init__","title":"<code>__init__(config, validate, **config_overrides)</code>","text":"<p>Initialize the Config object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str | dict[str, Any] | None</code> <p>Path to a YAML configuration file or a dictionary containing the configuration.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate the configuration.</p> required <code>config_overrides</code> <code>Any</code> <p>Keyword arguments to override values in the configuration file</p> <code>{}</code> Source code in <code>src/lighter/engine/config.py</code> <pre><code>def __init__(\n    self,\n    config: str | dict[str, Any] | None,\n    validate: bool,\n    **config_overrides: Any,\n):\n    \"\"\"\n    Initialize the Config object.\n\n    Args:\n        config: Path to a YAML configuration file or a dictionary containing the configuration.\n        validate: Whether to validate the configuration.\n        config_overrides: Keyword arguments to override values in the configuration file\n    \"\"\"\n    if not isinstance(config, (dict, str, type(None))):\n        raise ValueError(\"Invalid type for 'config'. Must be a dictionary or (comma-separated) path(s) to YAML file(s).\")\n\n    self._config_parser = ConfigParser(globals=False)\n    self._config_parser.read_config(config)\n    self._config_parser.parse()\n\n    # TODO: verify that switching from .update(config_overrides) to .set(value, name) is\n    # a valid approach. The latter allows creation of currently non-existent keys.\n    for name, value in config_overrides.items():\n        self._config_parser.set(value, name)\n\n    # Validate the configuration\n    if validate:\n        validator = cerberus.Validator(SCHEMA)\n        valid = validator.validate(self.get())\n        if not valid:\n            errors = format_validation_errors(validator.errors)\n            raise ConfigurationException(errors)\n</code></pre>"},{"location":"reference/#lighter.Config.get","title":"<code>get(key=None, default=None)</code>","text":"<p>Get raw content for the given key. If key is None, get the entire config.</p> Source code in <code>src/lighter/engine/config.py</code> <pre><code>def get(self, key: str | None = None, default: Any = None) -&gt; Any:\n    \"\"\"Get raw content for the given key. If key is None, get the entire config.\"\"\"\n    return self._config_parser.config if key is None else self._config_parser.config.get(key, default)\n</code></pre>"},{"location":"reference/#lighter.Config.get_parsed_content","title":"<code>get_parsed_content(key=None, default=None)</code>","text":"<p>Get the parsed content for the given key. If key is None, get the entire parsed config.</p> Source code in <code>src/lighter/engine/config.py</code> <pre><code>def get_parsed_content(self, key: str | None = None, default: Any = None) -&gt; Any:\n    \"\"\"\n    Get the parsed content for the given key. If key is None, get the entire parsed config.\n    \"\"\"\n    return self._config_parser.get_parsed_content(key, default=default)\n</code></pre>"},{"location":"reference/#lighter.Runner","title":"<code>Runner</code>","text":"<p>Executes the specified stage using the validated and resolved configurations.</p> Source code in <code>src/lighter/engine/runner.py</code> <pre><code>class Runner:\n    \"\"\"\n    Executes the specified stage using the validated and resolved configurations.\n    \"\"\"\n\n    def __init__(self):\n        self.config = None\n        self.resolver = None\n        self.system = None\n        self.trainer = None\n        self.args = None\n\n    def run(self, stage: str, config: str | dict | None = None, **config_overrides: Any) -&gt; None:\n        \"\"\"Run the specified stage with the given configuration.\"\"\"\n        seed_everything()\n        self.config = Config(config, **config_overrides, validate=True)\n\n        # Resolves stage-specific configuration\n        self.resolver = Resolver(self.config)\n\n        # Setup stage\n        self._setup_stage(stage)\n\n        # Run stage\n        self._run_stage(stage)\n\n    def _run_stage(self, stage: str) -&gt; None:\n        \"\"\"Execute the specified stage (method) of the trainer.\"\"\"\n        stage_method = getattr(self.trainer, stage)\n        stage_method(self.system, **self.args)\n\n    def _setup_stage(self, stage: str) -&gt; None:\n        # Prune the configuration to the stage-specific components\n        stage_config = self.resolver.get_stage_config(stage)\n\n        # Import project module\n        project_path = stage_config.get(\"project\")\n        if project_path:\n            import_module_from_path(\"project\", project_path)\n\n        # Initialize system\n        self.system = stage_config.get_parsed_content(\"system\")\n        if not isinstance(self.system, System):\n            raise ValueError(\"'system' must be an instance of System\")\n\n        # Initialize trainer\n        self.trainer = stage_config.get_parsed_content(\"trainer\")\n        if not isinstance(self.trainer, Trainer):\n            raise ValueError(\"'trainer' must be an instance of Trainer\")\n\n        # Set up arguments for the stage\n        self.args = stage_config.get_parsed_content(f\"args#{stage}\", default={})\n\n        # Save config to system checkpoint and trainer logger\n        self._save_config()\n\n    def _save_config(self) -&gt; None:\n        \"\"\"Save config to system checkpoint and trainer logger.\"\"\"\n        if self.system:\n            self.system.save_hyperparameters(self.config.get())\n        if self.trainer and self.trainer.logger:\n            self.trainer.logger.log_hyperparams(self.config.get())\n</code></pre>"},{"location":"reference/#lighter.Runner._run_stage","title":"<code>_run_stage(stage)</code>","text":"<p>Execute the specified stage (method) of the trainer.</p> Source code in <code>src/lighter/engine/runner.py</code> <pre><code>def _run_stage(self, stage: str) -&gt; None:\n    \"\"\"Execute the specified stage (method) of the trainer.\"\"\"\n    stage_method = getattr(self.trainer, stage)\n    stage_method(self.system, **self.args)\n</code></pre>"},{"location":"reference/#lighter.Runner._save_config","title":"<code>_save_config()</code>","text":"<p>Save config to system checkpoint and trainer logger.</p> Source code in <code>src/lighter/engine/runner.py</code> <pre><code>def _save_config(self) -&gt; None:\n    \"\"\"Save config to system checkpoint and trainer logger.\"\"\"\n    if self.system:\n        self.system.save_hyperparameters(self.config.get())\n    if self.trainer and self.trainer.logger:\n        self.trainer.logger.log_hyperparams(self.config.get())\n</code></pre>"},{"location":"reference/#lighter.Runner.run","title":"<code>run(stage, config=None, **config_overrides)</code>","text":"<p>Run the specified stage with the given configuration.</p> Source code in <code>src/lighter/engine/runner.py</code> <pre><code>def run(self, stage: str, config: str | dict | None = None, **config_overrides: Any) -&gt; None:\n    \"\"\"Run the specified stage with the given configuration.\"\"\"\n    seed_everything()\n    self.config = Config(config, **config_overrides, validate=True)\n\n    # Resolves stage-specific configuration\n    self.resolver = Resolver(self.config)\n\n    # Setup stage\n    self._setup_stage(stage)\n\n    # Run stage\n    self._run_stage(stage)\n</code></pre>"},{"location":"reference/#lighter.System","title":"<code>System</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>System encapsulates the components of a deep learning system, extending PyTorch Lightning's LightningModule.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model.</p> required <code>optimizer</code> <code>Optimizer | None</code> <p>Optimizer.</p> <code>None</code> <code>scheduler</code> <code>LRScheduler | None</code> <p>Learning rate scheduler.</p> <code>None</code> <code>criterion</code> <code>Callable | None</code> <p>Criterion (loss) function.</p> <code>None</code> <code>metrics</code> <code>dict[str, Metric | list[Metric] | dict[str, Metric]] | None</code> <p>Metrics for train, val, and test. Supports a single/list/dict of <code>torchmetrics</code> metrics.</p> <code>None</code> <code>dataloaders</code> <code>dict[str, DataLoader]</code> <p>Dataloaders for train, val, test, and predict.</p> required <code>adapters</code> <code>dict[str, Callable] | None</code> <p>Adapters for batch preparation, criterion argument adaptation, metrics argument adaptation, and logging data adaptation.</p> <code>None</code> <code>inferer</code> <code>Callable | None</code> <p>Inferer to use in val/test/predict modes. See MONAI inferers for more details: (https://docs.monai.io/en/stable/inferers.html).</p> <code>None</code> Source code in <code>src/lighter/system.py</code> <pre><code>class System(pl.LightningModule):\n    \"\"\"\n    System encapsulates the components of a deep learning system, extending PyTorch Lightning's LightningModule.\n\n    Args:\n        model: Model.\n        optimizer: Optimizer.\n        scheduler: Learning rate scheduler.\n        criterion: Criterion (loss) function.\n        metrics: Metrics for train, val, and test. Supports a single/list/dict of `torchmetrics` metrics.\n        dataloaders: Dataloaders for train, val, test, and predict.\n        adapters: Adapters for batch preparation, criterion argument adaptation, metrics argument adaptation, and logging data adaptation.\n        inferer: Inferer to use in val/test/predict modes.\n            See MONAI inferers for more details: (https://docs.monai.io/en/stable/inferers.html).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Module,\n        dataloaders: dict[str, DataLoader],\n        optimizer: Optimizer | None = None,\n        scheduler: LRScheduler | None = None,\n        criterion: Callable | None = None,\n        metrics: dict[str, Metric | list[Metric] | dict[str, Metric]] | None = None,\n        adapters: dict[str, Callable] | None = None,\n        inferer: Callable | None = None,\n    ) -&gt; None:\n        super().__init__()\n\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.inferer = inferer\n\n        #  Containers\n        self.dataloaders = DataLoaders(**(dataloaders or {}))\n        self.metrics = Metrics(**(metrics or {}))\n        self.adapters = Adapters(**(adapters or {}))\n\n        # Turn metrics container into a ModuleDict to register them properly.\n        self.metrics = PatchedModuleDict(asdict(self.metrics))\n\n        self.mode = None\n        self._setup_mode_hooks()\n\n    def _step(self, batch: dict, batch_idx: int) -&gt; dict[str, Any] | Any:\n        \"\"\"\n        Performs a step in the specified mode, processing the batch and calculating loss and metrics.\n\n        Args:\n            batch: The batch of data.\n            batch_idx: The index of the batch.\n        Returns:\n            dict or Any: For predict step, returns prediction only. For other steps,\n            returns dict with loss, metrics, input, target, pred, and identifier. Loss is None\n            for test step, metrics is None if unspecified.\n        \"\"\"\n        input, target, identifier = self._prepare_batch(batch)\n        pred = self.forward(input)\n\n        loss = self._calculate_loss(input, target, pred)\n        metrics = self._calculate_metrics(input, target, pred)\n\n        self._log_stats(loss, metrics, batch_idx)\n        output = self._prepare_output(identifier, input, target, pred, loss, metrics)\n        return output\n\n    def _prepare_batch(self, batch: dict) -&gt; tuple[Any, Any, Any]:\n        \"\"\"\n        Prepares the batch data.\n\n        Args:\n            batch: The input batch dictionary.\n\n        Returns:\n            tuple: A tuple containing (input, target, identifier).\n        \"\"\"\n        adapters = getattr(self.adapters, self.mode)\n        input, target, identifier = adapters.batch(batch)\n        return input, target, identifier\n\n    def forward(self, input: Any) -&gt; Any:\n        \"\"\"\n        Forward pass through the model.\n\n        Args:\n            input: The input data.\n\n        Returns:\n            Any: The model's output.\n        \"\"\"\n\n        # Pass `epoch` and/or `step` argument to forward if it accepts them\n        kwargs = {}\n        if hasarg(self.model.forward, Data.EPOCH):\n            kwargs[Data.EPOCH] = self.current_epoch\n        if hasarg(self.model.forward, Data.STEP):\n            kwargs[Data.STEP] = self.global_step\n\n        # Predict. Use inferer if available in val, test, and predict modes.\n        if self.inferer and self.mode in [Mode.VAL, Mode.TEST, Mode.PREDICT]:\n            return self.inferer(input, self.model, **kwargs)\n        return self.model(input, **kwargs)\n\n    def _calculate_loss(self, input: Any, target: Any, pred: Any) -&gt; Tensor | dict[str, Tensor] | None:\n        \"\"\"\n        Calculates the loss using the criterion if in train or validation mode.\n\n        Args:\n            input: The input data.\n            target: The target data.\n            pred: The model predictions.\n\n        Returns:\n            The calculated loss or None if not in train/val mode.\n\n        Raises:\n            ValueError: If criterion is not specified in train/val mode or if loss dict is missing 'total' key.\n        \"\"\"\n        loss = None\n        if self.mode in [Mode.TRAIN, Mode.VAL]:\n            if self.criterion is None:\n                raise ValueError(\"Please specify 'system.criterion' in the config.\")\n\n            adapters = getattr(self.adapters, self.mode)\n            loss = adapters.criterion(self.criterion, input, target, pred)\n\n            if isinstance(loss, dict) and \"total\" not in loss:\n                raise ValueError(\n                    \"The loss dictionary must include a 'total' key that combines all sublosses. \"\n                    \"Example: {'total': combined_loss, 'subloss1': loss1, ...}\"\n                )\n        return loss\n\n    def _calculate_metrics(self, input: Any, target: Any, pred: Any) -&gt; Any | None:\n        \"\"\"\n        Calculates the metrics if not in predict mode.\n\n        Args:\n            input: The input data.\n            target: The target data.\n            pred: The model predictions.\n\n        Returns:\n            The calculated metrics or None if in predict mode or no metrics specified.\n        \"\"\"\n        if self.mode == Mode.PREDICT or self.metrics[self.mode] is None:\n            return None\n\n        adapters = getattr(self.adapters, self.mode)\n        metrics = adapters.metrics(self.metrics[self.mode], input, target, pred)\n        return metrics\n\n    def _log_stats(self, loss: Tensor | dict[str, Tensor], metrics: MetricCollection, batch_idx: int) -&gt; None:\n        \"\"\"\n        Logs the loss, metrics, and optimizer statistics.\n\n        Args:\n            loss: The calculated loss.\n            metrics: The calculated metrics.\n            batch_idx: The index of the batch.\n        \"\"\"\n        if self.trainer.logger is None:\n            return\n\n        # Loss\n        if loss is not None:\n            if not isinstance(loss, dict):\n                self._log(f\"{self.mode}/{Data.LOSS}/{Data.STEP}\", loss, on_step=True)\n                self._log(f\"{self.mode}/{Data.LOSS}/{Data.EPOCH}\", loss, on_epoch=True)\n            else:\n                for name, subloss in loss.items():\n                    self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.STEP}\", subloss, on_step=True)\n                    self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.EPOCH}\", subloss, on_epoch=True)\n\n        # Metrics\n        if metrics is not None:\n            for name, metric in metrics.items():\n                self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.STEP}\", metric, on_step=True)\n                self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.EPOCH}\", metric, on_epoch=True)\n\n        # Optimizer's lr, momentum, beta. Logged in train mode and once per epoch.\n        if self.mode == Mode.TRAIN and batch_idx == 0:\n            for name, optimizer_stat in get_optimizer_stats(self.optimizer).items():\n                self._log(f\"{self.mode}/{name}\", optimizer_stat, on_epoch=True)\n\n    def _log(self, name: str, value: Any, on_step: bool = False, on_epoch: bool = False) -&gt; None:\n        \"\"\"Log a key, value pair. Syncs across distributed nodes if `on_epoch` is True.\n\n        Args:\n            name (str): key to log.\n            value (Any): value to log.\n            on_step (bool, optional): if True, logs on step.\n            on_epoch (bool, optional): if True, logs on epoch with sync_dist=True.\n        \"\"\"\n        batch_size = getattr(self.dataloaders, self.mode).batch_size\n        self.log(name, value, logger=True, batch_size=batch_size, on_step=on_step, on_epoch=on_epoch, sync_dist=on_epoch)\n\n    def _prepare_output(\n        self,\n        identifier: Any,\n        input: Any,\n        target: Any,\n        pred: Any,\n        loss: Tensor | dict[str, Tensor] | None,\n        metrics: Any | None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Prepares the data to be returned by the step function to callbacks.\n\n        Args:\n            identifier: The batch identifier.\n            input: The input data.\n            target: The target data.\n            pred: The model predictions.\n            loss: The calculated loss.\n            metrics: The calculated metrics.\n\n        Returns:\n            dict: A dictionary containing all the step information.\n        \"\"\"\n        adapters = getattr(self.adapters, self.mode)\n        input, target, pred = adapters.logging(input, target, pred)\n        return {\n            Data.IDENTIFIER: identifier,\n            Data.INPUT: input,\n            Data.TARGET: target,\n            Data.PRED: pred,\n            Data.LOSS: loss,\n            Data.METRICS: metrics,\n            Data.STEP: self.global_step,\n            Data.EPOCH: self.current_epoch,\n        }\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"\n        Configures the optimizers and learning rate schedulers.\n\n        Returns:\n            dict: A dictionary containing the optimizer and scheduler.\n\n        Raises:\n            ValueError: If optimizer is not specified.\n        \"\"\"\n        if self.optimizer is None:\n            raise ValueError(\"Please specify 'system.optimizer' in the config.\")\n        if self.scheduler is None:\n            return {\"optimizer\": self.optimizer}\n        else:\n            return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}\n\n    def _setup_mode_hooks(self):\n        \"\"\"\n        Sets up the training, validation, testing, and prediction hooks based on defined dataloaders.\n        \"\"\"\n        if self.dataloaders.train is not None:\n            self.training_step = self._step\n            self.train_dataloader = lambda: self.dataloaders.train\n            self.on_train_start = lambda: self._on_mode_start(Mode.TRAIN)\n            self.on_train_end = self._on_mode_end\n        if self.dataloaders.val is not None:\n            self.validation_step = self._step\n            self.val_dataloader = lambda: self.dataloaders.val\n            self.on_validation_start = lambda: self._on_mode_start(Mode.VAL)\n            self.on_validation_end = self._on_mode_end\n        if self.dataloaders.test is not None:\n            self.test_step = self._step\n            self.test_dataloader = lambda: self.dataloaders.test\n            self.on_test_start = lambda: self._on_mode_start(Mode.TEST)\n            self.on_test_end = self._on_mode_end\n        if self.dataloaders.predict is not None:\n            self.predict_step = self._step\n            self.predict_dataloader = lambda: self.dataloaders.predict\n            self.on_predict_start = lambda: self._on_mode_start(Mode.PREDICT)\n            self.on_predict_end = self._on_mode_end\n\n    def _on_mode_start(self, mode: str | None) -&gt; None:\n        \"\"\"\n        Sets the current mode at the start of a phase.\n\n        Args:\n            mode: The mode to set (train, val, test, or predict).\n        \"\"\"\n        self.mode = mode\n\n    def _on_mode_end(self) -&gt; None:\n        \"\"\"\n        Resets the mode at the end of a phase.\n        \"\"\"\n        self.mode = None\n\n    @property\n    def learning_rate(self) -&gt; float:\n        \"\"\"\n        Gets the learning rate of the optimizer.\n\n        Returns:\n            float: The learning rate.\n\n        Raises:\n            ValueError: If there are multiple optimizer parameter groups.\n        \"\"\"\n        if len(self.optimizer.param_groups) &gt; 1:\n            raise ValueError(\"The learning rate is not available when there are multiple optimizer parameter groups.\")\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    @learning_rate.setter\n    def learning_rate(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the learning rate of the optimizer.\n\n        Args:\n            value: The new learning rate.\n\n        Raises:\n            ValueError: If there are multiple optimizer parameter groups.\n        \"\"\"\n        if len(self.optimizer.param_groups) &gt; 1:\n            raise ValueError(\"The learning rate is not available when there are multiple optimizer parameter groups.\")\n        self.optimizer.param_groups[0][\"lr\"] = value\n</code></pre>"},{"location":"reference/#lighter.System.learning_rate","title":"<code>learning_rate</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the learning rate of the optimizer.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The learning rate.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are multiple optimizer parameter groups.</p>"},{"location":"reference/#lighter.System._calculate_loss","title":"<code>_calculate_loss(input, target, pred)</code>","text":"<p>Calculates the loss using the criterion if in train or validation mode.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The model predictions.</p> required <p>Returns:</p> Type Description <code>Tensor | dict[str, Tensor] | None</code> <p>The calculated loss or None if not in train/val mode.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If criterion is not specified in train/val mode or if loss dict is missing 'total' key.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _calculate_loss(self, input: Any, target: Any, pred: Any) -&gt; Tensor | dict[str, Tensor] | None:\n    \"\"\"\n    Calculates the loss using the criterion if in train or validation mode.\n\n    Args:\n        input: The input data.\n        target: The target data.\n        pred: The model predictions.\n\n    Returns:\n        The calculated loss or None if not in train/val mode.\n\n    Raises:\n        ValueError: If criterion is not specified in train/val mode or if loss dict is missing 'total' key.\n    \"\"\"\n    loss = None\n    if self.mode in [Mode.TRAIN, Mode.VAL]:\n        if self.criterion is None:\n            raise ValueError(\"Please specify 'system.criterion' in the config.\")\n\n        adapters = getattr(self.adapters, self.mode)\n        loss = adapters.criterion(self.criterion, input, target, pred)\n\n        if isinstance(loss, dict) and \"total\" not in loss:\n            raise ValueError(\n                \"The loss dictionary must include a 'total' key that combines all sublosses. \"\n                \"Example: {'total': combined_loss, 'subloss1': loss1, ...}\"\n            )\n    return loss\n</code></pre>"},{"location":"reference/#lighter.System._calculate_metrics","title":"<code>_calculate_metrics(input, target, pred)</code>","text":"<p>Calculates the metrics if not in predict mode.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The model predictions.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The calculated metrics or None if in predict mode or no metrics specified.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _calculate_metrics(self, input: Any, target: Any, pred: Any) -&gt; Any | None:\n    \"\"\"\n    Calculates the metrics if not in predict mode.\n\n    Args:\n        input: The input data.\n        target: The target data.\n        pred: The model predictions.\n\n    Returns:\n        The calculated metrics or None if in predict mode or no metrics specified.\n    \"\"\"\n    if self.mode == Mode.PREDICT or self.metrics[self.mode] is None:\n        return None\n\n    adapters = getattr(self.adapters, self.mode)\n    metrics = adapters.metrics(self.metrics[self.mode], input, target, pred)\n    return metrics\n</code></pre>"},{"location":"reference/#lighter.System._log","title":"<code>_log(name, value, on_step=False, on_epoch=False)</code>","text":"<p>Log a key, value pair. Syncs across distributed nodes if <code>on_epoch</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>key to log.</p> required <code>value</code> <code>Any</code> <p>value to log.</p> required <code>on_step</code> <code>bool</code> <p>if True, logs on step.</p> <code>False</code> <code>on_epoch</code> <code>bool</code> <p>if True, logs on epoch with sync_dist=True.</p> <code>False</code> Source code in <code>src/lighter/system.py</code> <pre><code>def _log(self, name: str, value: Any, on_step: bool = False, on_epoch: bool = False) -&gt; None:\n    \"\"\"Log a key, value pair. Syncs across distributed nodes if `on_epoch` is True.\n\n    Args:\n        name (str): key to log.\n        value (Any): value to log.\n        on_step (bool, optional): if True, logs on step.\n        on_epoch (bool, optional): if True, logs on epoch with sync_dist=True.\n    \"\"\"\n    batch_size = getattr(self.dataloaders, self.mode).batch_size\n    self.log(name, value, logger=True, batch_size=batch_size, on_step=on_step, on_epoch=on_epoch, sync_dist=on_epoch)\n</code></pre>"},{"location":"reference/#lighter.System._log_stats","title":"<code>_log_stats(loss, metrics, batch_idx)</code>","text":"<p>Logs the loss, metrics, and optimizer statistics.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor | dict[str, Tensor]</code> <p>The calculated loss.</p> required <code>metrics</code> <code>MetricCollection</code> <p>The calculated metrics.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>src/lighter/system.py</code> <pre><code>def _log_stats(self, loss: Tensor | dict[str, Tensor], metrics: MetricCollection, batch_idx: int) -&gt; None:\n    \"\"\"\n    Logs the loss, metrics, and optimizer statistics.\n\n    Args:\n        loss: The calculated loss.\n        metrics: The calculated metrics.\n        batch_idx: The index of the batch.\n    \"\"\"\n    if self.trainer.logger is None:\n        return\n\n    # Loss\n    if loss is not None:\n        if not isinstance(loss, dict):\n            self._log(f\"{self.mode}/{Data.LOSS}/{Data.STEP}\", loss, on_step=True)\n            self._log(f\"{self.mode}/{Data.LOSS}/{Data.EPOCH}\", loss, on_epoch=True)\n        else:\n            for name, subloss in loss.items():\n                self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.STEP}\", subloss, on_step=True)\n                self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.EPOCH}\", subloss, on_epoch=True)\n\n    # Metrics\n    if metrics is not None:\n        for name, metric in metrics.items():\n            self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.STEP}\", metric, on_step=True)\n            self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.EPOCH}\", metric, on_epoch=True)\n\n    # Optimizer's lr, momentum, beta. Logged in train mode and once per epoch.\n    if self.mode == Mode.TRAIN and batch_idx == 0:\n        for name, optimizer_stat in get_optimizer_stats(self.optimizer).items():\n            self._log(f\"{self.mode}/{name}\", optimizer_stat, on_epoch=True)\n</code></pre>"},{"location":"reference/#lighter.System._on_mode_end","title":"<code>_on_mode_end()</code>","text":"<p>Resets the mode at the end of a phase.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _on_mode_end(self) -&gt; None:\n    \"\"\"\n    Resets the mode at the end of a phase.\n    \"\"\"\n    self.mode = None\n</code></pre>"},{"location":"reference/#lighter.System._on_mode_start","title":"<code>_on_mode_start(mode)</code>","text":"<p>Sets the current mode at the start of a phase.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str | None</code> <p>The mode to set (train, val, test, or predict).</p> required Source code in <code>src/lighter/system.py</code> <pre><code>def _on_mode_start(self, mode: str | None) -&gt; None:\n    \"\"\"\n    Sets the current mode at the start of a phase.\n\n    Args:\n        mode: The mode to set (train, val, test, or predict).\n    \"\"\"\n    self.mode = mode\n</code></pre>"},{"location":"reference/#lighter.System._prepare_batch","title":"<code>_prepare_batch(batch)</code>","text":"<p>Prepares the batch data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>The input batch dictionary.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Any, Any, Any]</code> <p>A tuple containing (input, target, identifier).</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _prepare_batch(self, batch: dict) -&gt; tuple[Any, Any, Any]:\n    \"\"\"\n    Prepares the batch data.\n\n    Args:\n        batch: The input batch dictionary.\n\n    Returns:\n        tuple: A tuple containing (input, target, identifier).\n    \"\"\"\n    adapters = getattr(self.adapters, self.mode)\n    input, target, identifier = adapters.batch(batch)\n    return input, target, identifier\n</code></pre>"},{"location":"reference/#lighter.System._prepare_output","title":"<code>_prepare_output(identifier, input, target, pred, loss, metrics)</code>","text":"<p>Prepares the data to be returned by the step function to callbacks.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Any</code> <p>The batch identifier.</p> required <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The model predictions.</p> required <code>loss</code> <code>Tensor | dict[str, Tensor] | None</code> <p>The calculated loss.</p> required <code>metrics</code> <code>Any | None</code> <p>The calculated metrics.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>A dictionary containing all the step information.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _prepare_output(\n    self,\n    identifier: Any,\n    input: Any,\n    target: Any,\n    pred: Any,\n    loss: Tensor | dict[str, Tensor] | None,\n    metrics: Any | None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepares the data to be returned by the step function to callbacks.\n\n    Args:\n        identifier: The batch identifier.\n        input: The input data.\n        target: The target data.\n        pred: The model predictions.\n        loss: The calculated loss.\n        metrics: The calculated metrics.\n\n    Returns:\n        dict: A dictionary containing all the step information.\n    \"\"\"\n    adapters = getattr(self.adapters, self.mode)\n    input, target, pred = adapters.logging(input, target, pred)\n    return {\n        Data.IDENTIFIER: identifier,\n        Data.INPUT: input,\n        Data.TARGET: target,\n        Data.PRED: pred,\n        Data.LOSS: loss,\n        Data.METRICS: metrics,\n        Data.STEP: self.global_step,\n        Data.EPOCH: self.current_epoch,\n    }\n</code></pre>"},{"location":"reference/#lighter.System._setup_mode_hooks","title":"<code>_setup_mode_hooks()</code>","text":"<p>Sets up the training, validation, testing, and prediction hooks based on defined dataloaders.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _setup_mode_hooks(self):\n    \"\"\"\n    Sets up the training, validation, testing, and prediction hooks based on defined dataloaders.\n    \"\"\"\n    if self.dataloaders.train is not None:\n        self.training_step = self._step\n        self.train_dataloader = lambda: self.dataloaders.train\n        self.on_train_start = lambda: self._on_mode_start(Mode.TRAIN)\n        self.on_train_end = self._on_mode_end\n    if self.dataloaders.val is not None:\n        self.validation_step = self._step\n        self.val_dataloader = lambda: self.dataloaders.val\n        self.on_validation_start = lambda: self._on_mode_start(Mode.VAL)\n        self.on_validation_end = self._on_mode_end\n    if self.dataloaders.test is not None:\n        self.test_step = self._step\n        self.test_dataloader = lambda: self.dataloaders.test\n        self.on_test_start = lambda: self._on_mode_start(Mode.TEST)\n        self.on_test_end = self._on_mode_end\n    if self.dataloaders.predict is not None:\n        self.predict_step = self._step\n        self.predict_dataloader = lambda: self.dataloaders.predict\n        self.on_predict_start = lambda: self._on_mode_start(Mode.PREDICT)\n        self.on_predict_end = self._on_mode_end\n</code></pre>"},{"location":"reference/#lighter.System._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Performs a step in the specified mode, processing the batch and calculating loss and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>The batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:     dict or Any: For predict step, returns prediction only. For other steps,     returns dict with loss, metrics, input, target, pred, and identifier. Loss is None     for test step, metrics is None if unspecified.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _step(self, batch: dict, batch_idx: int) -&gt; dict[str, Any] | Any:\n    \"\"\"\n    Performs a step in the specified mode, processing the batch and calculating loss and metrics.\n\n    Args:\n        batch: The batch of data.\n        batch_idx: The index of the batch.\n    Returns:\n        dict or Any: For predict step, returns prediction only. For other steps,\n        returns dict with loss, metrics, input, target, pred, and identifier. Loss is None\n        for test step, metrics is None if unspecified.\n    \"\"\"\n    input, target, identifier = self._prepare_batch(batch)\n    pred = self.forward(input)\n\n    loss = self._calculate_loss(input, target, pred)\n    metrics = self._calculate_metrics(input, target, pred)\n\n    self._log_stats(loss, metrics, batch_idx)\n    output = self._prepare_output(identifier, input, target, pred, loss, metrics)\n    return output\n</code></pre>"},{"location":"reference/#lighter.System.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizers and learning rate schedulers.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the optimizer and scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If optimizer is not specified.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"\n    Configures the optimizers and learning rate schedulers.\n\n    Returns:\n        dict: A dictionary containing the optimizer and scheduler.\n\n    Raises:\n        ValueError: If optimizer is not specified.\n    \"\"\"\n    if self.optimizer is None:\n        raise ValueError(\"Please specify 'system.optimizer' in the config.\")\n    if self.scheduler is None:\n        return {\"optimizer\": self.optimizer}\n    else:\n        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}\n</code></pre>"},{"location":"reference/#lighter.System.forward","title":"<code>forward(input)</code>","text":"<p>Forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The model's output.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def forward(self, input: Any) -&gt; Any:\n    \"\"\"\n    Forward pass through the model.\n\n    Args:\n        input: The input data.\n\n    Returns:\n        Any: The model's output.\n    \"\"\"\n\n    # Pass `epoch` and/or `step` argument to forward if it accepts them\n    kwargs = {}\n    if hasarg(self.model.forward, Data.EPOCH):\n        kwargs[Data.EPOCH] = self.current_epoch\n    if hasarg(self.model.forward, Data.STEP):\n        kwargs[Data.STEP] = self.global_step\n\n    # Predict. Use inferer if available in val, test, and predict modes.\n    if self.inferer and self.mode in [Mode.VAL, Mode.TEST, Mode.PREDICT]:\n        return self.inferer(input, self.model, **kwargs)\n    return self.model(input, **kwargs)\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>lighter<ul> <li>adapters</li> <li>callbacks<ul> <li>freezer</li> <li>utils</li> <li>writer<ul> <li>base</li> <li>file</li> <li>table</li> </ul> </li> </ul> </li> <li>engine<ul> <li>config</li> <li>resolver</li> <li>runner</li> <li>schema</li> </ul> </li> <li>system</li> <li>utils<ul> <li>data</li> <li>dynamic_imports</li> <li>logging</li> <li>misc</li> <li>model</li> <li>patches</li> <li>types<ul> <li>containers</li> <li>enums</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/adapters/","title":"adapters","text":""},{"location":"reference/adapters/#lighter.adapters.BatchAdapter","title":"<code>BatchAdapter</code>","text":"Source code in <code>src/lighter/adapters.py</code> <pre><code>class BatchAdapter:\n    def __init__(\n        self,\n        input_accessor: int | str | Callable,\n        target_accessor: int | str | Callable | None = None,\n        identifier_accessor: int | str | Callable | None = None,\n    ):\n        \"\"\"\n        Initializes BatchAdapter with accessors for input, target, and identifier.\n\n        Args:\n            input_accessor: Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries),\n                a callable (custom batch processing).\n            target_accessor: Accessor for the target data. Can be an index (for lists/tuples),\n                             a key (for dictionaries), or a callable (for custom batch processing).\n            identifier_accessor: Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries),\n                a callable (custom batch processing), or None if no identifier is present.\n        \"\"\"\n        self.input_accessor = input_accessor\n        self.target_accessor = target_accessor\n        self.identifier_accessor = identifier_accessor\n\n    def __call__(self, batch: Any) -&gt; tuple[Any, Any, Any]:\n        \"\"\"\n        Accesses the identifier, input, and target data from the batch.\n\n        Args:\n            batch: The batch data from which to extract information.\n\n        Returns:\n            A tuple containing (identifier, input, target).\n\n        Raises:\n            ValueError: If accessors are invalid for the provided batch structure.\n        \"\"\"\n        input = self._access_value(batch, self.input_accessor)\n        target = self._access_value(batch, self.target_accessor)\n        identifier = self._access_value(batch, self.identifier_accessor)\n        return input, target, identifier\n\n    def _access_value(self, data: Any, accessor: int | str | Callable) -&gt; Any:\n        \"\"\"\n        Accesses a value from the data using the provided accessor.\n\n        Args:\n            data: The data to access the value from.\n            accessor: The accessor to use. Can be an index (for lists/tuples),\n                      a key (for dictionaries), or a callable.\n\n        Returns:\n            The accessed value.\n\n        Raises:\n            ValueError: If the accessor type or data structure is invalid.\n        \"\"\"\n        if accessor is None:\n            return None\n        elif isinstance(accessor, int) and isinstance(data, (tuple, list)):\n            return data[accessor]\n        elif isinstance(accessor, str) and isinstance(data, dict):\n            return data[accessor]\n        elif callable(accessor):\n            return accessor(data)\n        else:\n            raise ValueError(f\"Invalid accessor {accessor} of type {type(accessor)} for data type {type(data)}.\")\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.BatchAdapter.__call__","title":"<code>__call__(batch)</code>","text":"<p>Accesses the identifier, input, and target data from the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch data from which to extract information.</p> required <p>Returns:</p> Type Description <code>tuple[Any, Any, Any]</code> <p>A tuple containing (identifier, input, target).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If accessors are invalid for the provided batch structure.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def __call__(self, batch: Any) -&gt; tuple[Any, Any, Any]:\n    \"\"\"\n    Accesses the identifier, input, and target data from the batch.\n\n    Args:\n        batch: The batch data from which to extract information.\n\n    Returns:\n        A tuple containing (identifier, input, target).\n\n    Raises:\n        ValueError: If accessors are invalid for the provided batch structure.\n    \"\"\"\n    input = self._access_value(batch, self.input_accessor)\n    target = self._access_value(batch, self.target_accessor)\n    identifier = self._access_value(batch, self.identifier_accessor)\n    return input, target, identifier\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.BatchAdapter.__init__","title":"<code>__init__(input_accessor, target_accessor=None, identifier_accessor=None)</code>","text":"<p>Initializes BatchAdapter with accessors for input, target, and identifier.</p> <p>Parameters:</p> Name Type Description Default <code>input_accessor</code> <code>int | str | Callable</code> <p>Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries), a callable (custom batch processing).</p> required <code>target_accessor</code> <code>int | str | Callable | None</code> <p>Accessor for the target data. Can be an index (for lists/tuples),              a key (for dictionaries), or a callable (for custom batch processing).</p> <code>None</code> <code>identifier_accessor</code> <code>int | str | Callable | None</code> <p>Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries), a callable (custom batch processing), or None if no identifier is present.</p> <code>None</code> Source code in <code>src/lighter/adapters.py</code> <pre><code>def __init__(\n    self,\n    input_accessor: int | str | Callable,\n    target_accessor: int | str | Callable | None = None,\n    identifier_accessor: int | str | Callable | None = None,\n):\n    \"\"\"\n    Initializes BatchAdapter with accessors for input, target, and identifier.\n\n    Args:\n        input_accessor: Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries),\n            a callable (custom batch processing).\n        target_accessor: Accessor for the target data. Can be an index (for lists/tuples),\n                         a key (for dictionaries), or a callable (for custom batch processing).\n        identifier_accessor: Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries),\n            a callable (custom batch processing), or None if no identifier is present.\n    \"\"\"\n    self.input_accessor = input_accessor\n    self.target_accessor = target_accessor\n    self.identifier_accessor = identifier_accessor\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.BatchAdapter._access_value","title":"<code>_access_value(data, accessor)</code>","text":"<p>Accesses a value from the data using the provided accessor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to access the value from.</p> required <code>accessor</code> <code>int | str | Callable</code> <p>The accessor to use. Can be an index (for lists/tuples),       a key (for dictionaries), or a callable.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The accessed value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the accessor type or data structure is invalid.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def _access_value(self, data: Any, accessor: int | str | Callable) -&gt; Any:\n    \"\"\"\n    Accesses a value from the data using the provided accessor.\n\n    Args:\n        data: The data to access the value from.\n        accessor: The accessor to use. Can be an index (for lists/tuples),\n                  a key (for dictionaries), or a callable.\n\n    Returns:\n        The accessed value.\n\n    Raises:\n        ValueError: If the accessor type or data structure is invalid.\n    \"\"\"\n    if accessor is None:\n        return None\n    elif isinstance(accessor, int) and isinstance(data, (tuple, list)):\n        return data[accessor]\n    elif isinstance(accessor, str) and isinstance(data, dict):\n        return data[accessor]\n    elif callable(accessor):\n        return accessor(data)\n    else:\n        raise ValueError(f\"Invalid accessor {accessor} of type {type(accessor)} for data type {type(data)}.\")\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.CriterionAdapter","title":"<code>CriterionAdapter</code>","text":"<p>               Bases: <code>_ArgumentsAndTransformsAdapter</code></p> <p>This adapter processes and transforms the input, target, and prediction data, if specified, and forwards them to the specified arguments of a criterion (loss function).</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>class CriterionAdapter(_ArgumentsAndTransformsAdapter):\n    \"\"\"\n    This adapter processes and transforms the input, target, and prediction data, if specified,\n    and forwards them to the specified arguments of a criterion (loss function).\n    \"\"\"\n\n    def __call__(self, criterion: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n        \"\"\"\n        Applies transforms and adapts arguments before calling the provided metric function.\n\n        Args:\n            criterion: The criterion (loss function).\n            input: The input data to transform with `input_transforms` if specified and pass to the metric with\n                the position or argument name specified by `input_argument`.\n            target: The target data to transform with `target_transforms` if specified and pass to the metric with\n                the position or argument name specified by `target_argument`.\n            pred: The prediction data to transform with `pred_transforms` if specified and pass to the metric with\n                the position or argument name specified by `pred_argument`.\n\n        Returns:\n            The result of the metric function call.\n        \"\"\"\n        return super().__call__(criterion, input, target, pred)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.CriterionAdapter.__call__","title":"<code>__call__(criterion, input, target, pred)</code>","text":"<p>Applies transforms and adapts arguments before calling the provided metric function.</p> <p>Parameters:</p> Name Type Description Default <code>criterion</code> <code>Callable</code> <p>The criterion (loss function).</p> required <code>input</code> <code>Any</code> <p>The input data to transform with <code>input_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>input_argument</code>.</p> required <code>target</code> <code>Any</code> <p>The target data to transform with <code>target_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>target_argument</code>.</p> required <code>pred</code> <code>Any</code> <p>The prediction data to transform with <code>pred_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>pred_argument</code>.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the metric function call.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def __call__(self, criterion: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n    \"\"\"\n    Applies transforms and adapts arguments before calling the provided metric function.\n\n    Args:\n        criterion: The criterion (loss function).\n        input: The input data to transform with `input_transforms` if specified and pass to the metric with\n            the position or argument name specified by `input_argument`.\n        target: The target data to transform with `target_transforms` if specified and pass to the metric with\n            the position or argument name specified by `target_argument`.\n        pred: The prediction data to transform with `pred_transforms` if specified and pass to the metric with\n            the position or argument name specified by `pred_argument`.\n\n    Returns:\n        The result of the metric function call.\n    \"\"\"\n    return super().__call__(criterion, input, target, pred)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.LoggingAdapter","title":"<code>LoggingAdapter</code>","text":"<p>               Bases: <code>_TransformsAdapter</code></p> <p>Adapter for applying logging transformations to data.</p> <p>This adapter handles the transformation of input, target, and prediction data specifically for logging purposes. It can preprocess or format the data before logging, ensuring consistency and readability in logs.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>class LoggingAdapter(_TransformsAdapter):\n    \"\"\"\n    Adapter for applying logging transformations to data.\n\n    This adapter handles the transformation of input, target, and prediction data\n    specifically for logging purposes. It can preprocess or format the data before\n    logging, ensuring consistency and readability in logs.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_transforms: list[Callable] | None = None,\n        target_transforms: list[Callable] | None = None,\n        pred_transforms: list[Callable] | None = None,\n    ):\n        super().__init__(input_transforms, target_transforms, pred_transforms)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.MetricsAdapter","title":"<code>MetricsAdapter</code>","text":"<p>               Bases: <code>_ArgumentsAndTransformsAdapter</code></p> <p>This adapter processes and transforms the input, target, and prediction data, if specified, and forwards them to the specified arguments of a metric.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>class MetricsAdapter(_ArgumentsAndTransformsAdapter):\n    \"\"\"\n    This adapter processes and transforms the input, target, and prediction data, if specified,\n    and forwards them to the specified arguments of a metric.\n    \"\"\"\n\n    def __call__(self, metric: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n        \"\"\"\n        Applies transforms and adapts arguments before calling the provided metric function.\n\n        Args:\n            metric: The metric.\n            input: The input data to transform with `input_transforms` if specified and pass to the metric with\n                the position or argument name specified by `input_argument`.\n            target: The target data to transform with `target_transforms` if specified and pass to the metric with\n                the position or argument name specified by `target_argument`.\n            pred: The prediction data to transform with `pred_transforms` if specified and pass to the metric with\n                the position or argument name specified by `pred_argument`.\n\n        Returns:\n            The result of the metric function call.\n        \"\"\"\n        return super().__call__(metric, input, target, pred)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.MetricsAdapter.__call__","title":"<code>__call__(metric, input, target, pred)</code>","text":"<p>Applies transforms and adapts arguments before calling the provided metric function.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Callable</code> <p>The metric.</p> required <code>input</code> <code>Any</code> <p>The input data to transform with <code>input_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>input_argument</code>.</p> required <code>target</code> <code>Any</code> <p>The target data to transform with <code>target_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>target_argument</code>.</p> required <code>pred</code> <code>Any</code> <p>The prediction data to transform with <code>pred_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>pred_argument</code>.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the metric function call.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def __call__(self, metric: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n    \"\"\"\n    Applies transforms and adapts arguments before calling the provided metric function.\n\n    Args:\n        metric: The metric.\n        input: The input data to transform with `input_transforms` if specified and pass to the metric with\n            the position or argument name specified by `input_argument`.\n        target: The target data to transform with `target_transforms` if specified and pass to the metric with\n            the position or argument name specified by `target_argument`.\n        pred: The prediction data to transform with `pred_transforms` if specified and pass to the metric with\n            the position or argument name specified by `pred_argument`.\n\n    Returns:\n        The result of the metric function call.\n    \"\"\"\n    return super().__call__(metric, input, target, pred)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAdapter","title":"<code>_ArgumentsAdapter</code>","text":"<p>Base adapter for adapting arguments to a function based on specified argument names or positions.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>class _ArgumentsAdapter:\n    \"\"\"\n    Base adapter for adapting arguments to a function based on specified argument names or positions.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_argument: int | str | None = None,\n        target_argument: int | str | None = None,\n        pred_argument: int | str | None = None,\n    ):\n        # Ensure that the positionals are consecutive integers.\n        # There cannot be positional 0 and 2, without 1. Same with a positional 1 without 0.\n        positionals = sorted(arg for arg in (input_argument, target_argument, pred_argument) if isinstance(arg, int))\n        if positionals != list(range(len(positionals))):\n            raise ValueError(\"Positional arguments must be consecutive integers starting from 0.\")\n\n        self.input_argument = input_argument\n        self.target_argument = target_argument\n        self.pred_argument = pred_argument\n\n    def __call__(self, input: Any, target: Any, pred: Any) -&gt; tuple[list[Any], dict[str, Any]]:\n        \"\"\"\n        Adapts the input, target, and prediction data to the specified argument positions or names.\n\n        Args:\n            input: The input data to be adapted.\n            target: The target data to be adapted.\n            pred: The prediction data to be adapted.\n\n        Returns:\n            A tuple containing a list of positional arguments and a dictionary of keyword arguments.\n        \"\"\"\n        args = []  # List to store positional arguments\n        kwargs = {}  # Dictionary to store keyword arguments\n\n        # Mapping of argument names to their respective values\n        argument_map = {\"input_argument\": input, \"target_argument\": target, \"pred_argument\": pred}\n\n        # Iterate over the argument map to adapt arguments\n        for arg_name, value in argument_map.items():\n            # Get the position or name of the argument from the instance attributes\n            arg_position = getattr(self, arg_name)\n            if arg_position is not None:\n                if isinstance(arg_position, int):\n                    # Insert the value into the args list at the specified position\n                    args.insert(arg_position, value)\n                elif isinstance(arg_position, str):\n                    # Add the value to the kwargs dictionary with the specified name\n                    kwargs[arg_position] = value\n                else:\n                    # Raise an error if the argument type is invalid\n                    raise ValueError(f\"Invalid {arg_name} type: {type(arg_position)}\")\n\n        # Return the adapted positional and keyword arguments\n        return args, kwargs\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAdapter.__call__","title":"<code>__call__(input, target, pred)</code>","text":"<p>Adapts the input, target, and prediction data to the specified argument positions or names.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data to be adapted.</p> required <code>target</code> <code>Any</code> <p>The target data to be adapted.</p> required <code>pred</code> <code>Any</code> <p>The prediction data to be adapted.</p> required <p>Returns:</p> Type Description <code>tuple[list[Any], dict[str, Any]]</code> <p>A tuple containing a list of positional arguments and a dictionary of keyword arguments.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def __call__(self, input: Any, target: Any, pred: Any) -&gt; tuple[list[Any], dict[str, Any]]:\n    \"\"\"\n    Adapts the input, target, and prediction data to the specified argument positions or names.\n\n    Args:\n        input: The input data to be adapted.\n        target: The target data to be adapted.\n        pred: The prediction data to be adapted.\n\n    Returns:\n        A tuple containing a list of positional arguments and a dictionary of keyword arguments.\n    \"\"\"\n    args = []  # List to store positional arguments\n    kwargs = {}  # Dictionary to store keyword arguments\n\n    # Mapping of argument names to their respective values\n    argument_map = {\"input_argument\": input, \"target_argument\": target, \"pred_argument\": pred}\n\n    # Iterate over the argument map to adapt arguments\n    for arg_name, value in argument_map.items():\n        # Get the position or name of the argument from the instance attributes\n        arg_position = getattr(self, arg_name)\n        if arg_position is not None:\n            if isinstance(arg_position, int):\n                # Insert the value into the args list at the specified position\n                args.insert(arg_position, value)\n            elif isinstance(arg_position, str):\n                # Add the value to the kwargs dictionary with the specified name\n                kwargs[arg_position] = value\n            else:\n                # Raise an error if the argument type is invalid\n                raise ValueError(f\"Invalid {arg_name} type: {type(arg_position)}\")\n\n    # Return the adapted positional and keyword arguments\n    return args, kwargs\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAndTransformsAdapter","title":"<code>_ArgumentsAndTransformsAdapter</code>","text":"<p>               Bases: <code>_ArgumentsAdapter</code>, <code>_TransformsAdapter</code></p> <p>A generic adapter for applying functions (criterion or metrics) to data.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>class _ArgumentsAndTransformsAdapter(_ArgumentsAdapter, _TransformsAdapter):\n    \"\"\"\n    A generic adapter for applying functions (criterion or metrics) to data.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_argument: int | str | None = None,\n        target_argument: int | str | None = None,\n        pred_argument: int | str | None = None,\n        input_transforms: list[Callable] | None = None,\n        target_transforms: list[Callable] | None = None,\n        pred_transforms: list[Callable] | None = None,\n    ):\n        \"\"\"\n        Initializes the Arguments and Transforms Adapter.\n\n        Args:\n            input_argument: Position or name for the input data.\n            target_argument: Position or name for the target data.\n            pred_argument: Position or name for the prediction data.\n            input_transforms: Transforms to apply to the input data.\n            target_transforms: Transforms to apply to the target data.\n            pred_transforms: Transforms to apply to the prediction data.\n\n        Raises:\n            ValueError: If transforms are provided without corresponding argument specifications.\n        \"\"\"\n        # Validate transform arguments\n        if input_argument is None and input_transforms is not None:\n            raise ValueError(\"Input transforms provided but input_argument is None\")\n        if target_argument is None and target_transforms is not None:\n            raise ValueError(\"Target transforms provided but target_argument is None\")\n        if pred_argument is None and pred_transforms is not None:\n            raise ValueError(\"Pred transforms provided but pred_argument is None\")\n\n        _ArgumentsAdapter.__init__(self, input_argument, target_argument, pred_argument)\n        _TransformsAdapter.__init__(self, input_transforms, target_transforms, pred_transforms)\n\n    def __call__(self, fn: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n        \"\"\"\n        Applies transforms and adapts arguments before calling the provided function.\n\n        Args:\n            fn: The function/method to be called (e.g., a loss function or metric).\n            input: The input data.\n            target: The target data.\n            pred: The prediction data.\n\n        Returns:\n            The result of the function call.\n        \"\"\"\n        # Apply the transforms to the input, target, and prediction data\n        input, target, pred = _TransformsAdapter.__call__(self, input, target, pred)\n        # Map the input, target, and prediction data to the function arguments\n        args, kwargs = _ArgumentsAdapter.__call__(self, input, target, pred)\n        # Call the provided function with the adapted arguments\n        return fn(*args, **kwargs)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAndTransformsAdapter.__call__","title":"<code>__call__(fn, input, target, pred)</code>","text":"<p>Applies transforms and adapts arguments before calling the provided function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function/method to be called (e.g., a loss function or metric).</p> required <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The prediction data.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the function call.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def __call__(self, fn: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n    \"\"\"\n    Applies transforms and adapts arguments before calling the provided function.\n\n    Args:\n        fn: The function/method to be called (e.g., a loss function or metric).\n        input: The input data.\n        target: The target data.\n        pred: The prediction data.\n\n    Returns:\n        The result of the function call.\n    \"\"\"\n    # Apply the transforms to the input, target, and prediction data\n    input, target, pred = _TransformsAdapter.__call__(self, input, target, pred)\n    # Map the input, target, and prediction data to the function arguments\n    args, kwargs = _ArgumentsAdapter.__call__(self, input, target, pred)\n    # Call the provided function with the adapted arguments\n    return fn(*args, **kwargs)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAndTransformsAdapter.__init__","title":"<code>__init__(input_argument=None, target_argument=None, pred_argument=None, input_transforms=None, target_transforms=None, pred_transforms=None)</code>","text":"<p>Initializes the Arguments and Transforms Adapter.</p> <p>Parameters:</p> Name Type Description Default <code>input_argument</code> <code>int | str | None</code> <p>Position or name for the input data.</p> <code>None</code> <code>target_argument</code> <code>int | str | None</code> <p>Position or name for the target data.</p> <code>None</code> <code>pred_argument</code> <code>int | str | None</code> <p>Position or name for the prediction data.</p> <code>None</code> <code>input_transforms</code> <code>list[Callable] | None</code> <p>Transforms to apply to the input data.</p> <code>None</code> <code>target_transforms</code> <code>list[Callable] | None</code> <p>Transforms to apply to the target data.</p> <code>None</code> <code>pred_transforms</code> <code>list[Callable] | None</code> <p>Transforms to apply to the prediction data.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If transforms are provided without corresponding argument specifications.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def __init__(\n    self,\n    input_argument: int | str | None = None,\n    target_argument: int | str | None = None,\n    pred_argument: int | str | None = None,\n    input_transforms: list[Callable] | None = None,\n    target_transforms: list[Callable] | None = None,\n    pred_transforms: list[Callable] | None = None,\n):\n    \"\"\"\n    Initializes the Arguments and Transforms Adapter.\n\n    Args:\n        input_argument: Position or name for the input data.\n        target_argument: Position or name for the target data.\n        pred_argument: Position or name for the prediction data.\n        input_transforms: Transforms to apply to the input data.\n        target_transforms: Transforms to apply to the target data.\n        pred_transforms: Transforms to apply to the prediction data.\n\n    Raises:\n        ValueError: If transforms are provided without corresponding argument specifications.\n    \"\"\"\n    # Validate transform arguments\n    if input_argument is None and input_transforms is not None:\n        raise ValueError(\"Input transforms provided but input_argument is None\")\n    if target_argument is None and target_transforms is not None:\n        raise ValueError(\"Target transforms provided but target_argument is None\")\n    if pred_argument is None and pred_transforms is not None:\n        raise ValueError(\"Pred transforms provided but pred_argument is None\")\n\n    _ArgumentsAdapter.__init__(self, input_argument, target_argument, pred_argument)\n    _TransformsAdapter.__init__(self, input_transforms, target_transforms, pred_transforms)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._TransformsAdapter","title":"<code>_TransformsAdapter</code>","text":"<p>Adapter for applying transformations to data.</p> <p>Parameters:</p> Name Type Description Default <code>input_transforms</code> <code>Callable | list[Callable] | None</code> <p>A single or a list of transforms to apply to the input data.</p> <code>None</code> <code>target_transforms</code> <code>Callable | list[Callable] | None</code> <p>A single or a list of transforms to apply to the target data.</p> <code>None</code> <code>pred_transforms</code> <code>Callable | list[Callable] | None</code> <p>A single or a list of transforms to apply to the prediction data.</p> <code>None</code> Source code in <code>src/lighter/adapters.py</code> <pre><code>class _TransformsAdapter:\n    \"\"\"\n    Adapter for applying transformations to data.\n\n    Args:\n        input_transforms: A single or a list of transforms to apply to the input data.\n        target_transforms: A single or a list of transforms to apply to the target data.\n        pred_transforms: A single or a list of transforms to apply to the prediction data.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_transforms: Callable | list[Callable] | None = None,\n        target_transforms: Callable | list[Callable] | None = None,\n        pred_transforms: Callable | list[Callable] | None = None,\n    ):\n        self.input_transforms = input_transforms\n        self.target_transforms = target_transforms\n        self.pred_transforms = pred_transforms\n\n    def __call__(self, input: Any, target: Any, pred: Any) -&gt; tuple[Any, Any, Any]:\n        \"\"\"\n        Applies the specified transforms to the input, target, and prediction data.\n\n        Args:\n            input: The input data.\n            target: The target data.\n            pred: The prediction data.\n\n        Returns:\n            The transformed (input, target, prediction) data.\n        \"\"\"\n        input = self._transform(input, self.input_transforms)\n        target = self._transform(target, self.target_transforms)\n        pred = self._transform(pred, self.pred_transforms)\n        return input, target, pred\n\n    def _transform(self, data: Any, transforms: Callable | list[Callable]) -&gt; Any:\n        \"\"\"\n        Applies a list of transform functions to the data.\n\n        Args:\n            data: The data to be transformed.\n            transforms: A single transform function or a list of functions.\n\n        Returns:\n            The transformed data.\n\n        Raises:\n            ValueError: If any transform is not callable.\n        \"\"\"\n        for transform in ensure_list(transforms):\n            if callable(transform):\n                data = transform(data)\n            else:\n                raise ValueError(f\"Invalid transform type for transform: {transform}\")\n        return data\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._TransformsAdapter.__call__","title":"<code>__call__(input, target, pred)</code>","text":"<p>Applies the specified transforms to the input, target, and prediction data.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The prediction data.</p> required <p>Returns:</p> Type Description <code>tuple[Any, Any, Any]</code> <p>The transformed (input, target, prediction) data.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def __call__(self, input: Any, target: Any, pred: Any) -&gt; tuple[Any, Any, Any]:\n    \"\"\"\n    Applies the specified transforms to the input, target, and prediction data.\n\n    Args:\n        input: The input data.\n        target: The target data.\n        pred: The prediction data.\n\n    Returns:\n        The transformed (input, target, prediction) data.\n    \"\"\"\n    input = self._transform(input, self.input_transforms)\n    target = self._transform(target, self.target_transforms)\n    pred = self._transform(pred, self.pred_transforms)\n    return input, target, pred\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._TransformsAdapter._transform","title":"<code>_transform(data, transforms)</code>","text":"<p>Applies a list of transform functions to the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to be transformed.</p> required <code>transforms</code> <code>Callable | list[Callable]</code> <p>A single transform function or a list of functions.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The transformed data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any transform is not callable.</p> Source code in <code>src/lighter/adapters.py</code> <pre><code>def _transform(self, data: Any, transforms: Callable | list[Callable]) -&gt; Any:\n    \"\"\"\n    Applies a list of transform functions to the data.\n\n    Args:\n        data: The data to be transformed.\n        transforms: A single transform function or a list of functions.\n\n    Returns:\n        The transformed data.\n\n    Raises:\n        ValueError: If any transform is not callable.\n    \"\"\"\n    for transform in ensure_list(transforms):\n        if callable(transform):\n            data = transform(data)\n        else:\n            raise ValueError(f\"Invalid transform type for transform: {transform}\")\n    return data\n</code></pre>"},{"location":"reference/system/","title":"system","text":"<p>This module defines the System class, which encapsulates the components of a deep learning system, including the model, optimizer, datasets, and more. It extends PyTorch Lightning's LightningModule.</p>"},{"location":"reference/system/#lighter.system.System","title":"<code>System</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>System encapsulates the components of a deep learning system, extending PyTorch Lightning's LightningModule.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model.</p> required <code>optimizer</code> <code>Optimizer | None</code> <p>Optimizer.</p> <code>None</code> <code>scheduler</code> <code>LRScheduler | None</code> <p>Learning rate scheduler.</p> <code>None</code> <code>criterion</code> <code>Callable | None</code> <p>Criterion (loss) function.</p> <code>None</code> <code>metrics</code> <code>dict[str, Metric | list[Metric] | dict[str, Metric]] | None</code> <p>Metrics for train, val, and test. Supports a single/list/dict of <code>torchmetrics</code> metrics.</p> <code>None</code> <code>dataloaders</code> <code>dict[str, DataLoader]</code> <p>Dataloaders for train, val, test, and predict.</p> required <code>adapters</code> <code>dict[str, Callable] | None</code> <p>Adapters for batch preparation, criterion argument adaptation, metrics argument adaptation, and logging data adaptation.</p> <code>None</code> <code>inferer</code> <code>Callable | None</code> <p>Inferer to use in val/test/predict modes. See MONAI inferers for more details: (https://docs.monai.io/en/stable/inferers.html).</p> <code>None</code> Source code in <code>src/lighter/system.py</code> <pre><code>class System(pl.LightningModule):\n    \"\"\"\n    System encapsulates the components of a deep learning system, extending PyTorch Lightning's LightningModule.\n\n    Args:\n        model: Model.\n        optimizer: Optimizer.\n        scheduler: Learning rate scheduler.\n        criterion: Criterion (loss) function.\n        metrics: Metrics for train, val, and test. Supports a single/list/dict of `torchmetrics` metrics.\n        dataloaders: Dataloaders for train, val, test, and predict.\n        adapters: Adapters for batch preparation, criterion argument adaptation, metrics argument adaptation, and logging data adaptation.\n        inferer: Inferer to use in val/test/predict modes.\n            See MONAI inferers for more details: (https://docs.monai.io/en/stable/inferers.html).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Module,\n        dataloaders: dict[str, DataLoader],\n        optimizer: Optimizer | None = None,\n        scheduler: LRScheduler | None = None,\n        criterion: Callable | None = None,\n        metrics: dict[str, Metric | list[Metric] | dict[str, Metric]] | None = None,\n        adapters: dict[str, Callable] | None = None,\n        inferer: Callable | None = None,\n    ) -&gt; None:\n        super().__init__()\n\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.inferer = inferer\n\n        #  Containers\n        self.dataloaders = DataLoaders(**(dataloaders or {}))\n        self.metrics = Metrics(**(metrics or {}))\n        self.adapters = Adapters(**(adapters or {}))\n\n        # Turn metrics container into a ModuleDict to register them properly.\n        self.metrics = PatchedModuleDict(asdict(self.metrics))\n\n        self.mode = None\n        self._setup_mode_hooks()\n\n    def _step(self, batch: dict, batch_idx: int) -&gt; dict[str, Any] | Any:\n        \"\"\"\n        Performs a step in the specified mode, processing the batch and calculating loss and metrics.\n\n        Args:\n            batch: The batch of data.\n            batch_idx: The index of the batch.\n        Returns:\n            dict or Any: For predict step, returns prediction only. For other steps,\n            returns dict with loss, metrics, input, target, pred, and identifier. Loss is None\n            for test step, metrics is None if unspecified.\n        \"\"\"\n        input, target, identifier = self._prepare_batch(batch)\n        pred = self.forward(input)\n\n        loss = self._calculate_loss(input, target, pred)\n        metrics = self._calculate_metrics(input, target, pred)\n\n        self._log_stats(loss, metrics, batch_idx)\n        output = self._prepare_output(identifier, input, target, pred, loss, metrics)\n        return output\n\n    def _prepare_batch(self, batch: dict) -&gt; tuple[Any, Any, Any]:\n        \"\"\"\n        Prepares the batch data.\n\n        Args:\n            batch: The input batch dictionary.\n\n        Returns:\n            tuple: A tuple containing (input, target, identifier).\n        \"\"\"\n        adapters = getattr(self.adapters, self.mode)\n        input, target, identifier = adapters.batch(batch)\n        return input, target, identifier\n\n    def forward(self, input: Any) -&gt; Any:\n        \"\"\"\n        Forward pass through the model.\n\n        Args:\n            input: The input data.\n\n        Returns:\n            Any: The model's output.\n        \"\"\"\n\n        # Pass `epoch` and/or `step` argument to forward if it accepts them\n        kwargs = {}\n        if hasarg(self.model.forward, Data.EPOCH):\n            kwargs[Data.EPOCH] = self.current_epoch\n        if hasarg(self.model.forward, Data.STEP):\n            kwargs[Data.STEP] = self.global_step\n\n        # Predict. Use inferer if available in val, test, and predict modes.\n        if self.inferer and self.mode in [Mode.VAL, Mode.TEST, Mode.PREDICT]:\n            return self.inferer(input, self.model, **kwargs)\n        return self.model(input, **kwargs)\n\n    def _calculate_loss(self, input: Any, target: Any, pred: Any) -&gt; Tensor | dict[str, Tensor] | None:\n        \"\"\"\n        Calculates the loss using the criterion if in train or validation mode.\n\n        Args:\n            input: The input data.\n            target: The target data.\n            pred: The model predictions.\n\n        Returns:\n            The calculated loss or None if not in train/val mode.\n\n        Raises:\n            ValueError: If criterion is not specified in train/val mode or if loss dict is missing 'total' key.\n        \"\"\"\n        loss = None\n        if self.mode in [Mode.TRAIN, Mode.VAL]:\n            if self.criterion is None:\n                raise ValueError(\"Please specify 'system.criterion' in the config.\")\n\n            adapters = getattr(self.adapters, self.mode)\n            loss = adapters.criterion(self.criterion, input, target, pred)\n\n            if isinstance(loss, dict) and \"total\" not in loss:\n                raise ValueError(\n                    \"The loss dictionary must include a 'total' key that combines all sublosses. \"\n                    \"Example: {'total': combined_loss, 'subloss1': loss1, ...}\"\n                )\n        return loss\n\n    def _calculate_metrics(self, input: Any, target: Any, pred: Any) -&gt; Any | None:\n        \"\"\"\n        Calculates the metrics if not in predict mode.\n\n        Args:\n            input: The input data.\n            target: The target data.\n            pred: The model predictions.\n\n        Returns:\n            The calculated metrics or None if in predict mode or no metrics specified.\n        \"\"\"\n        if self.mode == Mode.PREDICT or self.metrics[self.mode] is None:\n            return None\n\n        adapters = getattr(self.adapters, self.mode)\n        metrics = adapters.metrics(self.metrics[self.mode], input, target, pred)\n        return metrics\n\n    def _log_stats(self, loss: Tensor | dict[str, Tensor], metrics: MetricCollection, batch_idx: int) -&gt; None:\n        \"\"\"\n        Logs the loss, metrics, and optimizer statistics.\n\n        Args:\n            loss: The calculated loss.\n            metrics: The calculated metrics.\n            batch_idx: The index of the batch.\n        \"\"\"\n        if self.trainer.logger is None:\n            return\n\n        # Loss\n        if loss is not None:\n            if not isinstance(loss, dict):\n                self._log(f\"{self.mode}/{Data.LOSS}/{Data.STEP}\", loss, on_step=True)\n                self._log(f\"{self.mode}/{Data.LOSS}/{Data.EPOCH}\", loss, on_epoch=True)\n            else:\n                for name, subloss in loss.items():\n                    self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.STEP}\", subloss, on_step=True)\n                    self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.EPOCH}\", subloss, on_epoch=True)\n\n        # Metrics\n        if metrics is not None:\n            for name, metric in metrics.items():\n                self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.STEP}\", metric, on_step=True)\n                self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.EPOCH}\", metric, on_epoch=True)\n\n        # Optimizer's lr, momentum, beta. Logged in train mode and once per epoch.\n        if self.mode == Mode.TRAIN and batch_idx == 0:\n            for name, optimizer_stat in get_optimizer_stats(self.optimizer).items():\n                self._log(f\"{self.mode}/{name}\", optimizer_stat, on_epoch=True)\n\n    def _log(self, name: str, value: Any, on_step: bool = False, on_epoch: bool = False) -&gt; None:\n        \"\"\"Log a key, value pair. Syncs across distributed nodes if `on_epoch` is True.\n\n        Args:\n            name (str): key to log.\n            value (Any): value to log.\n            on_step (bool, optional): if True, logs on step.\n            on_epoch (bool, optional): if True, logs on epoch with sync_dist=True.\n        \"\"\"\n        batch_size = getattr(self.dataloaders, self.mode).batch_size\n        self.log(name, value, logger=True, batch_size=batch_size, on_step=on_step, on_epoch=on_epoch, sync_dist=on_epoch)\n\n    def _prepare_output(\n        self,\n        identifier: Any,\n        input: Any,\n        target: Any,\n        pred: Any,\n        loss: Tensor | dict[str, Tensor] | None,\n        metrics: Any | None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Prepares the data to be returned by the step function to callbacks.\n\n        Args:\n            identifier: The batch identifier.\n            input: The input data.\n            target: The target data.\n            pred: The model predictions.\n            loss: The calculated loss.\n            metrics: The calculated metrics.\n\n        Returns:\n            dict: A dictionary containing all the step information.\n        \"\"\"\n        adapters = getattr(self.adapters, self.mode)\n        input, target, pred = adapters.logging(input, target, pred)\n        return {\n            Data.IDENTIFIER: identifier,\n            Data.INPUT: input,\n            Data.TARGET: target,\n            Data.PRED: pred,\n            Data.LOSS: loss,\n            Data.METRICS: metrics,\n            Data.STEP: self.global_step,\n            Data.EPOCH: self.current_epoch,\n        }\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"\n        Configures the optimizers and learning rate schedulers.\n\n        Returns:\n            dict: A dictionary containing the optimizer and scheduler.\n\n        Raises:\n            ValueError: If optimizer is not specified.\n        \"\"\"\n        if self.optimizer is None:\n            raise ValueError(\"Please specify 'system.optimizer' in the config.\")\n        if self.scheduler is None:\n            return {\"optimizer\": self.optimizer}\n        else:\n            return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}\n\n    def _setup_mode_hooks(self):\n        \"\"\"\n        Sets up the training, validation, testing, and prediction hooks based on defined dataloaders.\n        \"\"\"\n        if self.dataloaders.train is not None:\n            self.training_step = self._step\n            self.train_dataloader = lambda: self.dataloaders.train\n            self.on_train_start = lambda: self._on_mode_start(Mode.TRAIN)\n            self.on_train_end = self._on_mode_end\n        if self.dataloaders.val is not None:\n            self.validation_step = self._step\n            self.val_dataloader = lambda: self.dataloaders.val\n            self.on_validation_start = lambda: self._on_mode_start(Mode.VAL)\n            self.on_validation_end = self._on_mode_end\n        if self.dataloaders.test is not None:\n            self.test_step = self._step\n            self.test_dataloader = lambda: self.dataloaders.test\n            self.on_test_start = lambda: self._on_mode_start(Mode.TEST)\n            self.on_test_end = self._on_mode_end\n        if self.dataloaders.predict is not None:\n            self.predict_step = self._step\n            self.predict_dataloader = lambda: self.dataloaders.predict\n            self.on_predict_start = lambda: self._on_mode_start(Mode.PREDICT)\n            self.on_predict_end = self._on_mode_end\n\n    def _on_mode_start(self, mode: str | None) -&gt; None:\n        \"\"\"\n        Sets the current mode at the start of a phase.\n\n        Args:\n            mode: The mode to set (train, val, test, or predict).\n        \"\"\"\n        self.mode = mode\n\n    def _on_mode_end(self) -&gt; None:\n        \"\"\"\n        Resets the mode at the end of a phase.\n        \"\"\"\n        self.mode = None\n\n    @property\n    def learning_rate(self) -&gt; float:\n        \"\"\"\n        Gets the learning rate of the optimizer.\n\n        Returns:\n            float: The learning rate.\n\n        Raises:\n            ValueError: If there are multiple optimizer parameter groups.\n        \"\"\"\n        if len(self.optimizer.param_groups) &gt; 1:\n            raise ValueError(\"The learning rate is not available when there are multiple optimizer parameter groups.\")\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    @learning_rate.setter\n    def learning_rate(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the learning rate of the optimizer.\n\n        Args:\n            value: The new learning rate.\n\n        Raises:\n            ValueError: If there are multiple optimizer parameter groups.\n        \"\"\"\n        if len(self.optimizer.param_groups) &gt; 1:\n            raise ValueError(\"The learning rate is not available when there are multiple optimizer parameter groups.\")\n        self.optimizer.param_groups[0][\"lr\"] = value\n</code></pre>"},{"location":"reference/system/#lighter.system.System.learning_rate","title":"<code>learning_rate</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the learning rate of the optimizer.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The learning rate.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are multiple optimizer parameter groups.</p>"},{"location":"reference/system/#lighter.system.System._calculate_loss","title":"<code>_calculate_loss(input, target, pred)</code>","text":"<p>Calculates the loss using the criterion if in train or validation mode.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The model predictions.</p> required <p>Returns:</p> Type Description <code>Tensor | dict[str, Tensor] | None</code> <p>The calculated loss or None if not in train/val mode.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If criterion is not specified in train/val mode or if loss dict is missing 'total' key.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _calculate_loss(self, input: Any, target: Any, pred: Any) -&gt; Tensor | dict[str, Tensor] | None:\n    \"\"\"\n    Calculates the loss using the criterion if in train or validation mode.\n\n    Args:\n        input: The input data.\n        target: The target data.\n        pred: The model predictions.\n\n    Returns:\n        The calculated loss or None if not in train/val mode.\n\n    Raises:\n        ValueError: If criterion is not specified in train/val mode or if loss dict is missing 'total' key.\n    \"\"\"\n    loss = None\n    if self.mode in [Mode.TRAIN, Mode.VAL]:\n        if self.criterion is None:\n            raise ValueError(\"Please specify 'system.criterion' in the config.\")\n\n        adapters = getattr(self.adapters, self.mode)\n        loss = adapters.criterion(self.criterion, input, target, pred)\n\n        if isinstance(loss, dict) and \"total\" not in loss:\n            raise ValueError(\n                \"The loss dictionary must include a 'total' key that combines all sublosses. \"\n                \"Example: {'total': combined_loss, 'subloss1': loss1, ...}\"\n            )\n    return loss\n</code></pre>"},{"location":"reference/system/#lighter.system.System._calculate_metrics","title":"<code>_calculate_metrics(input, target, pred)</code>","text":"<p>Calculates the metrics if not in predict mode.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The model predictions.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The calculated metrics or None if in predict mode or no metrics specified.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _calculate_metrics(self, input: Any, target: Any, pred: Any) -&gt; Any | None:\n    \"\"\"\n    Calculates the metrics if not in predict mode.\n\n    Args:\n        input: The input data.\n        target: The target data.\n        pred: The model predictions.\n\n    Returns:\n        The calculated metrics or None if in predict mode or no metrics specified.\n    \"\"\"\n    if self.mode == Mode.PREDICT or self.metrics[self.mode] is None:\n        return None\n\n    adapters = getattr(self.adapters, self.mode)\n    metrics = adapters.metrics(self.metrics[self.mode], input, target, pred)\n    return metrics\n</code></pre>"},{"location":"reference/system/#lighter.system.System._log","title":"<code>_log(name, value, on_step=False, on_epoch=False)</code>","text":"<p>Log a key, value pair. Syncs across distributed nodes if <code>on_epoch</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>key to log.</p> required <code>value</code> <code>Any</code> <p>value to log.</p> required <code>on_step</code> <code>bool</code> <p>if True, logs on step.</p> <code>False</code> <code>on_epoch</code> <code>bool</code> <p>if True, logs on epoch with sync_dist=True.</p> <code>False</code> Source code in <code>src/lighter/system.py</code> <pre><code>def _log(self, name: str, value: Any, on_step: bool = False, on_epoch: bool = False) -&gt; None:\n    \"\"\"Log a key, value pair. Syncs across distributed nodes if `on_epoch` is True.\n\n    Args:\n        name (str): key to log.\n        value (Any): value to log.\n        on_step (bool, optional): if True, logs on step.\n        on_epoch (bool, optional): if True, logs on epoch with sync_dist=True.\n    \"\"\"\n    batch_size = getattr(self.dataloaders, self.mode).batch_size\n    self.log(name, value, logger=True, batch_size=batch_size, on_step=on_step, on_epoch=on_epoch, sync_dist=on_epoch)\n</code></pre>"},{"location":"reference/system/#lighter.system.System._log_stats","title":"<code>_log_stats(loss, metrics, batch_idx)</code>","text":"<p>Logs the loss, metrics, and optimizer statistics.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor | dict[str, Tensor]</code> <p>The calculated loss.</p> required <code>metrics</code> <code>MetricCollection</code> <p>The calculated metrics.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>src/lighter/system.py</code> <pre><code>def _log_stats(self, loss: Tensor | dict[str, Tensor], metrics: MetricCollection, batch_idx: int) -&gt; None:\n    \"\"\"\n    Logs the loss, metrics, and optimizer statistics.\n\n    Args:\n        loss: The calculated loss.\n        metrics: The calculated metrics.\n        batch_idx: The index of the batch.\n    \"\"\"\n    if self.trainer.logger is None:\n        return\n\n    # Loss\n    if loss is not None:\n        if not isinstance(loss, dict):\n            self._log(f\"{self.mode}/{Data.LOSS}/{Data.STEP}\", loss, on_step=True)\n            self._log(f\"{self.mode}/{Data.LOSS}/{Data.EPOCH}\", loss, on_epoch=True)\n        else:\n            for name, subloss in loss.items():\n                self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.STEP}\", subloss, on_step=True)\n                self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.EPOCH}\", subloss, on_epoch=True)\n\n    # Metrics\n    if metrics is not None:\n        for name, metric in metrics.items():\n            self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.STEP}\", metric, on_step=True)\n            self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.EPOCH}\", metric, on_epoch=True)\n\n    # Optimizer's lr, momentum, beta. Logged in train mode and once per epoch.\n    if self.mode == Mode.TRAIN and batch_idx == 0:\n        for name, optimizer_stat in get_optimizer_stats(self.optimizer).items():\n            self._log(f\"{self.mode}/{name}\", optimizer_stat, on_epoch=True)\n</code></pre>"},{"location":"reference/system/#lighter.system.System._on_mode_end","title":"<code>_on_mode_end()</code>","text":"<p>Resets the mode at the end of a phase.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _on_mode_end(self) -&gt; None:\n    \"\"\"\n    Resets the mode at the end of a phase.\n    \"\"\"\n    self.mode = None\n</code></pre>"},{"location":"reference/system/#lighter.system.System._on_mode_start","title":"<code>_on_mode_start(mode)</code>","text":"<p>Sets the current mode at the start of a phase.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str | None</code> <p>The mode to set (train, val, test, or predict).</p> required Source code in <code>src/lighter/system.py</code> <pre><code>def _on_mode_start(self, mode: str | None) -&gt; None:\n    \"\"\"\n    Sets the current mode at the start of a phase.\n\n    Args:\n        mode: The mode to set (train, val, test, or predict).\n    \"\"\"\n    self.mode = mode\n</code></pre>"},{"location":"reference/system/#lighter.system.System._prepare_batch","title":"<code>_prepare_batch(batch)</code>","text":"<p>Prepares the batch data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>The input batch dictionary.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Any, Any, Any]</code> <p>A tuple containing (input, target, identifier).</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _prepare_batch(self, batch: dict) -&gt; tuple[Any, Any, Any]:\n    \"\"\"\n    Prepares the batch data.\n\n    Args:\n        batch: The input batch dictionary.\n\n    Returns:\n        tuple: A tuple containing (input, target, identifier).\n    \"\"\"\n    adapters = getattr(self.adapters, self.mode)\n    input, target, identifier = adapters.batch(batch)\n    return input, target, identifier\n</code></pre>"},{"location":"reference/system/#lighter.system.System._prepare_output","title":"<code>_prepare_output(identifier, input, target, pred, loss, metrics)</code>","text":"<p>Prepares the data to be returned by the step function to callbacks.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Any</code> <p>The batch identifier.</p> required <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The model predictions.</p> required <code>loss</code> <code>Tensor | dict[str, Tensor] | None</code> <p>The calculated loss.</p> required <code>metrics</code> <code>Any | None</code> <p>The calculated metrics.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>A dictionary containing all the step information.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _prepare_output(\n    self,\n    identifier: Any,\n    input: Any,\n    target: Any,\n    pred: Any,\n    loss: Tensor | dict[str, Tensor] | None,\n    metrics: Any | None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepares the data to be returned by the step function to callbacks.\n\n    Args:\n        identifier: The batch identifier.\n        input: The input data.\n        target: The target data.\n        pred: The model predictions.\n        loss: The calculated loss.\n        metrics: The calculated metrics.\n\n    Returns:\n        dict: A dictionary containing all the step information.\n    \"\"\"\n    adapters = getattr(self.adapters, self.mode)\n    input, target, pred = adapters.logging(input, target, pred)\n    return {\n        Data.IDENTIFIER: identifier,\n        Data.INPUT: input,\n        Data.TARGET: target,\n        Data.PRED: pred,\n        Data.LOSS: loss,\n        Data.METRICS: metrics,\n        Data.STEP: self.global_step,\n        Data.EPOCH: self.current_epoch,\n    }\n</code></pre>"},{"location":"reference/system/#lighter.system.System._setup_mode_hooks","title":"<code>_setup_mode_hooks()</code>","text":"<p>Sets up the training, validation, testing, and prediction hooks based on defined dataloaders.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _setup_mode_hooks(self):\n    \"\"\"\n    Sets up the training, validation, testing, and prediction hooks based on defined dataloaders.\n    \"\"\"\n    if self.dataloaders.train is not None:\n        self.training_step = self._step\n        self.train_dataloader = lambda: self.dataloaders.train\n        self.on_train_start = lambda: self._on_mode_start(Mode.TRAIN)\n        self.on_train_end = self._on_mode_end\n    if self.dataloaders.val is not None:\n        self.validation_step = self._step\n        self.val_dataloader = lambda: self.dataloaders.val\n        self.on_validation_start = lambda: self._on_mode_start(Mode.VAL)\n        self.on_validation_end = self._on_mode_end\n    if self.dataloaders.test is not None:\n        self.test_step = self._step\n        self.test_dataloader = lambda: self.dataloaders.test\n        self.on_test_start = lambda: self._on_mode_start(Mode.TEST)\n        self.on_test_end = self._on_mode_end\n    if self.dataloaders.predict is not None:\n        self.predict_step = self._step\n        self.predict_dataloader = lambda: self.dataloaders.predict\n        self.on_predict_start = lambda: self._on_mode_start(Mode.PREDICT)\n        self.on_predict_end = self._on_mode_end\n</code></pre>"},{"location":"reference/system/#lighter.system.System._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Performs a step in the specified mode, processing the batch and calculating loss and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>The batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:     dict or Any: For predict step, returns prediction only. For other steps,     returns dict with loss, metrics, input, target, pred, and identifier. Loss is None     for test step, metrics is None if unspecified.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def _step(self, batch: dict, batch_idx: int) -&gt; dict[str, Any] | Any:\n    \"\"\"\n    Performs a step in the specified mode, processing the batch and calculating loss and metrics.\n\n    Args:\n        batch: The batch of data.\n        batch_idx: The index of the batch.\n    Returns:\n        dict or Any: For predict step, returns prediction only. For other steps,\n        returns dict with loss, metrics, input, target, pred, and identifier. Loss is None\n        for test step, metrics is None if unspecified.\n    \"\"\"\n    input, target, identifier = self._prepare_batch(batch)\n    pred = self.forward(input)\n\n    loss = self._calculate_loss(input, target, pred)\n    metrics = self._calculate_metrics(input, target, pred)\n\n    self._log_stats(loss, metrics, batch_idx)\n    output = self._prepare_output(identifier, input, target, pred, loss, metrics)\n    return output\n</code></pre>"},{"location":"reference/system/#lighter.system.System.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizers and learning rate schedulers.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the optimizer and scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If optimizer is not specified.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"\n    Configures the optimizers and learning rate schedulers.\n\n    Returns:\n        dict: A dictionary containing the optimizer and scheduler.\n\n    Raises:\n        ValueError: If optimizer is not specified.\n    \"\"\"\n    if self.optimizer is None:\n        raise ValueError(\"Please specify 'system.optimizer' in the config.\")\n    if self.scheduler is None:\n        return {\"optimizer\": self.optimizer}\n    else:\n        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}\n</code></pre>"},{"location":"reference/system/#lighter.system.System.forward","title":"<code>forward(input)</code>","text":"<p>Forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The model's output.</p> Source code in <code>src/lighter/system.py</code> <pre><code>def forward(self, input: Any) -&gt; Any:\n    \"\"\"\n    Forward pass through the model.\n\n    Args:\n        input: The input data.\n\n    Returns:\n        Any: The model's output.\n    \"\"\"\n\n    # Pass `epoch` and/or `step` argument to forward if it accepts them\n    kwargs = {}\n    if hasarg(self.model.forward, Data.EPOCH):\n        kwargs[Data.EPOCH] = self.current_epoch\n    if hasarg(self.model.forward, Data.STEP):\n        kwargs[Data.STEP] = self.global_step\n\n    # Predict. Use inferer if available in val, test, and predict modes.\n    if self.inferer and self.mode in [Mode.VAL, Mode.TEST, Mode.PREDICT]:\n        return self.inferer(input, self.model, **kwargs)\n    return self.model(input, **kwargs)\n</code></pre>"},{"location":"reference/callbacks/","title":"callbacks","text":"<ul> <li>freezer</li> <li>writer</li> <li>utils</li> </ul>"},{"location":"reference/callbacks/#lighter.callbacks.FileWriter","title":"<code>FileWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for saving predictions to files in various formats including tensors, images, videos, and ITK images. Custom writer functions can be provided to extend supported formats. Args:     path: Directory path where output files will be saved.     writer: Either a string specifying a built-in writer or a custom writer function.         Built-in writers:             - \"tensor\": Saves raw tensor data (.pt)             - \"image\": Saves as image file (.png)             - \"video\": Saves as video file             - \"itk_nrrd\": Saves as ITK NRRD file (.nrrd)             - \"itk_seg_nrrd\": Saves as ITK segmentation NRRD file (.seg.nrrd)             - \"itk_nifti\": Saves as ITK NIfTI file (.nii.gz)         Custom writers must:             - Accept (path, tensor) arguments             - Handle single tensor input (no batch dimension)             - Save output to the specified path</p> Source code in <code>src/lighter/callbacks/writer/file.py</code> <pre><code>class FileWriter(BaseWriter):\n    \"\"\"\n    Writer for saving predictions to files in various formats including tensors, images, videos, and ITK images.\n    Custom writer functions can be provided to extend supported formats.\n    Args:\n        path: Directory path where output files will be saved.\n        writer: Either a string specifying a built-in writer or a custom writer function.\n            Built-in writers:\n                - \"tensor\": Saves raw tensor data (.pt)\n                - \"image\": Saves as image file (.png)\n                - \"video\": Saves as video file\n                - \"itk_nrrd\": Saves as ITK NRRD file (.nrrd)\n                - \"itk_seg_nrrd\": Saves as ITK segmentation NRRD file (.seg.nrrd)\n                - \"itk_nifti\": Saves as ITK NIfTI file (.nii.gz)\n            Custom writers must:\n                - Accept (path, tensor) arguments\n                - Handle single tensor input (no batch dimension)\n                - Save output to the specified path\n    \"\"\"\n\n    @property\n    def writers(self) -&gt; dict[str, Callable]:\n        return {\n            \"tensor\": write_tensor,\n            \"image\": write_image,\n            \"video\": write_video,\n            \"itk_nrrd\": partial(write_itk_image, suffix=\".nrrd\"),\n            \"itk_seg_nrrd\": partial(write_itk_image, suffix=\".seg.nrrd\"),\n            \"itk_nifti\": partial(write_itk_image, suffix=\".nii.gz\"),\n        }\n\n    def write(self, tensor: Tensor, identifier: int | str) -&gt; None:\n        \"\"\"\n        Writes the tensor to a file using the specified writer.\n\n        Args:\n            tensor: The tensor to write.\n            identifier: Identifier for naming the file.\n        \"\"\"\n        if not self.path.is_dir():\n            raise RuntimeError(f\"FileWriter expects a directory path, got {self.path}\")\n\n        # Determine the path for the file based on prediction count. The suffix must be added by the writer function.\n        path = self.path / str(identifier)\n        # Write the tensor to the file.\n        self.writer(path, tensor)\n</code></pre>"},{"location":"reference/callbacks/#lighter.callbacks.FileWriter.write","title":"<code>write(tensor, identifier)</code>","text":"<p>Writes the tensor to a file using the specified writer.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to write.</p> required <code>identifier</code> <code>int | str</code> <p>Identifier for naming the file.</p> required Source code in <code>src/lighter/callbacks/writer/file.py</code> <pre><code>def write(self, tensor: Tensor, identifier: int | str) -&gt; None:\n    \"\"\"\n    Writes the tensor to a file using the specified writer.\n\n    Args:\n        tensor: The tensor to write.\n        identifier: Identifier for naming the file.\n    \"\"\"\n    if not self.path.is_dir():\n        raise RuntimeError(f\"FileWriter expects a directory path, got {self.path}\")\n\n    # Determine the path for the file based on prediction count. The suffix must be added by the writer function.\n    path = self.path / str(identifier)\n    # Write the tensor to the file.\n    self.writer(path, tensor)\n</code></pre>"},{"location":"reference/callbacks/#lighter.callbacks.Freezer","title":"<code>Freezer</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to freeze model parameters during training. Parameters can be frozen by exact name or prefix. Freezing can be applied indefinitely or until a specified step/epoch.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str | list[str] | None</code> <p>Full names of parameters to freeze.</p> <code>None</code> <code>name_starts_with</code> <code>str | list[str] | None</code> <p>Prefixes of parameter names to freeze.</p> <code>None</code> <code>except_names</code> <code>str | list[str] | None</code> <p>Names of parameters to exclude from freezing.</p> <code>None</code> <code>except_name_starts_with</code> <code>str | list[str] | None</code> <p>Prefixes of parameter names to exclude from freezing.</p> <code>None</code> <code>until_step</code> <code>int | None</code> <p>Maximum step to freeze parameters until.</p> <code>None</code> <code>until_epoch</code> <code>int | None</code> <p>Maximum epoch to freeze parameters until.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>names</code> nor <code>name_starts_with</code> are specified.</p> <code>ValueError</code> <p>If both <code>until_step</code> and <code>until_epoch</code> are specified.</p> Source code in <code>src/lighter/callbacks/freezer.py</code> <pre><code>class Freezer(Callback):\n    \"\"\"\n    Callback to freeze model parameters during training. Parameters can be frozen by exact name or prefix.\n    Freezing can be applied indefinitely or until a specified step/epoch.\n\n    Args:\n        names: Full names of parameters to freeze.\n        name_starts_with: Prefixes of parameter names to freeze.\n        except_names: Names of parameters to exclude from freezing.\n        except_name_starts_with: Prefixes of parameter names to exclude from freezing.\n        until_step: Maximum step to freeze parameters until.\n        until_epoch: Maximum epoch to freeze parameters until.\n\n    Raises:\n        ValueError: If neither `names` nor `name_starts_with` are specified.\n        ValueError: If both `until_step` and `until_epoch` are specified.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        names: str | list[str] | None = None,\n        name_starts_with: str | list[str] | None = None,\n        except_names: str | list[str] | None = None,\n        except_name_starts_with: str | list[str] | None = None,\n        until_step: int | None = None,\n        until_epoch: int | None = None,\n    ) -&gt; None:\n        super().__init__()\n\n        if names is None and name_starts_with is None:\n            raise ValueError(\"At least one of `names` or `name_starts_with` must be specified.\")\n\n        if until_step is not None and until_epoch is not None:\n            raise ValueError(\"Only one of `until_step` or `until_epoch` can be specified.\")\n\n        self.names = ensure_list(names)\n        self.name_starts_with = ensure_list(name_starts_with)\n        self.except_names = ensure_list(except_names)\n        self.except_name_starts_with = ensure_list(except_name_starts_with)\n        self.until_step = until_step\n        self.until_epoch = until_epoch\n\n        self._frozen_state = False\n\n    def on_train_batch_start(self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int) -&gt; None:\n        \"\"\"\n        Called at the start of each training batch to potentially freeze parameters.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n            batch: The current batch.\n            batch_idx: The index of the batch.\n        \"\"\"\n        self._on_batch_start(trainer, pl_module)\n\n    def on_validation_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def on_test_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def on_predict_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def _on_batch_start(self, trainer: Trainer, pl_module: System) -&gt; None:\n        \"\"\"\n        Freezes or unfreezes model parameters based on the current step or epoch.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n        \"\"\"\n        current_step = trainer.global_step\n        current_epoch = trainer.current_epoch\n\n        if self.until_step is not None and current_step &gt;= self.until_step:\n            if self._frozen_state:\n                logger.info(f\"Reached step {self.until_step} - unfreezing the previously frozen layers.\")\n                self._set_model_requires_grad(pl_module, True)\n            return\n\n        if self.until_epoch is not None and current_epoch &gt;= self.until_epoch:\n            if self._frozen_state:\n                logger.info(f\"Reached epoch {self.until_epoch} - unfreezing the previously frozen layers.\")\n                self._set_model_requires_grad(pl_module, True)\n            return\n\n        if not self._frozen_state:\n            self._set_model_requires_grad(pl_module, False)\n\n    def _set_model_requires_grad(self, model: Module | System, requires_grad: bool) -&gt; None:\n        \"\"\"\n        Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.\n\n        Args:\n            model: The model whose parameters to modify.\n            requires_grad: Whether to allow gradients (unfreeze) or not (freeze).\n        \"\"\"\n        # If the model is a `System`, get the underlying PyTorch model.\n        if isinstance(model, System):\n            model = model.model\n\n        frozen_layers = []\n        # Freeze the specified parameters.\n        for name, param in model.named_parameters():\n            # Leave the excluded-from-freezing parameters trainable.\n            if self.except_names and name in self.except_names:\n                param.requires_grad = True\n                continue\n            if self.except_name_starts_with and any(name.startswith(prefix) for prefix in self.except_name_starts_with):\n                param.requires_grad = True\n                continue\n\n            # Freeze/unfreeze the specified parameters, based on the `requires_grad` argument.\n            if self.names and name in self.names:\n                param.requires_grad = requires_grad\n                frozen_layers.append(name)\n                continue\n            if self.name_starts_with and any(name.startswith(prefix) for prefix in self.name_starts_with):\n                param.requires_grad = requires_grad\n                frozen_layers.append(name)\n                continue\n\n            # Otherwise, leave the parameter trainable.\n            param.requires_grad = True\n\n        self._frozen_state = not requires_grad\n        # Log only when freezing the parameters.\n        if self._frozen_state:\n            logger.info(\n                f\"Setting requires_grad={requires_grad} the following layers\"\n                + (f\" until step {self.until_step}\" if self.until_step is not None else \"\")\n                + (f\" until epoch {self.until_epoch}\" if self.until_epoch is not None else \"\")\n                + f\": {frozen_layers}\"\n            )\n</code></pre>"},{"location":"reference/callbacks/#lighter.callbacks.Freezer._on_batch_start","title":"<code>_on_batch_start(trainer, pl_module)</code>","text":"<p>Freezes or unfreezes model parameters based on the current step or epoch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required Source code in <code>src/lighter/callbacks/freezer.py</code> <pre><code>def _on_batch_start(self, trainer: Trainer, pl_module: System) -&gt; None:\n    \"\"\"\n    Freezes or unfreezes model parameters based on the current step or epoch.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n    \"\"\"\n    current_step = trainer.global_step\n    current_epoch = trainer.current_epoch\n\n    if self.until_step is not None and current_step &gt;= self.until_step:\n        if self._frozen_state:\n            logger.info(f\"Reached step {self.until_step} - unfreezing the previously frozen layers.\")\n            self._set_model_requires_grad(pl_module, True)\n        return\n\n    if self.until_epoch is not None and current_epoch &gt;= self.until_epoch:\n        if self._frozen_state:\n            logger.info(f\"Reached epoch {self.until_epoch} - unfreezing the previously frozen layers.\")\n            self._set_model_requires_grad(pl_module, True)\n        return\n\n    if not self._frozen_state:\n        self._set_model_requires_grad(pl_module, False)\n</code></pre>"},{"location":"reference/callbacks/#lighter.callbacks.Freezer._set_model_requires_grad","title":"<code>_set_model_requires_grad(model, requires_grad)</code>","text":"<p>Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | System</code> <p>The model whose parameters to modify.</p> required <code>requires_grad</code> <code>bool</code> <p>Whether to allow gradients (unfreeze) or not (freeze).</p> required Source code in <code>src/lighter/callbacks/freezer.py</code> <pre><code>def _set_model_requires_grad(self, model: Module | System, requires_grad: bool) -&gt; None:\n    \"\"\"\n    Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.\n\n    Args:\n        model: The model whose parameters to modify.\n        requires_grad: Whether to allow gradients (unfreeze) or not (freeze).\n    \"\"\"\n    # If the model is a `System`, get the underlying PyTorch model.\n    if isinstance(model, System):\n        model = model.model\n\n    frozen_layers = []\n    # Freeze the specified parameters.\n    for name, param in model.named_parameters():\n        # Leave the excluded-from-freezing parameters trainable.\n        if self.except_names and name in self.except_names:\n            param.requires_grad = True\n            continue\n        if self.except_name_starts_with and any(name.startswith(prefix) for prefix in self.except_name_starts_with):\n            param.requires_grad = True\n            continue\n\n        # Freeze/unfreeze the specified parameters, based on the `requires_grad` argument.\n        if self.names and name in self.names:\n            param.requires_grad = requires_grad\n            frozen_layers.append(name)\n            continue\n        if self.name_starts_with and any(name.startswith(prefix) for prefix in self.name_starts_with):\n            param.requires_grad = requires_grad\n            frozen_layers.append(name)\n            continue\n\n        # Otherwise, leave the parameter trainable.\n        param.requires_grad = True\n\n    self._frozen_state = not requires_grad\n    # Log only when freezing the parameters.\n    if self._frozen_state:\n        logger.info(\n            f\"Setting requires_grad={requires_grad} the following layers\"\n            + (f\" until step {self.until_step}\" if self.until_step is not None else \"\")\n            + (f\" until epoch {self.until_epoch}\" if self.until_epoch is not None else \"\")\n            + f\": {frozen_layers}\"\n        )\n</code></pre>"},{"location":"reference/callbacks/#lighter.callbacks.Freezer.on_train_batch_start","title":"<code>on_train_batch_start(trainer, pl_module, batch, batch_idx)</code>","text":"<p>Called at the start of each training batch to potentially freeze parameters.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required <code>batch</code> <code>Any</code> <p>The current batch.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>src/lighter/callbacks/freezer.py</code> <pre><code>def on_train_batch_start(self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int) -&gt; None:\n    \"\"\"\n    Called at the start of each training batch to potentially freeze parameters.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n        batch: The current batch.\n        batch_idx: The index of the batch.\n    \"\"\"\n    self._on_batch_start(trainer, pl_module)\n</code></pre>"},{"location":"reference/callbacks/#lighter.callbacks.TableWriter","title":"<code>TableWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for saving predictions in a table format, such as CSV.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>CSV filepath.</p> required <code>writer</code> <code>str | Callable</code> <p>Writer function or name of a registered writer.</p> required Source code in <code>src/lighter/callbacks/writer/table.py</code> <pre><code>class TableWriter(BaseWriter):\n    \"\"\"\n    Writer for saving predictions in a table format, such as CSV.\n\n    Args:\n        path: CSV filepath.\n        writer: Writer function or name of a registered writer.\n    \"\"\"\n\n    def __init__(self, path: str | Path, writer: str | Callable) -&gt; None:\n        super().__init__(path, writer)\n        self.csv_records = []\n\n    @property\n    def writers(self) -&gt; dict[str, Callable]:\n        return {\n            \"tensor\": lambda tensor: tensor.item() if tensor.numel() == 1 else tensor.tolist(),\n        }\n\n    def write(self, tensor: Any, identifier: int | str) -&gt; None:\n        \"\"\"\n        Writes the tensor as a table record using the specified writer.\n\n        Args:\n            tensor: The tensor to record. Should not have a batch dimension.\n            identifier: Identifier for the record.\n        \"\"\"\n        self.csv_records.append({\"identifier\": identifier, \"pred\": self.writer(tensor)})\n\n    def on_predict_epoch_end(self, trainer: Trainer, pl_module: System) -&gt; None:\n        \"\"\"\n        Called at the end of the prediction epoch to save predictions to a CSV file.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n        \"\"\"\n        # If in distributed data parallel mode, gather records from all processes to rank 0.\n        if trainer.world_size &gt; 1:\n            gather_csv_records = [None] * trainer.world_size if trainer.is_global_zero else None\n            torch.distributed.gather_object(self.csv_records, gather_csv_records, dst=0)\n            if trainer.is_global_zero:\n                self.csv_records = list(itertools.chain(*gather_csv_records))\n\n        # Save the records to a CSV file\n        if trainer.is_global_zero:\n            df = pd.DataFrame(self.csv_records)\n            try:\n                df = df.sort_values(\"identifier\")\n            except TypeError:\n                pass\n            df = df.set_index(\"identifier\")\n            df.to_csv(self.path)\n\n        # Clear the records after saving\n        self.csv_records = []\n</code></pre>"},{"location":"reference/callbacks/#lighter.callbacks.TableWriter.on_predict_epoch_end","title":"<code>on_predict_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of the prediction epoch to save predictions to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required Source code in <code>src/lighter/callbacks/writer/table.py</code> <pre><code>def on_predict_epoch_end(self, trainer: Trainer, pl_module: System) -&gt; None:\n    \"\"\"\n    Called at the end of the prediction epoch to save predictions to a CSV file.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n    \"\"\"\n    # If in distributed data parallel mode, gather records from all processes to rank 0.\n    if trainer.world_size &gt; 1:\n        gather_csv_records = [None] * trainer.world_size if trainer.is_global_zero else None\n        torch.distributed.gather_object(self.csv_records, gather_csv_records, dst=0)\n        if trainer.is_global_zero:\n            self.csv_records = list(itertools.chain(*gather_csv_records))\n\n    # Save the records to a CSV file\n    if trainer.is_global_zero:\n        df = pd.DataFrame(self.csv_records)\n        try:\n            df = df.sort_values(\"identifier\")\n        except TypeError:\n            pass\n        df = df.set_index(\"identifier\")\n        df.to_csv(self.path)\n\n    # Clear the records after saving\n    self.csv_records = []\n</code></pre>"},{"location":"reference/callbacks/#lighter.callbacks.TableWriter.write","title":"<code>write(tensor, identifier)</code>","text":"<p>Writes the tensor as a table record using the specified writer.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Any</code> <p>The tensor to record. Should not have a batch dimension.</p> required <code>identifier</code> <code>int | str</code> <p>Identifier for the record.</p> required Source code in <code>src/lighter/callbacks/writer/table.py</code> <pre><code>def write(self, tensor: Any, identifier: int | str) -&gt; None:\n    \"\"\"\n    Writes the tensor as a table record using the specified writer.\n\n    Args:\n        tensor: The tensor to record. Should not have a batch dimension.\n        identifier: Identifier for the record.\n    \"\"\"\n    self.csv_records.append({\"identifier\": identifier, \"pred\": self.writer(tensor)})\n</code></pre>"},{"location":"reference/callbacks/freezer/","title":"freezer","text":"<p>This module provides the Freezer callback, which allows freezing model parameters during training.</p>"},{"location":"reference/callbacks/freezer/#lighter.callbacks.freezer.Freezer","title":"<code>Freezer</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to freeze model parameters during training. Parameters can be frozen by exact name or prefix. Freezing can be applied indefinitely or until a specified step/epoch.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str | list[str] | None</code> <p>Full names of parameters to freeze.</p> <code>None</code> <code>name_starts_with</code> <code>str | list[str] | None</code> <p>Prefixes of parameter names to freeze.</p> <code>None</code> <code>except_names</code> <code>str | list[str] | None</code> <p>Names of parameters to exclude from freezing.</p> <code>None</code> <code>except_name_starts_with</code> <code>str | list[str] | None</code> <p>Prefixes of parameter names to exclude from freezing.</p> <code>None</code> <code>until_step</code> <code>int | None</code> <p>Maximum step to freeze parameters until.</p> <code>None</code> <code>until_epoch</code> <code>int | None</code> <p>Maximum epoch to freeze parameters until.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>names</code> nor <code>name_starts_with</code> are specified.</p> <code>ValueError</code> <p>If both <code>until_step</code> and <code>until_epoch</code> are specified.</p> Source code in <code>src/lighter/callbacks/freezer.py</code> <pre><code>class Freezer(Callback):\n    \"\"\"\n    Callback to freeze model parameters during training. Parameters can be frozen by exact name or prefix.\n    Freezing can be applied indefinitely or until a specified step/epoch.\n\n    Args:\n        names: Full names of parameters to freeze.\n        name_starts_with: Prefixes of parameter names to freeze.\n        except_names: Names of parameters to exclude from freezing.\n        except_name_starts_with: Prefixes of parameter names to exclude from freezing.\n        until_step: Maximum step to freeze parameters until.\n        until_epoch: Maximum epoch to freeze parameters until.\n\n    Raises:\n        ValueError: If neither `names` nor `name_starts_with` are specified.\n        ValueError: If both `until_step` and `until_epoch` are specified.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        names: str | list[str] | None = None,\n        name_starts_with: str | list[str] | None = None,\n        except_names: str | list[str] | None = None,\n        except_name_starts_with: str | list[str] | None = None,\n        until_step: int | None = None,\n        until_epoch: int | None = None,\n    ) -&gt; None:\n        super().__init__()\n\n        if names is None and name_starts_with is None:\n            raise ValueError(\"At least one of `names` or `name_starts_with` must be specified.\")\n\n        if until_step is not None and until_epoch is not None:\n            raise ValueError(\"Only one of `until_step` or `until_epoch` can be specified.\")\n\n        self.names = ensure_list(names)\n        self.name_starts_with = ensure_list(name_starts_with)\n        self.except_names = ensure_list(except_names)\n        self.except_name_starts_with = ensure_list(except_name_starts_with)\n        self.until_step = until_step\n        self.until_epoch = until_epoch\n\n        self._frozen_state = False\n\n    def on_train_batch_start(self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int) -&gt; None:\n        \"\"\"\n        Called at the start of each training batch to potentially freeze parameters.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n            batch: The current batch.\n            batch_idx: The index of the batch.\n        \"\"\"\n        self._on_batch_start(trainer, pl_module)\n\n    def on_validation_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def on_test_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def on_predict_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def _on_batch_start(self, trainer: Trainer, pl_module: System) -&gt; None:\n        \"\"\"\n        Freezes or unfreezes model parameters based on the current step or epoch.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n        \"\"\"\n        current_step = trainer.global_step\n        current_epoch = trainer.current_epoch\n\n        if self.until_step is not None and current_step &gt;= self.until_step:\n            if self._frozen_state:\n                logger.info(f\"Reached step {self.until_step} - unfreezing the previously frozen layers.\")\n                self._set_model_requires_grad(pl_module, True)\n            return\n\n        if self.until_epoch is not None and current_epoch &gt;= self.until_epoch:\n            if self._frozen_state:\n                logger.info(f\"Reached epoch {self.until_epoch} - unfreezing the previously frozen layers.\")\n                self._set_model_requires_grad(pl_module, True)\n            return\n\n        if not self._frozen_state:\n            self._set_model_requires_grad(pl_module, False)\n\n    def _set_model_requires_grad(self, model: Module | System, requires_grad: bool) -&gt; None:\n        \"\"\"\n        Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.\n\n        Args:\n            model: The model whose parameters to modify.\n            requires_grad: Whether to allow gradients (unfreeze) or not (freeze).\n        \"\"\"\n        # If the model is a `System`, get the underlying PyTorch model.\n        if isinstance(model, System):\n            model = model.model\n\n        frozen_layers = []\n        # Freeze the specified parameters.\n        for name, param in model.named_parameters():\n            # Leave the excluded-from-freezing parameters trainable.\n            if self.except_names and name in self.except_names:\n                param.requires_grad = True\n                continue\n            if self.except_name_starts_with and any(name.startswith(prefix) for prefix in self.except_name_starts_with):\n                param.requires_grad = True\n                continue\n\n            # Freeze/unfreeze the specified parameters, based on the `requires_grad` argument.\n            if self.names and name in self.names:\n                param.requires_grad = requires_grad\n                frozen_layers.append(name)\n                continue\n            if self.name_starts_with and any(name.startswith(prefix) for prefix in self.name_starts_with):\n                param.requires_grad = requires_grad\n                frozen_layers.append(name)\n                continue\n\n            # Otherwise, leave the parameter trainable.\n            param.requires_grad = True\n\n        self._frozen_state = not requires_grad\n        # Log only when freezing the parameters.\n        if self._frozen_state:\n            logger.info(\n                f\"Setting requires_grad={requires_grad} the following layers\"\n                + (f\" until step {self.until_step}\" if self.until_step is not None else \"\")\n                + (f\" until epoch {self.until_epoch}\" if self.until_epoch is not None else \"\")\n                + f\": {frozen_layers}\"\n            )\n</code></pre>"},{"location":"reference/callbacks/freezer/#lighter.callbacks.freezer.Freezer._on_batch_start","title":"<code>_on_batch_start(trainer, pl_module)</code>","text":"<p>Freezes or unfreezes model parameters based on the current step or epoch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required Source code in <code>src/lighter/callbacks/freezer.py</code> <pre><code>def _on_batch_start(self, trainer: Trainer, pl_module: System) -&gt; None:\n    \"\"\"\n    Freezes or unfreezes model parameters based on the current step or epoch.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n    \"\"\"\n    current_step = trainer.global_step\n    current_epoch = trainer.current_epoch\n\n    if self.until_step is not None and current_step &gt;= self.until_step:\n        if self._frozen_state:\n            logger.info(f\"Reached step {self.until_step} - unfreezing the previously frozen layers.\")\n            self._set_model_requires_grad(pl_module, True)\n        return\n\n    if self.until_epoch is not None and current_epoch &gt;= self.until_epoch:\n        if self._frozen_state:\n            logger.info(f\"Reached epoch {self.until_epoch} - unfreezing the previously frozen layers.\")\n            self._set_model_requires_grad(pl_module, True)\n        return\n\n    if not self._frozen_state:\n        self._set_model_requires_grad(pl_module, False)\n</code></pre>"},{"location":"reference/callbacks/freezer/#lighter.callbacks.freezer.Freezer._set_model_requires_grad","title":"<code>_set_model_requires_grad(model, requires_grad)</code>","text":"<p>Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | System</code> <p>The model whose parameters to modify.</p> required <code>requires_grad</code> <code>bool</code> <p>Whether to allow gradients (unfreeze) or not (freeze).</p> required Source code in <code>src/lighter/callbacks/freezer.py</code> <pre><code>def _set_model_requires_grad(self, model: Module | System, requires_grad: bool) -&gt; None:\n    \"\"\"\n    Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.\n\n    Args:\n        model: The model whose parameters to modify.\n        requires_grad: Whether to allow gradients (unfreeze) or not (freeze).\n    \"\"\"\n    # If the model is a `System`, get the underlying PyTorch model.\n    if isinstance(model, System):\n        model = model.model\n\n    frozen_layers = []\n    # Freeze the specified parameters.\n    for name, param in model.named_parameters():\n        # Leave the excluded-from-freezing parameters trainable.\n        if self.except_names and name in self.except_names:\n            param.requires_grad = True\n            continue\n        if self.except_name_starts_with and any(name.startswith(prefix) for prefix in self.except_name_starts_with):\n            param.requires_grad = True\n            continue\n\n        # Freeze/unfreeze the specified parameters, based on the `requires_grad` argument.\n        if self.names and name in self.names:\n            param.requires_grad = requires_grad\n            frozen_layers.append(name)\n            continue\n        if self.name_starts_with and any(name.startswith(prefix) for prefix in self.name_starts_with):\n            param.requires_grad = requires_grad\n            frozen_layers.append(name)\n            continue\n\n        # Otherwise, leave the parameter trainable.\n        param.requires_grad = True\n\n    self._frozen_state = not requires_grad\n    # Log only when freezing the parameters.\n    if self._frozen_state:\n        logger.info(\n            f\"Setting requires_grad={requires_grad} the following layers\"\n            + (f\" until step {self.until_step}\" if self.until_step is not None else \"\")\n            + (f\" until epoch {self.until_epoch}\" if self.until_epoch is not None else \"\")\n            + f\": {frozen_layers}\"\n        )\n</code></pre>"},{"location":"reference/callbacks/freezer/#lighter.callbacks.freezer.Freezer.on_train_batch_start","title":"<code>on_train_batch_start(trainer, pl_module, batch, batch_idx)</code>","text":"<p>Called at the start of each training batch to potentially freeze parameters.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required <code>batch</code> <code>Any</code> <p>The current batch.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>src/lighter/callbacks/freezer.py</code> <pre><code>def on_train_batch_start(self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int) -&gt; None:\n    \"\"\"\n    Called at the start of each training batch to potentially freeze parameters.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n        batch: The current batch.\n        batch_idx: The index of the batch.\n    \"\"\"\n    self._on_batch_start(trainer, pl_module)\n</code></pre>"},{"location":"reference/callbacks/utils/","title":"utils","text":"<p>This module provides utility functions for callbacks, including mode conversion and image preprocessing.</p>"},{"location":"reference/callbacks/utils/#lighter.callbacks.utils.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Preprocess image for logging. For multiple 2D images, creates a grid. For 3D images, stacks slices vertically. For multiple 3D images, creates a grid with each column showing a different 3D image stacked vertically.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>A 2D or 3D image tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The preprocessed image ready for logging.</p> Source code in <code>src/lighter/callbacks/utils.py</code> <pre><code>def preprocess_image(image: Tensor) -&gt; Tensor:\n    \"\"\"\n    Preprocess image for logging. For multiple 2D images, creates a grid.\n    For 3D images, stacks slices vertically. For multiple 3D images, creates a grid\n    with each column showing a different 3D image stacked vertically.\n\n    Args:\n        image: A 2D or 3D image tensor.\n\n    Returns:\n        Tensor: The preprocessed image ready for logging.\n    \"\"\"\n    # If 3D (BCDHW), concat the images vertically and horizontally.\n    if image.ndim == 5:\n        shape = image.shape\n        # BCDHW -&gt; BC(D*H)W. Combine slices of a 3D images vertically into a single 2D image.\n        image = image.view(shape[0], shape[1], shape[2] * shape[3], shape[4])\n        # BCDHW -&gt; 1CDH(B*W). Concat images in the batch horizontally, and unsqueeze to add back the B dim.\n        image = torch.cat([*image], dim=-1).unsqueeze(0)\n    # If only one image in the batch, select it and return it. Same happens when the images are 3D as they\n    # are combined into a single image. `make_grid` is called when a batch of multiple 2D image is provided.\n    return image[0] if image.shape[0] == 1 else torchvision.utils.make_grid(image, nrow=8)\n</code></pre>"},{"location":"reference/callbacks/writer/","title":"writer","text":"<ul> <li>base</li> <li>table</li> <li>file</li> </ul>"},{"location":"reference/callbacks/writer/base/","title":"base","text":"<p>This module provides the base class for defining custom writers in Lighter, allowing predictions to be saved in various formats.</p>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code>, <code>Callback</code></p> <p>Base class for defining custom Writer. It provides the structure to save predictions in various formats.</p> Subclasses should implement <p>1) <code>self.writers</code> attribute to specify the supported formats and their corresponding writer functions. 2) <code>self.write()</code> method to specify the saving strategy for a prediction.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path for saving predictions.</p> required <code>writer</code> <code>str | Callable</code> <p>Writer function or name of a registered writer.</p> required Source code in <code>src/lighter/callbacks/writer/base.py</code> <pre><code>class BaseWriter(ABC, Callback):\n    \"\"\"\n    Base class for defining custom Writer. It provides the structure to save predictions in various formats.\n\n    Subclasses should implement:\n        1) `self.writers` attribute to specify the supported formats and their corresponding writer functions.\n        2) `self.write()` method to specify the saving strategy for a prediction.\n\n    Args:\n        path (str | Path): Path for saving predictions.\n        writer (str | Callable): Writer function or name of a registered writer.\n    \"\"\"\n\n    def __init__(self, path: str | Path, writer: str | Callable) -&gt; None:\n        self.path = Path(path)\n\n        # Check if the writer is a string and if it exists in the writers dictionary\n        if isinstance(writer, str):\n            if writer not in self.writers:\n                raise ValueError(f\"Writer for format {writer} does not exist. Available writers: {self.writers.keys()}.\")\n            self.writer = self.writers[writer]\n        else:\n            # If the writer is not a string, it is assumed to be a callable function\n            self.writer = writer\n\n        # Prediction counter. Used when IDs are not provided. Initialized in `self.setup()` based on the DDP rank.\n        self._pred_counter = None\n\n    @property\n    @abstractmethod\n    def writers(self) -&gt; dict[str, Callable]:\n        \"\"\"\n        Property to define the default writer functions.\n        \"\"\"\n\n    @abstractmethod\n    def write(self, tensor: Tensor, identifier: int | str) -&gt; None:\n        \"\"\"\n        Method to define how a tensor should be saved. The input tensor will be a single tensor without\n        the batch dimension.\n\n        For each supported format, there should be a corresponding writer function registered in `self.writers`\n        A specific writer function can be retrieved using `self.get_writer(self.format)`.\n\n        Args:\n            tensor (Tensor): Tensor, without the batch dimension, to be saved.\n            identifier (int): Identifier for the tensor, can be used for naming files or adding table records.\n        \"\"\"\n\n    def setup(self, trainer: Trainer, pl_module: System, stage: str) -&gt; None:\n        \"\"\"\n        Sets up the writer, ensuring the path is ready for saving predictions.\n\n        Args:\n            trainer (Trainer): The trainer instance.\n            pl_module (System): The System instance.\n            stage (str): The current stage of training.\n        \"\"\"\n        if stage != Stage.PREDICT:\n            return\n\n        # Initialize the prediction count with the rank of the current process\n        self._pred_counter = torch.distributed.get_rank() if trainer.world_size &gt; 1 else 0\n\n        # Ensure all distributed nodes write to the same path\n        self.path = trainer.strategy.broadcast(self.path, src=0)\n        directory = self.path.parent if self.path.suffix else self.path\n\n        # Warn if the path already exists\n        if self.path.exists():\n            logger.warning(f\"{self.path} already exists, existing predictions will be overwritten.\")\n\n        if trainer.is_global_zero:\n            directory.mkdir(parents=True, exist_ok=True)\n\n        # Wait for rank 0 to create the directory\n        trainer.strategy.barrier()\n\n        # Ensure all distributed nodes have access to the path\n        if not directory.exists():\n            raise RuntimeError(\n                f\"Rank {trainer.global_rank} does not share storage with rank 0. Ensure nodes have common storage access.\"\n            )\n\n    def on_predict_batch_end(\n        self, trainer: Trainer, pl_module: System, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        \"\"\"\n        Callback method executed at the end of each prediction batch to write predictions with unique IDs.\n\n        Args:\n            trainer (Trainer): The trainer instance.\n            pl_module (System): The System instance.\n            outputs (Any): The outputs from the prediction step.\n            batch (Any): The current batch.\n            batch_idx (int): The index of the batch.\n            dataloader_idx (int): The index of the dataloader.\n        \"\"\"\n        # If the IDs are not provided, generate global unique IDs based on the prediction count. DDP supported.\n        if outputs[Data.IDENTIFIER] is None:\n            batch_size = len(outputs[Data.PRED])\n            world_size = trainer.world_size\n            outputs[Data.IDENTIFIER] = list(\n                range(\n                    self._pred_counter,  # Start: counted globally, initialized with the rank of the current process\n                    self._pred_counter + batch_size * world_size,  # Stop: count the total batch size across all processes\n                    world_size,  # Step: each process writes predictions for every Nth sample\n                )\n            )\n            self._pred_counter += batch_size * world_size\n\n        # Ensure equal number of predictions and identifiers\n        if len(outputs[Data.IDENTIFIER]) != len(outputs[Data.PRED]):\n            raise ValueError(\n                f\"The number of predictions ({len(outputs[Data.PRED])}) does not\"\n                f\"match the number of identifiers ({len(outputs[Data.IDENTIFIER])})\"\n            )\n\n        for pred, identifier in zip(outputs[Data.PRED], outputs[Data.IDENTIFIER], strict=True):\n            self.write(tensor=pred, identifier=identifier)\n\n        # Clear the predictions to save CPU memory. https://github.com/Lightning-AI/pytorch-lightning/issues/19398\n        trainer.predict_loop._predictions = [[] for _ in range(trainer.predict_loop.num_dataloaders)]\n        gc.collect()\n</code></pre>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter.writers","title":"<code>writers</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Property to define the default writer functions.</p>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter.on_predict_batch_end","title":"<code>on_predict_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Callback method executed at the end of each prediction batch to write predictions with unique IDs.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required <code>outputs</code> <code>Any</code> <p>The outputs from the prediction step.</p> required <code>batch</code> <code>Any</code> <p>The current batch.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the dataloader.</p> <code>0</code> Source code in <code>src/lighter/callbacks/writer/base.py</code> <pre><code>def on_predict_batch_end(\n    self, trainer: Trainer, pl_module: System, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int = 0\n) -&gt; None:\n    \"\"\"\n    Callback method executed at the end of each prediction batch to write predictions with unique IDs.\n\n    Args:\n        trainer (Trainer): The trainer instance.\n        pl_module (System): The System instance.\n        outputs (Any): The outputs from the prediction step.\n        batch (Any): The current batch.\n        batch_idx (int): The index of the batch.\n        dataloader_idx (int): The index of the dataloader.\n    \"\"\"\n    # If the IDs are not provided, generate global unique IDs based on the prediction count. DDP supported.\n    if outputs[Data.IDENTIFIER] is None:\n        batch_size = len(outputs[Data.PRED])\n        world_size = trainer.world_size\n        outputs[Data.IDENTIFIER] = list(\n            range(\n                self._pred_counter,  # Start: counted globally, initialized with the rank of the current process\n                self._pred_counter + batch_size * world_size,  # Stop: count the total batch size across all processes\n                world_size,  # Step: each process writes predictions for every Nth sample\n            )\n        )\n        self._pred_counter += batch_size * world_size\n\n    # Ensure equal number of predictions and identifiers\n    if len(outputs[Data.IDENTIFIER]) != len(outputs[Data.PRED]):\n        raise ValueError(\n            f\"The number of predictions ({len(outputs[Data.PRED])}) does not\"\n            f\"match the number of identifiers ({len(outputs[Data.IDENTIFIER])})\"\n        )\n\n    for pred, identifier in zip(outputs[Data.PRED], outputs[Data.IDENTIFIER], strict=True):\n        self.write(tensor=pred, identifier=identifier)\n\n    # Clear the predictions to save CPU memory. https://github.com/Lightning-AI/pytorch-lightning/issues/19398\n    trainer.predict_loop._predictions = [[] for _ in range(trainer.predict_loop.num_dataloaders)]\n    gc.collect()\n</code></pre>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter.setup","title":"<code>setup(trainer, pl_module, stage)</code>","text":"<p>Sets up the writer, ensuring the path is ready for saving predictions.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required <code>stage</code> <code>str</code> <p>The current stage of training.</p> required Source code in <code>src/lighter/callbacks/writer/base.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: System, stage: str) -&gt; None:\n    \"\"\"\n    Sets up the writer, ensuring the path is ready for saving predictions.\n\n    Args:\n        trainer (Trainer): The trainer instance.\n        pl_module (System): The System instance.\n        stage (str): The current stage of training.\n    \"\"\"\n    if stage != Stage.PREDICT:\n        return\n\n    # Initialize the prediction count with the rank of the current process\n    self._pred_counter = torch.distributed.get_rank() if trainer.world_size &gt; 1 else 0\n\n    # Ensure all distributed nodes write to the same path\n    self.path = trainer.strategy.broadcast(self.path, src=0)\n    directory = self.path.parent if self.path.suffix else self.path\n\n    # Warn if the path already exists\n    if self.path.exists():\n        logger.warning(f\"{self.path} already exists, existing predictions will be overwritten.\")\n\n    if trainer.is_global_zero:\n        directory.mkdir(parents=True, exist_ok=True)\n\n    # Wait for rank 0 to create the directory\n    trainer.strategy.barrier()\n\n    # Ensure all distributed nodes have access to the path\n    if not directory.exists():\n        raise RuntimeError(\n            f\"Rank {trainer.global_rank} does not share storage with rank 0. Ensure nodes have common storage access.\"\n        )\n</code></pre>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter.write","title":"<code>write(tensor, identifier)</code>  <code>abstractmethod</code>","text":"<p>Method to define how a tensor should be saved. The input tensor will be a single tensor without the batch dimension.</p> <p>For each supported format, there should be a corresponding writer function registered in <code>self.writers</code> A specific writer function can be retrieved using <code>self.get_writer(self.format)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Tensor, without the batch dimension, to be saved.</p> required <code>identifier</code> <code>int</code> <p>Identifier for the tensor, can be used for naming files or adding table records.</p> required Source code in <code>src/lighter/callbacks/writer/base.py</code> <pre><code>@abstractmethod\ndef write(self, tensor: Tensor, identifier: int | str) -&gt; None:\n    \"\"\"\n    Method to define how a tensor should be saved. The input tensor will be a single tensor without\n    the batch dimension.\n\n    For each supported format, there should be a corresponding writer function registered in `self.writers`\n    A specific writer function can be retrieved using `self.get_writer(self.format)`.\n\n    Args:\n        tensor (Tensor): Tensor, without the batch dimension, to be saved.\n        identifier (int): Identifier for the tensor, can be used for naming files or adding table records.\n    \"\"\"\n</code></pre>"},{"location":"reference/callbacks/writer/file/","title":"file","text":"<p>This module provides the FileWriter class, which writes predictions to files in various formats.</p>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.FileWriter","title":"<code>FileWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for saving predictions to files in various formats including tensors, images, videos, and ITK images. Custom writer functions can be provided to extend supported formats. Args:     path: Directory path where output files will be saved.     writer: Either a string specifying a built-in writer or a custom writer function.         Built-in writers:             - \"tensor\": Saves raw tensor data (.pt)             - \"image\": Saves as image file (.png)             - \"video\": Saves as video file             - \"itk_nrrd\": Saves as ITK NRRD file (.nrrd)             - \"itk_seg_nrrd\": Saves as ITK segmentation NRRD file (.seg.nrrd)             - \"itk_nifti\": Saves as ITK NIfTI file (.nii.gz)         Custom writers must:             - Accept (path, tensor) arguments             - Handle single tensor input (no batch dimension)             - Save output to the specified path</p> Source code in <code>src/lighter/callbacks/writer/file.py</code> <pre><code>class FileWriter(BaseWriter):\n    \"\"\"\n    Writer for saving predictions to files in various formats including tensors, images, videos, and ITK images.\n    Custom writer functions can be provided to extend supported formats.\n    Args:\n        path: Directory path where output files will be saved.\n        writer: Either a string specifying a built-in writer or a custom writer function.\n            Built-in writers:\n                - \"tensor\": Saves raw tensor data (.pt)\n                - \"image\": Saves as image file (.png)\n                - \"video\": Saves as video file\n                - \"itk_nrrd\": Saves as ITK NRRD file (.nrrd)\n                - \"itk_seg_nrrd\": Saves as ITK segmentation NRRD file (.seg.nrrd)\n                - \"itk_nifti\": Saves as ITK NIfTI file (.nii.gz)\n            Custom writers must:\n                - Accept (path, tensor) arguments\n                - Handle single tensor input (no batch dimension)\n                - Save output to the specified path\n    \"\"\"\n\n    @property\n    def writers(self) -&gt; dict[str, Callable]:\n        return {\n            \"tensor\": write_tensor,\n            \"image\": write_image,\n            \"video\": write_video,\n            \"itk_nrrd\": partial(write_itk_image, suffix=\".nrrd\"),\n            \"itk_seg_nrrd\": partial(write_itk_image, suffix=\".seg.nrrd\"),\n            \"itk_nifti\": partial(write_itk_image, suffix=\".nii.gz\"),\n        }\n\n    def write(self, tensor: Tensor, identifier: int | str) -&gt; None:\n        \"\"\"\n        Writes the tensor to a file using the specified writer.\n\n        Args:\n            tensor: The tensor to write.\n            identifier: Identifier for naming the file.\n        \"\"\"\n        if not self.path.is_dir():\n            raise RuntimeError(f\"FileWriter expects a directory path, got {self.path}\")\n\n        # Determine the path for the file based on prediction count. The suffix must be added by the writer function.\n        path = self.path / str(identifier)\n        # Write the tensor to the file.\n        self.writer(path, tensor)\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.FileWriter.write","title":"<code>write(tensor, identifier)</code>","text":"<p>Writes the tensor to a file using the specified writer.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to write.</p> required <code>identifier</code> <code>int | str</code> <p>Identifier for naming the file.</p> required Source code in <code>src/lighter/callbacks/writer/file.py</code> <pre><code>def write(self, tensor: Tensor, identifier: int | str) -&gt; None:\n    \"\"\"\n    Writes the tensor to a file using the specified writer.\n\n    Args:\n        tensor: The tensor to write.\n        identifier: Identifier for naming the file.\n    \"\"\"\n    if not self.path.is_dir():\n        raise RuntimeError(f\"FileWriter expects a directory path, got {self.path}\")\n\n    # Determine the path for the file based on prediction count. The suffix must be added by the writer function.\n    path = self.path / str(identifier)\n    # Write the tensor to the file.\n    self.writer(path, tensor)\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.write_image","title":"<code>write_image(path, tensor)</code>","text":"<p>Writes a tensor as an image file in .png format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to save the image.</p> required <code>tensor</code> <p>The tensor representing the image.</p> required Source code in <code>src/lighter/callbacks/writer/file.py</code> <pre><code>def write_image(path, tensor):\n    \"\"\"\n    Writes a tensor as an image file in .png format.\n\n    Args:\n        path: The path to save the image.\n        tensor: The tensor representing the image.\n    \"\"\"\n    path = path.with_suffix(\".png\")\n    tensor = preprocess_image(tensor)\n    torchvision.io.write_png(tensor, str(path))\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.write_itk_image","title":"<code>write_itk_image(path, tensor, suffix)</code>","text":"<p>Writes a tensor as an ITK image file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the ITK image.</p> required <code>tensor</code> <code>MetaTensor</code> <p>The tensor representing the image. Must be in MONAI MetaTensor format.</p> required <code>suffix</code> <p>The file suffix indicating the format.</p> required Source code in <code>src/lighter/callbacks/writer/file.py</code> <pre><code>def write_itk_image(path: str, tensor: MetaTensor, suffix) -&gt; None:\n    \"\"\"\n    Writes a tensor as an ITK image file.\n\n    Args:\n        path: The path to save the ITK image.\n        tensor: The tensor representing the image. Must be in MONAI MetaTensor format.\n        suffix: The file suffix indicating the format.\n    \"\"\"\n    path = path.with_suffix(suffix)\n    if not isinstance(tensor, MetaTensor):\n        raise TypeError(\"Tensor must be in MONAI MetaTensor format.\")\n    itk_image = metatensor_to_itk_image(tensor, channel_dim=0, dtype=tensor.dtype)\n    OPTIONAL_IMPORTS[\"itk\"].imwrite(itk_image, str(path), True)\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.write_tensor","title":"<code>write_tensor(path, tensor)</code>","text":"<p>Writes a tensor to a file in .pt format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to save the tensor.</p> required <code>tensor</code> <p>The tensor to save.</p> required Source code in <code>src/lighter/callbacks/writer/file.py</code> <pre><code>def write_tensor(path, tensor):\n    \"\"\"\n    Writes a tensor to a file in .pt format.\n\n    Args:\n        path: The path to save the tensor.\n        tensor: The tensor to save.\n    \"\"\"\n    torch.save(tensor, path.with_suffix(\".pt\"))  # nosec B614\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.write_video","title":"<code>write_video(path, tensor)</code>","text":"<p>Writes a tensor as a video file in .mp4 format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to save the video.</p> required <code>tensor</code> <p>The tensor representing the video.</p> required Source code in <code>src/lighter/callbacks/writer/file.py</code> <pre><code>def write_video(path, tensor):\n    \"\"\"\n    Writes a tensor as a video file in .mp4 format.\n\n    Args:\n        path: The path to save the video.\n        tensor: The tensor representing the video.\n    \"\"\"\n    path = path.with_suffix(\".mp4\")\n    # Video tensor must be divisible by 2. Pad the height and width.\n    tensor = DivisiblePad(k=(0, 2, 2), mode=\"minimum\")(tensor)\n    # Video tensor must be THWC. Permute CTHW -&gt; THWC.\n    tensor = tensor.permute(1, 2, 3, 0)\n    # Video tensor must have 3 channels (RGB). Repeat the channel dim to convert grayscale to RGB.\n    if tensor.shape[-1] == 1:\n        tensor = tensor.repeat(1, 1, 1, 3)\n    # Video tensor must be in the range [0, 1]. Scale to [0, 255].\n    tensor = (tensor * 255).to(torch.uint8)\n    torchvision.io.write_video(str(path), tensor, fps=24)\n</code></pre>"},{"location":"reference/callbacks/writer/table/","title":"table","text":"<p>This module provides the TableWriter class, which saves predictions in a table format, such as CSV.</p>"},{"location":"reference/callbacks/writer/table/#lighter.callbacks.writer.table.TableWriter","title":"<code>TableWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for saving predictions in a table format, such as CSV.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>CSV filepath.</p> required <code>writer</code> <code>str | Callable</code> <p>Writer function or name of a registered writer.</p> required Source code in <code>src/lighter/callbacks/writer/table.py</code> <pre><code>class TableWriter(BaseWriter):\n    \"\"\"\n    Writer for saving predictions in a table format, such as CSV.\n\n    Args:\n        path: CSV filepath.\n        writer: Writer function or name of a registered writer.\n    \"\"\"\n\n    def __init__(self, path: str | Path, writer: str | Callable) -&gt; None:\n        super().__init__(path, writer)\n        self.csv_records = []\n\n    @property\n    def writers(self) -&gt; dict[str, Callable]:\n        return {\n            \"tensor\": lambda tensor: tensor.item() if tensor.numel() == 1 else tensor.tolist(),\n        }\n\n    def write(self, tensor: Any, identifier: int | str) -&gt; None:\n        \"\"\"\n        Writes the tensor as a table record using the specified writer.\n\n        Args:\n            tensor: The tensor to record. Should not have a batch dimension.\n            identifier: Identifier for the record.\n        \"\"\"\n        self.csv_records.append({\"identifier\": identifier, \"pred\": self.writer(tensor)})\n\n    def on_predict_epoch_end(self, trainer: Trainer, pl_module: System) -&gt; None:\n        \"\"\"\n        Called at the end of the prediction epoch to save predictions to a CSV file.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n        \"\"\"\n        # If in distributed data parallel mode, gather records from all processes to rank 0.\n        if trainer.world_size &gt; 1:\n            gather_csv_records = [None] * trainer.world_size if trainer.is_global_zero else None\n            torch.distributed.gather_object(self.csv_records, gather_csv_records, dst=0)\n            if trainer.is_global_zero:\n                self.csv_records = list(itertools.chain(*gather_csv_records))\n\n        # Save the records to a CSV file\n        if trainer.is_global_zero:\n            df = pd.DataFrame(self.csv_records)\n            try:\n                df = df.sort_values(\"identifier\")\n            except TypeError:\n                pass\n            df = df.set_index(\"identifier\")\n            df.to_csv(self.path)\n\n        # Clear the records after saving\n        self.csv_records = []\n</code></pre>"},{"location":"reference/callbacks/writer/table/#lighter.callbacks.writer.table.TableWriter.on_predict_epoch_end","title":"<code>on_predict_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of the prediction epoch to save predictions to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required Source code in <code>src/lighter/callbacks/writer/table.py</code> <pre><code>def on_predict_epoch_end(self, trainer: Trainer, pl_module: System) -&gt; None:\n    \"\"\"\n    Called at the end of the prediction epoch to save predictions to a CSV file.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n    \"\"\"\n    # If in distributed data parallel mode, gather records from all processes to rank 0.\n    if trainer.world_size &gt; 1:\n        gather_csv_records = [None] * trainer.world_size if trainer.is_global_zero else None\n        torch.distributed.gather_object(self.csv_records, gather_csv_records, dst=0)\n        if trainer.is_global_zero:\n            self.csv_records = list(itertools.chain(*gather_csv_records))\n\n    # Save the records to a CSV file\n    if trainer.is_global_zero:\n        df = pd.DataFrame(self.csv_records)\n        try:\n            df = df.sort_values(\"identifier\")\n        except TypeError:\n            pass\n        df = df.set_index(\"identifier\")\n        df.to_csv(self.path)\n\n    # Clear the records after saving\n    self.csv_records = []\n</code></pre>"},{"location":"reference/callbacks/writer/table/#lighter.callbacks.writer.table.TableWriter.write","title":"<code>write(tensor, identifier)</code>","text":"<p>Writes the tensor as a table record using the specified writer.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Any</code> <p>The tensor to record. Should not have a batch dimension.</p> required <code>identifier</code> <code>int | str</code> <p>Identifier for the record.</p> required Source code in <code>src/lighter/callbacks/writer/table.py</code> <pre><code>def write(self, tensor: Any, identifier: int | str) -&gt; None:\n    \"\"\"\n    Writes the tensor as a table record using the specified writer.\n\n    Args:\n        tensor: The tensor to record. Should not have a batch dimension.\n        identifier: Identifier for the record.\n    \"\"\"\n    self.csv_records.append({\"identifier\": identifier, \"pred\": self.writer(tensor)})\n</code></pre>"},{"location":"reference/engine/","title":"engine","text":"<ul> <li>config</li> <li>resolver</li> <li>schema</li> <li>runner</li> </ul>"},{"location":"reference/engine/config/","title":"config","text":""},{"location":"reference/engine/config/#lighter.engine.config.Config","title":"<code>Config</code>","text":"<p>Handles loading, overriding, validating, and normalizing configurations.</p> Source code in <code>src/lighter/engine/config.py</code> <pre><code>class Config:\n    \"\"\"\n    Handles loading, overriding, validating, and normalizing configurations.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: str | dict[str, Any] | None,\n        validate: bool,\n        **config_overrides: Any,\n    ):\n        \"\"\"\n        Initialize the Config object.\n\n        Args:\n            config: Path to a YAML configuration file or a dictionary containing the configuration.\n            validate: Whether to validate the configuration.\n            config_overrides: Keyword arguments to override values in the configuration file\n        \"\"\"\n        if not isinstance(config, (dict, str, type(None))):\n            raise ValueError(\"Invalid type for 'config'. Must be a dictionary or (comma-separated) path(s) to YAML file(s).\")\n\n        self._config_parser = ConfigParser(globals=False)\n        self._config_parser.read_config(config)\n        self._config_parser.parse()\n\n        # TODO: verify that switching from .update(config_overrides) to .set(value, name) is\n        # a valid approach. The latter allows creation of currently non-existent keys.\n        for name, value in config_overrides.items():\n            self._config_parser.set(value, name)\n\n        # Validate the configuration\n        if validate:\n            validator = cerberus.Validator(SCHEMA)\n            valid = validator.validate(self.get())\n            if not valid:\n                errors = format_validation_errors(validator.errors)\n                raise ConfigurationException(errors)\n\n    def get(self, key: str | None = None, default: Any = None) -&gt; Any:\n        \"\"\"Get raw content for the given key. If key is None, get the entire config.\"\"\"\n        return self._config_parser.config if key is None else self._config_parser.config.get(key, default)\n\n    def get_parsed_content(self, key: str | None = None, default: Any = None) -&gt; Any:\n        \"\"\"\n        Get the parsed content for the given key. If key is None, get the entire parsed config.\n        \"\"\"\n        return self._config_parser.get_parsed_content(key, default=default)\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.Config.__init__","title":"<code>__init__(config, validate, **config_overrides)</code>","text":"<p>Initialize the Config object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str | dict[str, Any] | None</code> <p>Path to a YAML configuration file or a dictionary containing the configuration.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate the configuration.</p> required <code>config_overrides</code> <code>Any</code> <p>Keyword arguments to override values in the configuration file</p> <code>{}</code> Source code in <code>src/lighter/engine/config.py</code> <pre><code>def __init__(\n    self,\n    config: str | dict[str, Any] | None,\n    validate: bool,\n    **config_overrides: Any,\n):\n    \"\"\"\n    Initialize the Config object.\n\n    Args:\n        config: Path to a YAML configuration file or a dictionary containing the configuration.\n        validate: Whether to validate the configuration.\n        config_overrides: Keyword arguments to override values in the configuration file\n    \"\"\"\n    if not isinstance(config, (dict, str, type(None))):\n        raise ValueError(\"Invalid type for 'config'. Must be a dictionary or (comma-separated) path(s) to YAML file(s).\")\n\n    self._config_parser = ConfigParser(globals=False)\n    self._config_parser.read_config(config)\n    self._config_parser.parse()\n\n    # TODO: verify that switching from .update(config_overrides) to .set(value, name) is\n    # a valid approach. The latter allows creation of currently non-existent keys.\n    for name, value in config_overrides.items():\n        self._config_parser.set(value, name)\n\n    # Validate the configuration\n    if validate:\n        validator = cerberus.Validator(SCHEMA)\n        valid = validator.validate(self.get())\n        if not valid:\n            errors = format_validation_errors(validator.errors)\n            raise ConfigurationException(errors)\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.Config.get","title":"<code>get(key=None, default=None)</code>","text":"<p>Get raw content for the given key. If key is None, get the entire config.</p> Source code in <code>src/lighter/engine/config.py</code> <pre><code>def get(self, key: str | None = None, default: Any = None) -&gt; Any:\n    \"\"\"Get raw content for the given key. If key is None, get the entire config.\"\"\"\n    return self._config_parser.config if key is None else self._config_parser.config.get(key, default)\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.Config.get_parsed_content","title":"<code>get_parsed_content(key=None, default=None)</code>","text":"<p>Get the parsed content for the given key. If key is None, get the entire parsed config.</p> Source code in <code>src/lighter/engine/config.py</code> <pre><code>def get_parsed_content(self, key: str | None = None, default: Any = None) -&gt; Any:\n    \"\"\"\n    Get the parsed content for the given key. If key is None, get the entire parsed config.\n    \"\"\"\n    return self._config_parser.get_parsed_content(key, default=default)\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.ConfigurationException","title":"<code>ConfigurationException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for validation errors.</p> Source code in <code>src/lighter/engine/config.py</code> <pre><code>class ConfigurationException(Exception):\n    \"\"\"Custom exception for validation errors.\"\"\"\n\n    def __init__(self, errors: str):\n        super().__init__(f\"Configuration validation failed:\\n{errors}\")\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.format_validation_errors","title":"<code>format_validation_errors(errors)</code>","text":"<p>Recursively format validation errors into a readable string.</p> Source code in <code>src/lighter/engine/config.py</code> <pre><code>def format_validation_errors(errors: dict[str, Any]) -&gt; str:\n    \"\"\"\n    Recursively format validation errors into a readable string.\n    \"\"\"\n    messages = []\n\n    def process_error(key, value, base_path=\"\"):\n        full_key = f\"{base_path}.{key}\" if base_path else key\n\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                process_error(sub_key, sub_value, full_key)\n        elif isinstance(value, list):\n            for item in value:\n                if isinstance(item, str):\n                    messages.append(f\"{full_key}: {item}\")\n                elif isinstance(item, dict):\n                    process_error(key, item, base_path)\n                else:\n                    messages.append(f\"{full_key}: {item}\")\n        else:\n            messages.append(f\"{full_key}: {value}\")\n\n    process_error(\"\", errors)\n    return \"\\n\".join(messages)\n</code></pre>"},{"location":"reference/engine/resolver/","title":"resolver","text":""},{"location":"reference/engine/resolver/#lighter.engine.resolver.Resolver","title":"<code>Resolver</code>","text":"<p>Resolves stage-specific configurations from the main configuration.</p> Source code in <code>src/lighter/engine/resolver.py</code> <pre><code>class Resolver:\n    \"\"\"\n    Resolves stage-specific configurations from the main configuration.\n    \"\"\"\n\n    STAGE_MODES = {\n        Stage.FIT: [Mode.TRAIN, Mode.VAL],\n        Stage.VALIDATE: [Mode.VAL],\n        Stage.TEST: [Mode.TEST],\n        Stage.PREDICT: [Mode.PREDICT],\n    }\n\n    def __init__(self, config: Config):\n        self.config = config\n\n    def get_stage_config(self, stage: Stage) -&gt; Config:\n        \"\"\"Get stage-specific configuration by filtering unused components.\"\"\"\n        if stage not in self.STAGE_MODES:\n            raise ValueError(f\"Invalid stage: {stage}. Allowed stages are {list(self.STAGE_MODES)}\")\n\n        stage_config = self.config.get().copy()\n        system_config = stage_config.get(\"system\", {})\n        dataloader_config = system_config.get(\"dataloaders\", {})\n        metrics_config = system_config.get(\"metrics\", {})\n\n        # Remove dataloaders not relevant to the current stage\n        for mode in set(dataloader_config) - set(self.STAGE_MODES[stage]):\n            dataloader_config.pop(mode, None)\n\n        # Remove metrics not relevant to the current stage\n        for mode in set(metrics_config) - set(self.STAGE_MODES[stage]):\n            metrics_config.pop(mode, None)\n\n        # Remove optimizer, scheduler, and criterion if not relevant to the current stage\n        if stage in [Stage.VALIDATE, Stage.TEST, Stage.PREDICT]:\n            if stage != Stage.VALIDATE:\n                system_config.pop(\"criterion\", None)\n            system_config.pop(\"optimizer\", None)\n            system_config.pop(\"scheduler\", None)\n\n        # Retain only relevant args for the current stage\n        if \"args\" in stage_config:\n            stage_config[\"args\"] = {stage: stage_config[\"args\"].get(stage, {})}\n\n        return Config(stage_config, validate=False)\n</code></pre>"},{"location":"reference/engine/resolver/#lighter.engine.resolver.Resolver.get_stage_config","title":"<code>get_stage_config(stage)</code>","text":"<p>Get stage-specific configuration by filtering unused components.</p> Source code in <code>src/lighter/engine/resolver.py</code> <pre><code>def get_stage_config(self, stage: Stage) -&gt; Config:\n    \"\"\"Get stage-specific configuration by filtering unused components.\"\"\"\n    if stage not in self.STAGE_MODES:\n        raise ValueError(f\"Invalid stage: {stage}. Allowed stages are {list(self.STAGE_MODES)}\")\n\n    stage_config = self.config.get().copy()\n    system_config = stage_config.get(\"system\", {})\n    dataloader_config = system_config.get(\"dataloaders\", {})\n    metrics_config = system_config.get(\"metrics\", {})\n\n    # Remove dataloaders not relevant to the current stage\n    for mode in set(dataloader_config) - set(self.STAGE_MODES[stage]):\n        dataloader_config.pop(mode, None)\n\n    # Remove metrics not relevant to the current stage\n    for mode in set(metrics_config) - set(self.STAGE_MODES[stage]):\n        metrics_config.pop(mode, None)\n\n    # Remove optimizer, scheduler, and criterion if not relevant to the current stage\n    if stage in [Stage.VALIDATE, Stage.TEST, Stage.PREDICT]:\n        if stage != Stage.VALIDATE:\n            system_config.pop(\"criterion\", None)\n        system_config.pop(\"optimizer\", None)\n        system_config.pop(\"scheduler\", None)\n\n    # Retain only relevant args for the current stage\n    if \"args\" in stage_config:\n        stage_config[\"args\"] = {stage: stage_config[\"args\"].get(stage, {})}\n\n    return Config(stage_config, validate=False)\n</code></pre>"},{"location":"reference/engine/runner/","title":"runner","text":""},{"location":"reference/engine/runner/#lighter.engine.runner.Runner","title":"<code>Runner</code>","text":"<p>Executes the specified stage using the validated and resolved configurations.</p> Source code in <code>src/lighter/engine/runner.py</code> <pre><code>class Runner:\n    \"\"\"\n    Executes the specified stage using the validated and resolved configurations.\n    \"\"\"\n\n    def __init__(self):\n        self.config = None\n        self.resolver = None\n        self.system = None\n        self.trainer = None\n        self.args = None\n\n    def run(self, stage: str, config: str | dict | None = None, **config_overrides: Any) -&gt; None:\n        \"\"\"Run the specified stage with the given configuration.\"\"\"\n        seed_everything()\n        self.config = Config(config, **config_overrides, validate=True)\n\n        # Resolves stage-specific configuration\n        self.resolver = Resolver(self.config)\n\n        # Setup stage\n        self._setup_stage(stage)\n\n        # Run stage\n        self._run_stage(stage)\n\n    def _run_stage(self, stage: str) -&gt; None:\n        \"\"\"Execute the specified stage (method) of the trainer.\"\"\"\n        stage_method = getattr(self.trainer, stage)\n        stage_method(self.system, **self.args)\n\n    def _setup_stage(self, stage: str) -&gt; None:\n        # Prune the configuration to the stage-specific components\n        stage_config = self.resolver.get_stage_config(stage)\n\n        # Import project module\n        project_path = stage_config.get(\"project\")\n        if project_path:\n            import_module_from_path(\"project\", project_path)\n\n        # Initialize system\n        self.system = stage_config.get_parsed_content(\"system\")\n        if not isinstance(self.system, System):\n            raise ValueError(\"'system' must be an instance of System\")\n\n        # Initialize trainer\n        self.trainer = stage_config.get_parsed_content(\"trainer\")\n        if not isinstance(self.trainer, Trainer):\n            raise ValueError(\"'trainer' must be an instance of Trainer\")\n\n        # Set up arguments for the stage\n        self.args = stage_config.get_parsed_content(f\"args#{stage}\", default={})\n\n        # Save config to system checkpoint and trainer logger\n        self._save_config()\n\n    def _save_config(self) -&gt; None:\n        \"\"\"Save config to system checkpoint and trainer logger.\"\"\"\n        if self.system:\n            self.system.save_hyperparameters(self.config.get())\n        if self.trainer and self.trainer.logger:\n            self.trainer.logger.log_hyperparams(self.config.get())\n</code></pre>"},{"location":"reference/engine/runner/#lighter.engine.runner.Runner._run_stage","title":"<code>_run_stage(stage)</code>","text":"<p>Execute the specified stage (method) of the trainer.</p> Source code in <code>src/lighter/engine/runner.py</code> <pre><code>def _run_stage(self, stage: str) -&gt; None:\n    \"\"\"Execute the specified stage (method) of the trainer.\"\"\"\n    stage_method = getattr(self.trainer, stage)\n    stage_method(self.system, **self.args)\n</code></pre>"},{"location":"reference/engine/runner/#lighter.engine.runner.Runner._save_config","title":"<code>_save_config()</code>","text":"<p>Save config to system checkpoint and trainer logger.</p> Source code in <code>src/lighter/engine/runner.py</code> <pre><code>def _save_config(self) -&gt; None:\n    \"\"\"Save config to system checkpoint and trainer logger.\"\"\"\n    if self.system:\n        self.system.save_hyperparameters(self.config.get())\n    if self.trainer and self.trainer.logger:\n        self.trainer.logger.log_hyperparams(self.config.get())\n</code></pre>"},{"location":"reference/engine/runner/#lighter.engine.runner.Runner.run","title":"<code>run(stage, config=None, **config_overrides)</code>","text":"<p>Run the specified stage with the given configuration.</p> Source code in <code>src/lighter/engine/runner.py</code> <pre><code>def run(self, stage: str, config: str | dict | None = None, **config_overrides: Any) -&gt; None:\n    \"\"\"Run the specified stage with the given configuration.\"\"\"\n    seed_everything()\n    self.config = Config(config, **config_overrides, validate=True)\n\n    # Resolves stage-specific configuration\n    self.resolver = Resolver(self.config)\n\n    # Setup stage\n    self._setup_stage(stage)\n\n    # Run stage\n    self._run_stage(stage)\n</code></pre>"},{"location":"reference/engine/schema/","title":"schema","text":"<p>Defines the schema for configuration validation in the Lighter framework.</p> <p>The schema ensures user configurations are correctly structured and typed. It includes: - <code>_meta_</code>: Metadata as a dictionary. - <code>_requires_</code>: Runs first, primarily to be used for imports. - <code>project</code>: Project name as a string. - <code>vars</code>: Variables as a dictionary. - <code>args</code>: Arguments to pass to Trainer stage methods like <code>fit</code>, <code>validate</code>, <code>test</code>. - <code>trainer</code>: Trainer setup. - <code>system</code>: System setup, encapsulates model, criterion, optimizer, scheduler, inferer, metrics, dataloaders, and adapters.</p> <p>Used by the <code>Config</code> class for validation.</p>"},{"location":"reference/utils/","title":"utils","text":"<ul> <li>misc</li> <li>types</li> <li>patches</li> <li>logging</li> <li>model</li> <li>dynamic_imports</li> <li>data</li> </ul>"},{"location":"reference/utils/data/","title":"data","text":""},{"location":"reference/utils/data/#lighter.utils.data.collate_replace_corrupted","title":"<code>collate_replace_corrupted(batch, dataset, default_collate_fn=None)</code>","text":"<p>Collate function to handle corrupted examples in a batch by replacing them with valid ones.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data from the DataLoader.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset being used, which should return <code>None</code> for corrupted examples.</p> required <code>default_collate_fn</code> <code>Callable | None</code> <p>The default collate function to use once the batch is clean.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A batch with corrupted examples replaced by valid ones.</p> Source code in <code>src/lighter/utils/data.py</code> <pre><code>def collate_replace_corrupted(\n    batch: Any, dataset: torch.utils.data.Dataset, default_collate_fn: Callable | None = None\n) -&gt; Any:\n    \"\"\"\n    Collate function to handle corrupted examples in a batch by replacing them with valid ones.\n\n    Args:\n        batch: The batch of data from the DataLoader.\n        dataset: The dataset being used, which should return `None` for corrupted examples.\n        default_collate_fn: The default collate function to use once the batch is clean.\n\n    Returns:\n        A batch with corrupted examples replaced by valid ones.\n    \"\"\"\n    # Use `torch.utils.data.dataloader.default_collate` if no other default collate function is specified.\n    default_collate_fn = default_collate_fn if default_collate_fn is not None else default_collate\n    # Idea from https://stackoverflow.com/a/57882783\n    original_batch_len = len(batch)\n    # Filter out all the Nones (corrupted examples).\n    batch = list(filter(lambda x: x is not None, batch))\n    filtered_batch_len = len(batch)\n    # Num of corrupted examples.\n    num_corrupted = original_batch_len - filtered_batch_len\n    if num_corrupted &gt; 0:\n        # Replace a corrupted example with another randomly selected example.\n        batch.extend([dataset[random.randint(0, len(dataset) - 1)] for _ in range(num_corrupted)])\n        # Recursive call to replace the replacements if they are corrupted.\n        return collate_replace_corrupted(batch, dataset)\n    # Finally, when the whole batch is fine, apply the default collate function.\n    return default_collate_fn(batch)\n</code></pre>"},{"location":"reference/utils/dynamic_imports/","title":"dynamic_imports","text":"<p>This module provides utilities for dynamic imports, allowing optional imports and importing modules from paths.</p>"},{"location":"reference/utils/dynamic_imports/#lighter.utils.dynamic_imports.OptionalImports","title":"<code>OptionalImports</code>  <code>dataclass</code>","text":"<p>Handles optional imports, allowing modules to be imported only if they are available.</p> <p>Attributes:</p> Name Type Description <code>imports</code> <code>dict[str, object]</code> <p>A dictionary to store the imported modules.</p> Example <pre><code>from lighter.utils.dynamic_imports import OPTIONAL_IMPORTS\nwriter = OPTIONAL_IMPORTS[\"tensorboard\"].SummaryWriter()\n</code></pre> Source code in <code>src/lighter/utils/dynamic_imports.py</code> <pre><code>@dataclass\nclass OptionalImports:\n    \"\"\"\n    Handles optional imports, allowing modules to be imported only if they are available.\n\n    Attributes:\n        imports: A dictionary to store the imported modules.\n\n    Example:\n        ```\n        from lighter.utils.dynamic_imports import OPTIONAL_IMPORTS\n        writer = OPTIONAL_IMPORTS[\"tensorboard\"].SummaryWriter()\n        ```\n    \"\"\"\n\n    imports: dict[str, object] = field(default_factory=dict)\n\n    def __getitem__(self, module_name: str) -&gt; object:\n        \"\"\"\n        Get the imported module by name, importing it if necessary.\n\n        Args:\n            module_name: Name of the module to import.\n\n        Raises:\n            ImportError: If the module is not available.\n\n        Returns:\n            object: The imported module.\n        \"\"\"\n        \"\"\"Get the imported module by name.\n\n        Args:\n            module_name: Name of the module to import.\n\n        Raises:\n            ImportError: If the module is not available.\n\n        Returns:\n            Imported module.\n        \"\"\"\n        if module_name not in self.imports:\n            self.imports[module_name], module_available = optional_import(module_name)\n            if not module_available:\n                raise ImportError(f\"'{module_name}' is not available. Make sure that it is installed and spelled correctly.\")\n        return self.imports[module_name]\n</code></pre>"},{"location":"reference/utils/dynamic_imports/#lighter.utils.dynamic_imports.OptionalImports.__getitem__","title":"<code>__getitem__(module_name)</code>","text":"<p>Get the imported module by name, importing it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the module to import.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module is not available.</p> <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The imported module.</p> Source code in <code>src/lighter/utils/dynamic_imports.py</code> <pre><code>def __getitem__(self, module_name: str) -&gt; object:\n    \"\"\"\n    Get the imported module by name, importing it if necessary.\n\n    Args:\n        module_name: Name of the module to import.\n\n    Raises:\n        ImportError: If the module is not available.\n\n    Returns:\n        object: The imported module.\n    \"\"\"\n    \"\"\"Get the imported module by name.\n\n    Args:\n        module_name: Name of the module to import.\n\n    Raises:\n        ImportError: If the module is not available.\n\n    Returns:\n        Imported module.\n    \"\"\"\n    if module_name not in self.imports:\n        self.imports[module_name], module_available = optional_import(module_name)\n        if not module_available:\n            raise ImportError(f\"'{module_name}' is not available. Make sure that it is installed and spelled correctly.\")\n    return self.imports[module_name]\n</code></pre>"},{"location":"reference/utils/dynamic_imports/#lighter.utils.dynamic_imports.import_module_from_path","title":"<code>import_module_from_path(module_name, module_path)</code>","text":"<p>Import a module from a given path and assign it a specified name.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name to assign to the imported module.</p> required <code>module_path</code> <code>Path</code> <p>Path to the module being imported.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the module has already been imported.</p> <code>FileNotFoundError</code> <p>If the <code>__init__.py</code> file is not found in the module path.</p> Source code in <code>src/lighter/utils/dynamic_imports.py</code> <pre><code>def import_module_from_path(module_name: str, module_path: Path) -&gt; None:\n    \"\"\"\n    Import a module from a given path and assign it a specified name.\n\n    Args:\n        module_name: Name to assign to the imported module.\n        module_path: Path to the module being imported.\n\n    Raises:\n        ValueError: If the module has already been imported.\n        FileNotFoundError: If the `__init__.py` file is not found in the module path.\n    \"\"\"\n    # Based on https://stackoverflow.com/a/41595552.\n\n    if module_name in sys.modules:\n        logger.warning(f\"{module_name} has already been imported as module.\")\n        return\n\n    module_path = Path(module_path).resolve() / \"__init__.py\"\n    if not module_path.is_file():\n        raise FileNotFoundError(f\"No `__init__.py` in `{module_path}`.\")\n    spec = importlib.util.spec_from_file_location(module_name, str(module_path))\n    if spec is None:\n        raise ModuleNotFoundError(f\"Could not find module '{module_name}' at '{module_path}'.\")\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    sys.modules[module_name] = module\n    logger.info(f\"Imported {module_path.parent} as module '{module_name}'.\")\n</code></pre>"},{"location":"reference/utils/logging/","title":"logging","text":"<p>Logging utilities for configuring and setting up custom logging using Loguru and Rich.</p> <p>This module provides functionality to set up visually appealing logs with custom formatting, traceback handling, and suppression of detailed logs from specified modules. It includes color mapping for different log levels and handlers to intercept and redirect logging to Loguru.</p>"},{"location":"reference/utils/logging/#lighter.utils.logging._setup_logging","title":"<code>_setup_logging()</code>","text":"<p>Configures custom logging using Loguru and Rich for visually appealing logs. Sets up traceback handling and suppression of specific modules. Must be run before importing anything else to set up the loggers correctly.</p> Source code in <code>src/lighter/utils/logging.py</code> <pre><code>def _setup_logging():\n    \"\"\"\n    Configures custom logging using Loguru and Rich for visually appealing logs.\n    Sets up traceback handling and suppression of specific modules.\n    Must be run before importing anything else to set up the loggers correctly.\n    \"\"\"\n    import inspect\n    import logging\n    import warnings\n\n    import rich.logging\n    import rich.traceback\n    from loguru import logger\n\n    def formatter(record: dict) -&gt; str:\n        \"\"\"Format log messages for better readability and clarity. Used to configure Loguru with a Rich handler.\"\"\"\n        lvl_name = record[\"level\"].name\n        lvl_color = LOGGING_COLOR_MAP.get(lvl_name, \"cyan\")\n        return (\n            \"[not bold green]{time:YYYY/MM/DD HH:mm:ss.SSS}[/not bold green]  |  {level.icon}  \"\n            f\"[{lvl_color}]{lvl_name:&lt;10}[/{lvl_color}]|  [{lvl_color}]{{message}}[/{lvl_color}]\"\n        )\n\n    class InterceptHandler(logging.Handler):\n        \"\"\"Handler to redirect other libraries' logging to Loguru. Taken from Loguru's documentation:\n        https://github.com/Delgan/loguru?tab=readme-ov-file#entirely-compatible-with-standard-logging\n        \"\"\"\n\n        def emit(self, record: logging.LogRecord) -&gt; None:\n            # Get corresponding Loguru level if it exists.\n            level: str | int\n            try:\n                level = logger.level(record.levelname).name\n            except ValueError:\n                level = record.levelno\n\n            # Find caller from where originated the logged message.\n            frame, depth = inspect.currentframe(), 0\n            while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):\n                frame = frame.f_back\n                depth += 1\n\n            logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())\n\n    # Intercept logging and redirect to Loguru. Must be called before importing other libraries to work.\n    logging.getLogger().handlers = [InterceptHandler()]\n\n    # Configure Rich traceback.\n    suppress = [importlib.import_module(name) for name in SUPPRESSED_MODULES]\n    rich.traceback.install(show_locals=False, width=120, suppress=suppress)\n    # Rich handler for Loguru. Time and level are handled by the formatter.\n    rich_handler = rich.logging.RichHandler(markup=True, show_time=False, show_level=False)\n    logger.configure(handlers=[{\"sink\": rich_handler, \"format\": formatter}])\n\n    # Capture the `warnings` standard module with Loguru\n    # https://loguru.readthedocs.io/en/stable/resources/recipes.html#capturing-standard-stdout-stderr-and-warnings\n    warnings.showwarning = lambda message, *args, **kwargs: logger.opt(depth=2).warning(message)\n</code></pre>"},{"location":"reference/utils/misc/","title":"misc","text":"<p>This module contains miscellaneous utility functions for handling lists, attributes, and function arguments.</p>"},{"location":"reference/utils/misc/#lighter.utils.misc.ensure_list","title":"<code>ensure_list(input)</code>","text":"<p>Ensures that the input is wrapped in a list. If the input is None, returns an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input to wrap in a list.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>list</code> <p>The input wrapped in a list, or an empty list if input is None.</p> Source code in <code>src/lighter/utils/misc.py</code> <pre><code>def ensure_list(input: Any) -&gt; list:\n    \"\"\"\n    Ensures that the input is wrapped in a list. If the input is None, returns an empty list.\n\n    Args:\n        input: The input to wrap in a list.\n\n    Returns:\n        List: The input wrapped in a list, or an empty list if input is None.\n    \"\"\"\n    if isinstance(input, list):\n        return input\n    if isinstance(input, tuple):\n        return list(input)\n    if input is None:\n        return []\n    return [input]\n</code></pre>"},{"location":"reference/utils/misc/#lighter.utils.misc.get_name","title":"<code>get_name(_callable, include_module_name=False)</code>","text":"<p>Retrieves the name of a callable, optionally including the module name.</p> <p>Parameters:</p> Name Type Description Default <code>_callable</code> <code>Callable</code> <p>The callable whose name to retrieve.</p> required <code>include_module_name</code> <code>bool</code> <p>Whether to include the module name in the result.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the callable, optionally prefixed with the module name.</p> Source code in <code>src/lighter/utils/misc.py</code> <pre><code>def get_name(_callable: Callable, include_module_name: bool = False) -&gt; str:\n    \"\"\"\n    Retrieves the name of a callable, optionally including the module name.\n\n    Args:\n        _callable: The callable whose name to retrieve.\n        include_module_name: Whether to include the module name in the result.\n\n    Returns:\n        str: The name of the callable, optionally prefixed with the module name.\n    \"\"\"\n    # Get the name directly from the callable's __name__ attribute\n    name = getattr(_callable, \"__name__\", type(_callable).__name__)\n\n    if include_module_name:\n        # Get the module name directly from the callable's __module__ attribute\n        module = getattr(_callable, \"__module__\", type(_callable).__module__)\n        name = f\"{module}.{name}\"\n\n    return name\n</code></pre>"},{"location":"reference/utils/misc/#lighter.utils.misc.get_optimizer_stats","title":"<code>get_optimizer_stats(optimizer)</code>","text":"<p>Extract learning rates and momentum values from a PyTorch optimizer.</p> <p>Collects learning rate and momentum/beta values from each parameter group in the optimizer and returns them in a dictionary. Keys are formatted to show the optimizer type and group number (if multiple groups exist).</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The PyTorch optimizer to extract values from.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: dictionary containing: - Learning rates: \"optimizer/{name}/lr[/group{N}]\" - Momentum values: \"optimizer/{name}/momentum[/group{N}]\"</p> <p>Where [/group{N}] is only added for optimizers with multiple groups.</p> Source code in <code>src/lighter/utils/misc.py</code> <pre><code>def get_optimizer_stats(optimizer: Optimizer) -&gt; dict[str, float]:\n    \"\"\"\n    Extract learning rates and momentum values from a PyTorch optimizer.\n\n    Collects learning rate and momentum/beta values from each parameter group\n    in the optimizer and returns them in a dictionary. Keys are formatted to show\n    the optimizer type and group number (if multiple groups exist).\n\n    Args:\n        optimizer: The PyTorch optimizer to extract values from.\n\n    Returns:\n        dict[str, float]: dictionary containing:\n            - Learning rates: \"optimizer/{name}/lr[/group{N}]\"\n            - Momentum values: \"optimizer/{name}/momentum[/group{N}]\"\n\n            Where [/group{N}] is only added for optimizers with multiple groups.\n    \"\"\"\n    stats_dict = {}\n    for group_idx, group in enumerate(optimizer.param_groups):\n        lr_key = f\"optimizer/{optimizer.__class__.__name__}/lr\"\n        momentum_key = f\"optimizer/{optimizer.__class__.__name__}/momentum\"\n\n        # Add group index to the key if there are multiple parameter groups\n        if len(optimizer.param_groups) &gt; 1:\n            lr_key += f\"/group{group_idx + 1}\"\n            momentum_key += f\"/group{group_idx + 1}\"\n\n        # Extracting learning rate\n        stats_dict[lr_key] = group[\"lr\"]\n\n        # Extracting momentum or betas[0] if available\n        if \"momentum\" in group:\n            stats_dict[momentum_key] = group[\"momentum\"]\n        if \"betas\" in group:\n            stats_dict[momentum_key] = group[\"betas\"][0]\n\n    return stats_dict\n</code></pre>"},{"location":"reference/utils/misc/#lighter.utils.misc.hasarg","title":"<code>hasarg(fn, arg_name)</code>","text":"<p>Checks if a callable (function, method, or class) has a specific argument.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The callable to inspect.</p> required <code>arg_name</code> <code>str</code> <p>The name of the argument to check for.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the argument exists, False otherwise.</p> Source code in <code>src/lighter/utils/misc.py</code> <pre><code>def hasarg(fn: Callable, arg_name: str) -&gt; bool:\n    \"\"\"\n    Checks if a callable (function, method, or class) has a specific argument.\n\n    Args:\n        fn: The callable to inspect.\n        arg_name: The name of the argument to check for.\n\n    Returns:\n        bool: True if the argument exists, False otherwise.\n    \"\"\"\n    args = inspect.signature(fn).parameters.keys()\n    return arg_name in args\n</code></pre>"},{"location":"reference/utils/misc/#lighter.utils.misc.setattr_dot_notation","title":"<code>setattr_dot_notation(obj, attr, value)</code>","text":"<p>Sets an attribute on an object using dot notation.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Callable</code> <p>The object on which to set the attribute.</p> required <code>attr</code> <code>str</code> <p>The attribute name, which can use dot notation for nested attributes.</p> required <code>value</code> <code>Any</code> <p>The value to set the attribute to.</p> required Source code in <code>src/lighter/utils/misc.py</code> <pre><code>def setattr_dot_notation(obj: Callable, attr: str, value: Any) -&gt; None:\n    \"\"\"\n    Sets an attribute on an object using dot notation.\n\n    Args:\n        obj: The object on which to set the attribute.\n        attr: The attribute name, which can use dot notation for nested attributes.\n        value: The value to set the attribute to.\n    \"\"\"\n    if \".\" not in attr:\n        if not hasattr(obj, attr):\n            raise AttributeError(f\"`{get_name(obj, True)}` has no attribute `{attr}`.\")\n        setattr(obj, attr, value)\n    # Solve recursively if the attribute is defined in dot-notation\n    else:\n        obj_name, attr = attr.split(\".\", maxsplit=1)\n        setattr_dot_notation(getattr(obj, obj_name), attr, value)\n</code></pre>"},{"location":"reference/utils/model/","title":"model","text":"<p>This module provides utility functions for manipulating PyTorch models, such as replacing layers or loading state_dicts.</p>"},{"location":"reference/utils/model/#lighter.utils.model.adjust_prefix_and_load_state_dict","title":"<code>adjust_prefix_and_load_state_dict(model, ckpt_path, ckpt_to_model_prefix=None, layers_to_ignore=None)</code>","text":"<p>This function loads a state dictionary from a checkpoint file into a model using <code>torch.load(strict=False)</code>. It supports remapping layer names between the checkpoint and model through the <code>ckpt_to_model_prefix</code> parameter.</p> <p>This is useful when loading weights from a model that was trained as part of a larger architecture, where the layer names may not match the standalone version of the model.</p> <p>Before using <code>ckpt_to_model_prefix</code>, it's recommended to: 1. Check the layer names in both the checkpoint and target model 2. Map the mismatched prefixes accordingly</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to load the state_dict into.</p> required <code>ckpt_path</code> <code>str</code> <p>The path to the checkpoint file.</p> required <code>ckpt_to_model_prefix</code> <code>dict[str, str] | None</code> <p>Mapping of checkpoint prefixes to model prefixes.</p> <code>None</code> <code>layers_to_ignore</code> <code>list[str] | None</code> <p>Layers to ignore when loading the state_dict.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Module</code> <code>Module</code> <p>The model with the loaded state_dict.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no overlap between the checkpoint's and model's state_dict.</p> Source code in <code>src/lighter/utils/model.py</code> <pre><code>def adjust_prefix_and_load_state_dict(\n    model: Module,\n    ckpt_path: str,\n    ckpt_to_model_prefix: dict[str, str] | None = None,\n    layers_to_ignore: list[str] | None = None,\n) -&gt; Module:\n    \"\"\"\n    This function loads a state dictionary from a checkpoint file into a model using `torch.load(strict=False)`.\n    It supports remapping layer names between the checkpoint and model through the `ckpt_to_model_prefix` parameter.\n\n    This is useful when loading weights from a model that was trained as part of a larger architecture,\n    where the layer names may not match the standalone version of the model.\n\n    Before using `ckpt_to_model_prefix`, it's recommended to:\n    1. Check the layer names in both the checkpoint and target model\n    2. Map the mismatched prefixes accordingly\n\n    Args:\n        model: The model to load the state_dict into.\n        ckpt_path: The path to the checkpoint file.\n        ckpt_to_model_prefix: Mapping of checkpoint prefixes to model prefixes.\n        layers_to_ignore: Layers to ignore when loading the state_dict.\n\n    Returns:\n        Module: The model with the loaded state_dict.\n\n    Raises:\n        ValueError: If there is no overlap between the checkpoint's and model's state_dict.\n    \"\"\"\n    # Load checkpoint and handle if state_dict is nested.\n    ckpt = torch.load(ckpt_path)  # nosec B614\n    if \"state_dict\" in ckpt:\n        # System has a model attribute that contains the actual model, remove the \"model.\" prefix\n        ckpt = {key.replace(\"model.\", \"\"): value for key, value in ckpt[\"state_dict\"].items()}\n\n    # Adjust checkpoint keys based on prefix mapping\n    adjusted_ckpt = {}\n    if ckpt_to_model_prefix:\n        for ckpt_prefix, model_prefix in ckpt_to_model_prefix.items():\n            ckpt_prefix = f\"{ckpt_prefix}.\" if ckpt_prefix and not ckpt_prefix.endswith(\".\") else ckpt_prefix\n            model_prefix = f\"{model_prefix}.\" if model_prefix and not model_prefix.endswith(\".\") else model_prefix\n\n            if ckpt_prefix:\n                adjusted_ckpt.update(\n                    {key.replace(ckpt_prefix, model_prefix): value for key, value in ckpt.items() if ckpt_prefix in key}\n                )\n            else:\n                adjusted_ckpt.update({f\"{model_prefix}{key}\": value for key, value in ckpt.items()})\n\n        if not adjusted_ckpt:\n            adjusted_ckpt = ckpt\n    else:\n        adjusted_ckpt = ckpt\n\n    # Remove ignored layers if specified\n    if layers_to_ignore:\n        for layer in layers_to_ignore:\n            adjusted_ckpt.pop(layer)\n\n    # Verify overlap between model and checkpoint keys\n    model_keys = list(model.state_dict().keys())\n    ckpt_keys = list(adjusted_ckpt.keys())\n    if not set(model_keys) &amp; set(ckpt_keys):\n        raise ValueError(\n            \"There is no overlap between checkpoint's and model's state_dict.\"\n            f\"\\nModel keys: {model_keys[0] + ', ..., ' + model_keys[-1] if model_keys else '[]'}\"\n            f\"\\nCheckpoint keys: {ckpt_keys[0] + ', ..., ' + ckpt_keys[-1] if ckpt_keys else '[]'}\"\n        )\n    # Load state dict and handle incompatible keys\n    incompatible_keys = model.load_state_dict(adjusted_ckpt, strict=False)\n    if incompatible_keys.missing_keys or incompatible_keys.unexpected_keys:\n        logger.info(f\"Encountered incompatible keys during checkpoint loading. If intended, ignore.\\n{incompatible_keys}\")\n    else:\n        logger.info(\"Checkpoint loaded successfully.\")\n\n    return model\n</code></pre>"},{"location":"reference/utils/model/#lighter.utils.model.remove_n_last_layers_sequentially","title":"<code>remove_n_last_layers_sequentially(model, num_layers=1)</code>","text":"<p>Removes a specified number of layers from the end of a model and returns it as a Sequential model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module()</code> <p>The model to modify.</p> required <code>num_layers</code> <p>The number of layers to remove from the end.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>Sequential</code> <code>Sequential</code> <p>The modified model as a Sequential container.</p> Source code in <code>src/lighter/utils/model.py</code> <pre><code>def remove_n_last_layers_sequentially(model: Module(), num_layers=1) -&gt; Sequential:\n    \"\"\"\n    Removes a specified number of layers from the end of a model and returns it as a Sequential model.\n\n    Args:\n        model: The model to modify.\n        num_layers: The number of layers to remove from the end.\n\n    Returns:\n        Sequential: The modified model as a Sequential container.\n    \"\"\"\n    return Sequential(*list(model.children())[:-num_layers])\n</code></pre>"},{"location":"reference/utils/model/#lighter.utils.model.replace_layer_with","title":"<code>replace_layer_with(model, layer_name, new_layer)</code>","text":"<p>Replaces a specified layer in a PyTorch model with a new layer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to modify.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer to replace, using dot notation if necessary (e.g. \"layer10.fc.weights\").</p> required <code>new_layer</code> <code>Module</code> <p>The new layer to insert.</p> required <p>Returns:</p> Name Type Description <code>Module</code> <code>Module</code> <p>The modified model with the new layer.</p> Source code in <code>src/lighter/utils/model.py</code> <pre><code>def replace_layer_with(model: Module, layer_name: str, new_layer: Module) -&gt; Module:\n    \"\"\"\n    Replaces a specified layer in a PyTorch model with a new layer.\n\n    Args:\n        model: The model to modify.\n        layer_name: The name of the layer to replace,\n            using dot notation if necessary (e.g. \"layer10.fc.weights\").\n        new_layer: The new layer to insert.\n\n    Returns:\n        Module: The modified model with the new layer.\n    \"\"\"\n    setattr_dot_notation(model, layer_name, new_layer)\n    return model\n</code></pre>"},{"location":"reference/utils/model/#lighter.utils.model.replace_layer_with_identity","title":"<code>replace_layer_with_identity(model, layer_name)</code>","text":"<p>Replaces a specified layer in a PyTorch model with an Identity layer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to modify.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer to replace with an Identity layer, using dot notation if necessary (e.g. \"layer10.fc.weights\").</p> required <p>Returns:</p> Name Type Description <code>Module</code> <code>Module</code> <p>The modified model with the Identity layer.</p> Source code in <code>src/lighter/utils/model.py</code> <pre><code>def replace_layer_with_identity(model: Module, layer_name: str) -&gt; Module:\n    \"\"\"\n    Replaces a specified layer in a PyTorch model with an Identity layer.\n\n    Args:\n        model: The model to modify.\n        layer_name: The name of the layer to replace with an Identity layer,\n            using dot notation if necessary (e.g. \"layer10.fc.weights\").\n\n    Returns:\n        Module: The modified model with the Identity layer.\n    \"\"\"\n    return replace_layer_with(model, layer_name, Identity())\n</code></pre>"},{"location":"reference/utils/patches/","title":"patches","text":"<p>Contains code that patches certain issues from other libraries that we expect will be resolved in the future.</p>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict","title":"<code>PatchedModuleDict</code>","text":"<p>               Bases: <code>ModuleDict</code></p> <p>This class provides a workaround for key conflicts in PyTorch's ModuleDict by ensuring unique internal keys.</p> Source code in <code>src/lighter/utils/patches.py</code> <pre><code>class PatchedModuleDict(ModuleDict):\n    \"\"\"\n    This class provides a workaround for key conflicts in PyTorch's ModuleDict by ensuring unique internal keys.\n    \"\"\"\n\n    # https://github.com/pytorch/pytorch/issues/71203\n    def __init__(self, modules=None):\n        \"\"\"\n        Initializes the PatchedModuleDict with optional modules.\n\n        Args:\n            modules (dict, optional): A dictionary of modules to initialize the ModuleDict.\n        \"\"\"\n        self._key_map = {}\n        super().__init__(modules)\n\n    def __setitem__(self, key, module):\n        \"\"\"\n        Sets the module for the given key, ensuring a unique internal key.\n\n        Args:\n            key (str): The key to associate with the module.\n            module (torch.nn.Module): The module to store.\n        \"\"\"\n        internal_key = f\"_{key}\"\n        while internal_key in self._modules:\n            internal_key = f\"_{internal_key}\"\n        self._key_map[key] = internal_key\n        super().__setitem__(internal_key, module)\n\n    def __getitem__(self, key):\n        \"\"\"\n        Retrieves the module associated with the given key.\n\n        Args:\n            key (str): The key for which to retrieve the module.\n\n        Returns:\n            torch.nn.Module: The module associated with the key.\n        \"\"\"\n        internal_key = self._key_map.get(key, key)\n        return super().__getitem__(internal_key)\n\n    def __delitem__(self, key):\n        \"\"\"\n        Deletes the module associated with the given key.\n\n        Args:\n            key (str): The key for which to delete the module.\n        \"\"\"\n        internal_key = self._key_map.pop(key, key)\n        super().__delitem__(internal_key)\n\n    def __contains__(self, key):\n        \"\"\"\n        Checks if a module is associated with the given key.\n\n        Args:\n            key (str): The key to check.\n\n        Returns:\n            bool: True if the key exists, False otherwise.\n        \"\"\"\n        internal_key = self._key_map.get(key, key)\n        return super().__contains__(internal_key)\n\n    def keys(self):\n        \"\"\"\n        Returns the keys of the modules.\n\n        Returns:\n            KeysView: A view of the keys in the dictionary.\n        \"\"\"\n        return self._key_map.keys()\n\n    def items(self):\n        \"\"\"\n        Returns the items (key, module) in the dictionary.\n\n        Returns:\n            Generator: A generator yielding key, module pairs.\n        \"\"\"\n        return ((key, self._modules[internal_key]) for key, internal_key in self._key_map.items())\n\n    def values(self):\n        \"\"\"\n        Returns the modules in the dictionary.\n\n        Returns:\n            Generator: A generator yielding modules.\n        \"\"\"\n        return (self._modules[internal_key] for internal_key in self._key_map.values())\n</code></pre>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict.__contains__","title":"<code>__contains__(key)</code>","text":"<p>Checks if a module is associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the key exists, False otherwise.</p> Source code in <code>src/lighter/utils/patches.py</code> <pre><code>def __contains__(self, key):\n    \"\"\"\n    Checks if a module is associated with the given key.\n\n    Args:\n        key (str): The key to check.\n\n    Returns:\n        bool: True if the key exists, False otherwise.\n    \"\"\"\n    internal_key = self._key_map.get(key, key)\n    return super().__contains__(internal_key)\n</code></pre>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Deletes the module associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key for which to delete the module.</p> required Source code in <code>src/lighter/utils/patches.py</code> <pre><code>def __delitem__(self, key):\n    \"\"\"\n    Deletes the module associated with the given key.\n\n    Args:\n        key (str): The key for which to delete the module.\n    \"\"\"\n    internal_key = self._key_map.pop(key, key)\n    super().__delitem__(internal_key)\n</code></pre>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieves the module associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key for which to retrieve the module.</p> required <p>Returns:</p> Type Description <p>torch.nn.Module: The module associated with the key.</p> Source code in <code>src/lighter/utils/patches.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"\n    Retrieves the module associated with the given key.\n\n    Args:\n        key (str): The key for which to retrieve the module.\n\n    Returns:\n        torch.nn.Module: The module associated with the key.\n    \"\"\"\n    internal_key = self._key_map.get(key, key)\n    return super().__getitem__(internal_key)\n</code></pre>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict.__init__","title":"<code>__init__(modules=None)</code>","text":"<p>Initializes the PatchedModuleDict with optional modules.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>dict</code> <p>A dictionary of modules to initialize the ModuleDict.</p> <code>None</code> Source code in <code>src/lighter/utils/patches.py</code> <pre><code>def __init__(self, modules=None):\n    \"\"\"\n    Initializes the PatchedModuleDict with optional modules.\n\n    Args:\n        modules (dict, optional): A dictionary of modules to initialize the ModuleDict.\n    \"\"\"\n    self._key_map = {}\n    super().__init__(modules)\n</code></pre>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict.__setitem__","title":"<code>__setitem__(key, module)</code>","text":"<p>Sets the module for the given key, ensuring a unique internal key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to associate with the module.</p> required <code>module</code> <code>Module</code> <p>The module to store.</p> required Source code in <code>src/lighter/utils/patches.py</code> <pre><code>def __setitem__(self, key, module):\n    \"\"\"\n    Sets the module for the given key, ensuring a unique internal key.\n\n    Args:\n        key (str): The key to associate with the module.\n        module (torch.nn.Module): The module to store.\n    \"\"\"\n    internal_key = f\"_{key}\"\n    while internal_key in self._modules:\n        internal_key = f\"_{internal_key}\"\n    self._key_map[key] = internal_key\n    super().__setitem__(internal_key, module)\n</code></pre>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict.items","title":"<code>items()</code>","text":"<p>Returns the items (key, module) in the dictionary.</p> <p>Returns:</p> Name Type Description <code>Generator</code> <p>A generator yielding key, module pairs.</p> Source code in <code>src/lighter/utils/patches.py</code> <pre><code>def items(self):\n    \"\"\"\n    Returns the items (key, module) in the dictionary.\n\n    Returns:\n        Generator: A generator yielding key, module pairs.\n    \"\"\"\n    return ((key, self._modules[internal_key]) for key, internal_key in self._key_map.items())\n</code></pre>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict.keys","title":"<code>keys()</code>","text":"<p>Returns the keys of the modules.</p> <p>Returns:</p> Name Type Description <code>KeysView</code> <p>A view of the keys in the dictionary.</p> Source code in <code>src/lighter/utils/patches.py</code> <pre><code>def keys(self):\n    \"\"\"\n    Returns the keys of the modules.\n\n    Returns:\n        KeysView: A view of the keys in the dictionary.\n    \"\"\"\n    return self._key_map.keys()\n</code></pre>"},{"location":"reference/utils/patches/#lighter.utils.patches.PatchedModuleDict.values","title":"<code>values()</code>","text":"<p>Returns the modules in the dictionary.</p> <p>Returns:</p> Name Type Description <code>Generator</code> <p>A generator yielding modules.</p> Source code in <code>src/lighter/utils/patches.py</code> <pre><code>def values(self):\n    \"\"\"\n    Returns the modules in the dictionary.\n\n    Returns:\n        Generator: A generator yielding modules.\n    \"\"\"\n    return (self._modules[internal_key] for internal_key in self._key_map.values())\n</code></pre>"},{"location":"reference/utils/types/","title":"types","text":"<ul> <li>containers</li> <li>enums</li> </ul>"},{"location":"reference/utils/types/containers/","title":"containers","text":""},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Adapters","title":"<code>Adapters</code>  <code>dataclass</code>","text":"<p>Root configuration class for all adapters across different modes.</p> Source code in <code>src/lighter/utils/types/containers.py</code> <pre><code>@nested\n@dataclass\nclass Adapters:\n    \"\"\"Root configuration class for all adapters across different modes.\"\"\"\n\n    train: Train = field(default_factory=Train)\n    val: Val = field(default_factory=Val)\n    test: Test = field(default_factory=Test)\n    predict: Predict = field(default_factory=Predict)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Predict","title":"<code>Predict</code>  <code>dataclass</code>","text":"<p>Predict mode sub-dataclass for Adapters.</p> Source code in <code>src/lighter/utils/types/containers.py</code> <pre><code>@dataclass\nclass Predict:\n    \"\"\"Predict mode sub-dataclass for Adapters.\"\"\"\n\n    batch: BatchAdapter = field(default_factory=lambda: BatchAdapter(input_accessor=lambda batch: batch))\n    logging: LoggingAdapter = field(default_factory=LoggingAdapter)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Test","title":"<code>Test</code>  <code>dataclass</code>","text":"<p>Test mode sub-dataclass for Adapters.</p> Source code in <code>src/lighter/utils/types/containers.py</code> <pre><code>@dataclass\nclass Test:\n    \"\"\"Test mode sub-dataclass for Adapters.\"\"\"\n\n    batch: BatchAdapter = field(default_factory=lambda: BatchAdapter(input_accessor=0, target_accessor=1))\n    metrics: MetricsAdapter = field(default_factory=lambda: MetricsAdapter(pred_argument=0, target_argument=1))\n    logging: LoggingAdapter = field(default_factory=LoggingAdapter)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Train","title":"<code>Train</code>  <code>dataclass</code>","text":"<p>Train mode sub-dataclass for Adapters.</p> Source code in <code>src/lighter/utils/types/containers.py</code> <pre><code>@dataclass\nclass Train:\n    \"\"\"Train mode sub-dataclass for Adapters.\"\"\"\n\n    batch: BatchAdapter = field(default_factory=lambda: BatchAdapter(input_accessor=0, target_accessor=1))\n    criterion: CriterionAdapter = field(default_factory=lambda: CriterionAdapter(pred_argument=0, target_argument=1))\n    metrics: MetricsAdapter = field(default_factory=lambda: MetricsAdapter(pred_argument=0, target_argument=1))\n    logging: LoggingAdapter = field(default_factory=LoggingAdapter)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Val","title":"<code>Val</code>  <code>dataclass</code>","text":"<p>Val mode sub-dataclass for Adapters.</p> Source code in <code>src/lighter/utils/types/containers.py</code> <pre><code>@dataclass\nclass Val:\n    \"\"\"Val mode sub-dataclass for Adapters.\"\"\"\n\n    batch: BatchAdapter = field(default_factory=lambda: BatchAdapter(input_accessor=0, target_accessor=1))\n    criterion: CriterionAdapter = field(default_factory=lambda: CriterionAdapter(pred_argument=0, target_argument=1))\n    metrics: MetricsAdapter = field(default_factory=lambda: MetricsAdapter(pred_argument=0, target_argument=1))\n    logging: LoggingAdapter = field(default_factory=LoggingAdapter)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.nested","title":"<code>nested(cls)</code>","text":"<p>Decorator to handle nested dataclass creation. Example:     <pre><code>@nested\n@dataclass\nclass Example:\n    ...\n</code></pre></p> Source code in <code>src/lighter/utils/types/containers.py</code> <pre><code>def nested(cls):\n    \"\"\"\n    Decorator to handle nested dataclass creation.\n    Example:\n        ```\n        @nested\n        @dataclass\n        class Example:\n            ...\n        ```\n    \"\"\"\n    original_init = cls.__init__\n\n    def __init__(self, *args, **kwargs):\n        for f in fields(cls):\n            if is_dataclass(f.type) and f.name in kwargs:\n                kwargs[f.name] = f.type(**kwargs[f.name])\n        original_init(self, *args, **kwargs)\n\n    cls.__init__ = __init__\n    return cls\n</code></pre>"},{"location":"reference/utils/types/enums/","title":"enums","text":""},{"location":"reference/utils/types/enums/#lighter.utils.types.enums.StrEnum","title":"<code>StrEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum class that inherits from str. This allows for the enum values to be accessed as strings.</p> Source code in <code>src/lighter/utils/types/enums.py</code> <pre><code>class StrEnum(str, Enum):\n    \"\"\"\n    Enum class that inherits from str. This allows for the enum values to be accessed as strings.\n    \"\"\"\n\n    # Remove this class when Python 3.10 support is dropped, as Python &gt;=3.11 has StrEnum built-in.\n    def __str__(self) -&gt; str:\n        return str(self.value)\n</code></pre>"},{"location":"tutorials/configuration_basics/","title":"Configuration Basics","text":"<p>Lighter uses simple YAML config files to define your experiments. Think of a config as a recipe for your experiments, telling Lighter what to train, how to train it, and with what data.  This keeps your experiment setup clean and easy to understand.</p> <p>Every Lighter config needs two main parts: <code>trainer</code> and <code>system</code>.</p>"},{"location":"tutorials/configuration_basics/#core-sections-trainer-and-system","title":"Core Sections: <code>trainer</code> and <code>system</code>","text":"<ul> <li> <p><code>trainer</code>: This section is all about <code>pytorch_lightning.Trainer</code>. You configure how your model will be trained here, like how many epochs to run.</p> </li> <li> <p><code>system</code>: This is where you define what you are training. Using <code>lighter.System</code>, you specify the key ingredients of your experiment:</p> <ul> <li><code>model</code>: Your neural network itself (e.g., ResNet, Linear layers).</li> <li><code>criterion</code>: The loss function (e.g., CrossEntropyLoss).</li> <li><code>optimizer</code>: How the model learns (e.g., Adam).</li> <li><code>dataloaders</code>: How your data is loaded for training and validation.</li> </ul> </li> </ul> <p>Let's look at a super simple example:</p> config.yaml<pre><code>trainer:\n    _target_: pytorch_lightning.Trainer\n    max_epochs: 5  # Train for 5 epochs\n\nsystem:\n    _target_: lighter.System\n\n    model:\n        _target_: torch.nn.Linear  # A simple linear model\n        in_features: 784\n        out_features: 10\n\n    criterion:\n        _target_: torch.nn.CrossEntropyLoss\n\n    optimizer:\n        _target_: torch.optim.Adam\n        params: \"$@system#model.parameters()\" # Link to model's learnable parameters\n        lr: 0.001\n\n    dataloaders:\n        train:\n            _target_: torch.utils.data.DataLoader\n            dataset:\n                _target_: ... # (Dataset definition will come later in tutorials)\n            batch_size: 64\n</code></pre>"},{"location":"tutorials/configuration_basics/#key-config-concepts","title":"Key Config Concepts","text":""},{"location":"tutorials/configuration_basics/#_target_-saying-what-to-use","title":"<code>_target_</code>:  Saying \"What to Use\"","text":"<p>The special key <code>_target_</code> is how you tell Lighter what Python class or function to use.  For example:</p> <pre><code>system:\n    _target_: lighter.System\n\n    model:\n        _target_: torch.nn.Linear  # Use the Linear layer from torch.nn\n        in_features: 784\n        out_features: 10\n</code></pre> <p><code>_target_: torch.nn.Linear</code> means \"use the <code>Linear</code> class from the <code>torch.nn</code> module\".  Any other settings under <code>model</code> (like <code>in_features</code> and <code>out_features</code>) become the arguments used when creating this <code>Linear</code> layer.</p>"},{"location":"tutorials/configuration_basics/#and-referencing-connecting-things-together","title":"<code>@</code> and <code>%</code> Referencing: Connecting Things Together","text":"<p>Sometimes you need to link parts of your config together. We use <code>@</code> or <code>%</code> for this. The main difference is that <code>@</code> references the instance of an object, while <code>%</code> references the definition of an object. We'll dive deeper into the difference between the two at a later stage. For the moment, we will only use <code>@</code>.</p> <p>A common example is telling the optimizer which parameters to update \u2013 these are the parameters of your <code>model</code>:</p> <pre><code>system:\n    # ... model definition ...\n\n    optimizer:\n        _target_: torch.optim.Adam\n        params: \"$@system#model.parameters()\"\n        lr: 0.001\n</code></pre> <p><code>\"@system#model.parameters()\"</code> means:</p> <ol> <li><code>@system#model</code>:  \"Go to the <code>system</code> section and find the <code>model</code> we defined.\"</li> <li><code>.parameters()</code>: \"Then, get the <code>parameters()</code> method of that model.\"</li> <li><code>$</code>: \"Evaluate this as a Python expression.\"</li> </ol> <p>So, <code>params: \"$@system#model.parameters()\"</code> nicely links the <code>Adam</code> optimizer to the learnable weights of your <code>Linear</code> model.</p> <p>In Python, this is equivalent to:</p> <pre><code>model = torch.nn.Linear(in_features=784, out_features=10)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</code></pre>"},{"location":"tutorials/configuration_basics/#command-line-overrides","title":"Command Line Overrides","text":"<p>You can easily change settings in your config directly from the command line when you run your experiment. For instance, to train for 10 epochs instead of 5 (defined in <code>config.yaml</code>):</p> <pre><code>lighter fit config.yaml --trainer#max_epochs=10\n</code></pre> <p>This is super handy for quick experiments!</p>"},{"location":"tutorials/configuration_basics/#thats-the-basics","title":"That's the Basics!","text":"<p>For these tutorials, you'll mainly use <code>trainer</code> and <code>system</code> sections, <code>_target_</code> to define components, and <code>@</code>. For thorough explanation of the config system, please refer to Configure How-To guide.</p> <p>In the next tutorial, we'll use these config basics to build an image classification experiment!</p>"},{"location":"tutorials/image_classification/","title":"Image Classification","text":"<p>In this tutorial we will learn how to:</p> <ol> <li>Set up the project folder</li> <li>Implement a custom CNN model</li> <li>Define the config for training and testing on CIFAR10 dataset</li> <li>Train and test the model using Lighter</li> </ol>"},{"location":"tutorials/image_classification/#setting-up-the-project","title":"Setting up the Project","text":"<p>First, create a new project directory named <code>image_classification</code> with the following structure:</p> <pre><code>image_classification/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 experiments/\n\u2502  \u2514\u2500\u2500 config.yaml\n\u2514\u2500\u2500 models/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 simple_cnn.py\n</code></pre> <p>If you're using Unix/Linux, you can create this structure with the following command:</p> <pre><code>mkdir -p image_classification/{models,experiments} &amp;&amp; touch image_classification/__init__.py image_classification/experiments/config.yaml image_classification/models/__init__.py image_classification/models/simple_cnn.py\n</code></pre> <p>Warning</p> <p>Do not forget to add <code>__init__.py</code> to folders that contain Python modules. For more details, refer to the Project Module guide.</p>"},{"location":"tutorials/image_classification/#setting-up-dataloaders","title":"Setting up Dataloaders","text":"<p><code>system</code>'s <code>dataloaders</code> section defines dataloaders for <code>train</code>, <code>val</code>, <code>test</code>, and <code>predict</code> stages. Let's start by configuring the training dataloader for CIFAR10.</p> <p>Note</p> <p>The complete configuration is provided few sections later.</p> <pre><code>system:\n# ...\n    dataloaders:\n        train:\n            _target_: torch.utils.data.DataLoader\n            batch_size: 32\n            shuffle: True\n            num_workers: 4\n            dataset:\n                _target_: torchvision.datasets.CIFAR10\n                root: cifar10/\n                download: True\n                train: True\n                transform:\n                _target_: torchvision.transforms.Compose\n                transforms:\n                    - _target_: torchvision.transforms.ToTensor\n                    - _target_: torchvision.transforms.Normalize\n                      mean: [0.5, 0.5, 0.5]\n                      std: [0.5, 0.5, 0.5]\n</code></pre> <p>This is equivalent to the following Python code:</p> <pre><code>import torch\nimport torchvision\n\ntransforms = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root=\"cifar10/\",\n    download=True,\n    train=True,\n    transform=transforms\n)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4\n)\n</code></pre>"},{"location":"tutorials/image_classification/#setting-up-the-model","title":"Setting up the Model","text":""},{"location":"tutorials/image_classification/#defining-a-custom-model","title":"Defining a Custom Model","text":"<p>We will use a simple CNN for image classification. Define this model in <code>image_classification/models/simple_cnn.py</code>.</p> image_classification/models/simple_cnn.py<pre><code>import torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(64 * 8 * 8, num_classes) # Assuming 32x32 images\n\n    def forward(self, x):\n        x = self.pool1(self.relu1(self.conv1(x)))\n        x = self.pool2(self.relu2(self.conv2(x)))\n        x = self.flatten(x)\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"tutorials/image_classification/#reference-the-custom-model-in-configyaml","title":"Reference the Custom Model in <code>config.yaml</code>","text":"<p>Now that we have defined the model, let's specify it in the <code>config.yaml</code> file.</p> config.yaml<pre><code>project: /path/to/image_classification\n\nsystem:\n  model:\n    _target_: project.models.simple_cnn.SimpleCNN\n    num_classes: 10  # Matches CIFAR10 classes\n</code></pre> <p>The <code>project</code> specifies the root directory that Lighter will import as a Python module, allowing access to custom code and models. This is why we needed to set up <code>__init__.py</code> files. As a result, we reference the custom model we defined simply using <code>project.models.simple_cnn.SimpleCNN</code>.</p> <p>Note</p> <p>Learn more about the Project Module in the How-To guide.</p> <p>Warning</p> <p>Change <code>/path/to/image_classification</code> to the location of your project directory. Both absolute and relative paths work. To get the absolute path of your <code>image_classification/</code> directory, run: <pre><code>cd image_classification\npwd\n</code></pre></p> <p>If you're using a relative path, pay attention to where you are executing <code>lighter</code> from. For example, if running <code>lighter</code> while in the <code>image_classification/</code> folder, you can simply set <code>project: .</code> to reference the current directory.</p>"},{"location":"tutorials/image_classification/#complete-configuration","title":"Complete Configuration","text":"<p>Now, let's put together the complete <code>config.yaml</code> file for training the <code>SimpleCNN</code> on CIFAR10:</p> config.yaml<pre><code>project: /path/to/image_classification  # Update as noted above\n\ntrainer:\n    _target_: pytorch_lightning.Trainer\n    accelerator: \"auto\"  # Use GPU if available, else CPU\n    max_epochs: 10\n\nsystem:\n    _target_: lighter.System\n\n    model:\n        _target_: project.models.simple_cnn.SimpleCNN\n        num_classes: 10\n\n    criterion:\n        _target_: torch.nn.CrossEntropyLoss\n\n    optimizer:\n        _target_: torch.optim.Adam\n        params: \"$@system#model.parameters()\"  # Link to model's learnable parameters\n        lr: 1.0e-3\n\n    metrics:\n        train:\n            - _target_: torchmetrics.Accuracy\n              task: \"multiclass\"\n              num_classes: 10\n        test: \"%#train\"\n\n    dataloaders:\n        train:\n            _target_: torch.utils.data.DataLoader\n            batch_size: 32\n            shuffle: True\n            num_workers: 4\n            dataset:\n                _target_: torchvision.datasets.CIFAR10\n                root: cifar10/\n                download: True\n                train: True\n                transform:\n                    _target_: torchvision.transforms.Compose\n                    transforms:\n                        - _target_: torchvision.transforms.ToTensor\n                        - _target_: torchvision.transforms.Normalize\n                          mean: [0.5, 0.5, 0.5]\n                          std: [0.5, 0.5, 0.5]\n        test:\n            _target_: torch.utils.data.DataLoader\n            batch_size: 32\n            num_workers: 4\n            dataset:\n                _target_: torchvision.datasets.CIFAR10\n                root: cifar10/\n                download: True\n                train: False\n                transform:\n                    _target_: torchvision.transforms.Compose\n                    transforms:\n                        - _target_: torchvision.transforms.ToTensor\n                        - _target_: torchvision.transforms.Normalize\n                          mean: [0.5, 0.5, 0.5]\n                          std: [0.5, 0.5, 0.5]\n</code></pre> <p>This configuration defines all the necessary components for training and testing:</p> <ul> <li><code>trainer</code>: Configures the PyTorch Lightning Trainer to use automatic accelerator selection and train for a maximum of 10 epochs.</li> <li><code>system</code>: Defines the Lighter System.<ul> <li><code>model</code>: Specifies the <code>SimpleCNN</code> model, a custom model you defined in <code>image_classification/models/simple_cnn.py</code>.</li> <li><code>criterion</code>: Sets the loss function to <code>CrossEntropyLoss</code>.</li> <li><code>optimizer</code>: Uses the <code>Adam</code> optimizer with a learning rate of 1.0e-3.</li> <li><code>metrics</code>: Defines accuracy metrics for training and testing stages.</li> <li><code>dataloaders</code>: Configures <code>DataLoader</code>s for <code>train</code> and <code>test</code> stages, using the CIFAR10 dataset and appropriate transforms.</li> </ul> </li> </ul>"},{"location":"tutorials/image_classification/#training-execution","title":"Training Execution","text":"<p>To start training, save the above configuration as <code>config.yaml</code> in your project directory. Ensure that you have created the <code>image_classification/models/simple_cnn.py</code> file as well. Then, open your terminal, navigate to your project directory, and run the following command:</p> Terminal<pre><code>lighter fit experiments/config.yaml\n</code></pre> <p>Lighter will parse your <code>config.yaml</code>, initialize all the components, and start the training process using PyTorch Lightning. You will see the training progress, including loss and metrics, logged in your terminal.</p>"},{"location":"tutorials/image_classification/#evaluation","title":"Evaluation","text":"<p>After training, you can evaluate your model on the test set:</p> Terminal<pre><code>lighter test experiments/config.yaml\n</code></pre> <p>Lighter will load the best checkpoint saved during training (if a <code>ModelCheckpoint</code> callback is used in the configuration, which is often the default in more complex setups) and evaluate the model on the specified dataloader, reporting the metrics defined in the <code>system.metrics</code> section for the<code>test</code> stage, respectively.</p>"},{"location":"tutorials/image_classification/#next-steps","title":"Next Steps","text":"<p>In this tutorial, you have successfully trained and evaluated an image classification model on the CIFAR10 dataset using Lighter.</p> <p>You now have a solid foundation for building more complex experiments with Lighter. Head over to the How-To guides to explore Lighter's features in more detail.</p>"}]}